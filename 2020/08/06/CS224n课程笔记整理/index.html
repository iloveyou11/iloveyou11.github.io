<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><link rel="stylesheet" href="/js/fancybox/dist/jquery.fancybox.min.css"><title>CS224n课程笔记整理 | blog</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="YP's Blog"><meta name="description" content="课程搬运地址：CS224n 斯坦福深度自然语言处理课or斯坦福CS224n深度学习自然语言处理课程01-02 Word Vectors词向量 word vectors 有时被称为词嵌入 word embeddings 或词表示 word representations，它们是分布式表示 distributed representation。自然语言处理有不同层次的任务，从语言处理到语义解释再到语篇"><meta name="keywords" content="web"><meta property="og:type" content="article"><meta property="og:title" content="CS224n课程笔记整理"><meta property="og:url" content="http://localhost:4000/2020/08/06/CS224n课程笔记整理/index.html"><meta property="og:site_name" content="blog"><meta property="og:description" content="课程搬运地址：CS224n 斯坦福深度自然语言处理课or斯坦福CS224n深度学习自然语言处理课程01-02 Word Vectors词向量 word vectors 有时被称为词嵌入 word embeddings 或词表示 word representations，它们是分布式表示 distributed representation。自然语言处理有不同层次的任务，从语言处理到语义解释再到语篇"><meta property="og:locale" content="default"><meta property="og:updated_time" content="2020-08-06T08:17:46.836Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CS224n课程笔记整理"><meta name="twitter:description" content="课程搬运地址：CS224n 斯坦福深度自然语言处理课or斯坦福CS224n深度学习自然语言处理课程01-02 Word Vectors词向量 word vectors 有时被称为词嵌入 word embeddings 或词表示 word representations，它们是分布式表示 distributed representation。自然语言处理有不同层次的任务，从语言处理到语义解释再到语篇"><link rel="icon" href="/favicon.ico"><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/css/style.css"><script src="/js/pace.min.js"></script></head><script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="/js/fancybox/dist/jquery.fancybox.min.js"></script><script src="/js/wrapimg.js"></script></html><body><div id="container"><header id="header"><div id="banner"></div><div id="header-outer"><div id="header-menu" class="header-menu-pos animated"><div class="header-menu-container"><a href="/" class="left"><span class="site-title">YP&#39;s Blog</span></a><nav id="header-menu-nav" class="right"><a href="/"><i class="fa fa-home"></i> <span>主页</span> </a><a href="/archives"><i class="fa fa-archive"></i> <span>全部</span> </a><a href="/categories/AI/"><i class="fa fa-AI"></i> <span>AI</span> </a><a href="/categories/前端/"><i class="fa fa-前端"></i> <span>前端</span> </a><a href="/categories/计算机/"><i class="fa fa-计算机"></i> <span>计算机</span> </a><a href="/categories/语言/"><i class="fa fa-语言"></i> <span>语言</span> </a><a href="/categories/专题/"><i class="fa fa-专题"></i> <span>专题</span></a></nav><a class="mobile-header-menu-button"><i class="fa fa-bars"></i></a></div></div><div id="header-row"><div id="logo"><a href="/"><img src="/images/logo.png" alt="logo"></a></div><div class="header-info"><div id="header-title"><h2>YP&#39;s Blog</h2></div><div id="header-description"><h3>一个专注前端智能化开发的技术博客</h3></div></div><nav class="header-nav"><div class="social"><a title="Blog" target="_blank" href="https://iloveyou11.github.io/"><i class="fa fa-home fa-2x"></i></a> <a title="Github" target="_blank" href="https://github.com/iloveyou11"><i class="fa fa-github fa-2x"></i></a></div></nav></div></div></header><div class="outer"><section id="main" class="body-wrap"><article id="post-CS224n课程笔记整理" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="post-title" itemprop="name">CS224n课程笔记整理</h1><div class="post-title-bar"><ul><li><i class="fa fa-book"></i> <a href="/categories/AI/">AI</a></li><li><i class="fa fa-calendar"></i> 2020-08-06</li><li><i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span></li></ul></div><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">1.6k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">6分</span></span></span></div></header><div class="article-entry post-content" itemprop="articleBody"><div id="toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#01-02-Word-Vectors"><span class="toc-text">01-02 Word Vectors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CBOW、skip-gram"><span class="toc-text">CBOW、skip-gram</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GloVe"><span class="toc-text">GloVe</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#03-Word-Window-Classification-Neural-Networks-and-Matrix-Calculus"><span class="toc-text">03 Word Window Classification,Neural Networks, and Matrix Calculus</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Named-Entity-Recognition-NER"><span class="toc-text">Named Entity Recognition (NER)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络"><span class="toc-text">神经网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#04-Backpropagation-and-Computation-Graphs"><span class="toc-text">04 Backpropagation and Computation Graphs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#05-Linguistic-Structure-Dependency-Parsing"><span class="toc-text">05 Linguistic Structure Dependency Parsing</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#依存语法"><span class="toc-text">依存语法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#06-The-probability-of-a-sentence-Recurrent-Neural-Networks-and-Language-Models"><span class="toc-text">06 The probability of a sentence Recurrent Neural Networks and Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#07-Vanishing-Gradients-and-Fancy-RNNs"><span class="toc-text">07 Vanishing Gradients and Fancy RNNs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#08-Machine-Translation-Sequence-to-sequence-and-Attention"><span class="toc-text">08 Machine Translation, Sequence-to-sequence and Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#09-Practical-Tips-for-Final-Projects"><span class="toc-text">09 Practical Tips for Final Projects</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Question-Answering-and-the-Default-Final-Project"><span class="toc-text">10 Question Answering and the Default Final Project</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-ConvNets-for-NLP"><span class="toc-text">11 ConvNets for NLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Information-from-parts-of-words-Subword-Models"><span class="toc-text">12 Information from parts of words Subword Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-Modeling-contexts-of-use-Contextual-Representations-and-Pretraining"><span class="toc-text">13 Modeling contexts of use Contextual Representations and Pretraining</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-Transformers-and-Self-Attention-For-Generative-Models"><span class="toc-text">14 Transformers and Self-Attention For Generative Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Natural-Language-Generation"><span class="toc-text">15 Natural Language Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-Coreference-Resolution"><span class="toc-text">16 Coreference Resolution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-Multitask-Learning"><span class="toc-text">17 Multitask Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-Tree-Recursive-Neural-Networks-Constituency-Parsing-and-Sentiment"><span class="toc-text">18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#19-Safety-Bias-and-Fairness"><span class="toc-text">19 Safety, Bias, and Fairness</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#20-The-Future-of-NLP-Deep-Learning"><span class="toc-text">20 The Future of NLP + Deep Learning</span></a></li></ol></div><p><strong>课程搬运地址：</strong><br><a href="https://www.bilibili.com/video/BV1pt411h7aT?from=search&seid=12274901203401929275" target="_blank" rel="noopener">CS224n 斯坦福深度自然语言处理课</a><br>or<br><a href="https://www.bilibili.com/video/BV1Eb411H7Pq?from=search&seid=9061747879250481953" target="_blank" rel="noopener">斯坦福CS224n深度学习自然语言处理课程</a></p><h3 id="01-02-Word-Vectors"><a href="#01-02-Word-Vectors" class="headerlink" title="01-02 Word Vectors"></a>01-02 Word Vectors</h3><p>词向量 <code>word vectors</code> 有时被称为词嵌入 <code>word embeddings</code> 或词表示 <code>word representations</code>，它们是分布式表示 <code>distributed representation</code>。</p><p>自然语言处理有不同层次的任务，从语言处理到语义解释再到语篇处理：</p><ol><li>Easy</li></ol><ul><li>拼写检查 Spell Checking</li><li>关键词检索 Keyword Search</li><li>同义词查找 Finding Synonyms</li></ul><ol start="2"><li>Medium</li></ol><ul><li>解析来自网站、文档等的信息</li></ul><ol start="3"><li>Hard</li></ol><ul><li>机器翻译 Machine Translation</li><li>语义分析 Semantic Analysis</li><li>指代消解 Coreference</li><li>问答系统 Question Answering</li></ul><p><strong>两类词向量计算方法：</strong></p><ol><li>基于统计并且依赖矩阵分解（例如LSA，HAL），能有效利用全局信息</li><li>通过局部上下文信息来预测词向量</li></ol><h4 id="CBOW、skip-gram"><a href="#CBOW、skip-gram" class="headerlink" title="CBOW、skip-gram"></a>CBOW、skip-gram</h4><p>介绍一下非常有效的概率模型：Word2vec。包括了：</p><ul><li><code>算法</code>：<code>CBOW</code>（根据中心词周围的上下文单词来预测该词的词向量）和<code>skip-gram</code>（根据中心词预测周围上下文的词的概率分布）模型</li><li><code>训练方法</code>：<code>negative sampling</code> 和 <code>hierarchical softmax</code>，<code>negative sampling</code>是负采样，<code>hierarchical softmax</code>是使用层次树结构来计算所有词的概率。</li></ul><blockquote><p>视频详细介绍了<code>CBOW</code>和<code>skip-gram</code>模型，<code>negative sampling</code>和<code>hierarchical softmax</code>训练方法，具体讲解请观看视频内容</p></blockquote><p>工具包：<code>Gensim</code>，Gensim提供了将 Glove 转化为Word2Vec格式的API</p><h4 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h4><p>Glove 利用全局统计量，以最小二乘为目标，预测单词<code>i</code>出现在单词<code>j</code>上下文中的概率。</p><blockquote><p>视频详细介绍了<code>GloVe</code>模型，具体讲解请观看视频内容</p></blockquote><h3 id="03-Word-Window-Classification-Neural-Networks-and-Matrix-Calculus"><a href="#03-Word-Window-Classification-Neural-Networks-and-Matrix-Calculus" class="headerlink" title="03 Word Window Classification,Neural Networks, and Matrix Calculus"></a>03 Word Window Classification,Neural Networks, and Matrix Calculus</h3><h4 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a>Named Entity Recognition (NER)</h4><ul><li>NER是命名实体识别，查找和分类文本中的名称，可能的用途有：</li><li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li><li>对于问题回答，答案通常是命名实体</li><li>许多需要的信息实际上是命名实体之间的关联</li><li>同样的技术可以扩展到其他 slot-filling 槽填充 分类</li></ul><p>但是！NER存在一些难点：</p><ul><li>很难计算出实体的边界</li><li>很难知道某物是否是一个实</li><li>很难知道未知/新奇实体的类别</li><li>实体类是模糊的，依赖于上下文</li></ul><h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><blockquote><p>视频详细介绍一些单层和多层神经网络，以及如何将它们用于分类目的。讨论一些训练神经网络的实用技巧和技巧，包括:神经元单元(非线性)、梯度检查、Xavier参数初始化、学习率、Adagrad等。最后,我们将鼓励使用递归神经网络作为语言模型。</p></blockquote><h3 id="04-Backpropagation-and-Computation-Graphs"><a href="#04-Backpropagation-and-Computation-Graphs" class="headerlink" title="04 Backpropagation and Computation Graphs"></a>04 Backpropagation and Computation Graphs</h3><blockquote><p>视频详细介绍了反向传播的过程</p></blockquote><h3 id="05-Linguistic-Structure-Dependency-Parsing"><a href="#05-Linguistic-Structure-Dependency-Parsing" class="headerlink" title="05 Linguistic Structure Dependency Parsing"></a>05 Linguistic Structure Dependency Parsing</h3><h4 id="依存语法"><a href="#依存语法" class="headerlink" title="依存语法"></a>依存语法</h4><p>依存语法是给定一个输入句子S，分析句子的句法依存结构的任务。依存句法的输出是一棵依存语法树，其中输入句子的单词是通过依存关系的方式连接。正式地，依存语法问题是创建一个输入句子的单词 S=w0w1w2…wn 到它的依存语法树的映射图G，最近几年提出了很多以依存句法为基础的的变体，包括基于神经网络的方法。</p><blockquote><p>视频详细介绍了语法结构、依存关系语法分析（依存分析）</p></blockquote><h3 id="06-The-probability-of-a-sentence-Recurrent-Neural-Networks-and-Language-Models"><a href="#06-The-probability-of-a-sentence-Recurrent-Neural-Networks-and-Language-Models" class="headerlink" title="06 The probability of a sentence Recurrent Neural Networks and Language Models"></a>06 The probability of a sentence Recurrent Neural Networks and Language Models</h3><p>介绍一个新的神经网络家族Recurrent Neural Networks (RNNs)</p><ol><li>RNN的优点</li></ol><ul><li>可以处理任意长度的输入</li><li>步骤 t 的计算(理论上)可以使用许多步骤前的信息</li><li>模型大小不会 随着输入的增加而<strong>增加</strong></li><li>在每个时间步上应用相同的权重，因此在处理输入时具有对称性</li></ul><ol start="2"><li>RNN的缺点</li></ol><ul><li>递归计算速度慢</li><li>在实践中，很难从<strong>许多步骤前</strong>返回信息</li><li>后面的课程中会详细介绍</li></ul><blockquote><p>视频详细讲解了RNN模型</p></blockquote><h3 id="07-Vanishing-Gradients-and-Fancy-RNNs"><a href="#07-Vanishing-Gradients-and-Fancy-RNNs" class="headerlink" title="07 Vanishing Gradients and Fancy RNNs"></a>07 Vanishing Gradients and Fancy RNNs</h3><p>课程大纲：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">梯度消失问题 →两种新类型RNN：LSTM和GRU</span><br><span class="line">其他梯度消失（爆炸）的解决方案</span><br><span class="line">  - Gradient clipping</span><br><span class="line">  - Skip connections</span><br><span class="line">更多花哨的RNN变体</span><br><span class="line">  - 双向RNN</span><br><span class="line">  - 多层RNN</span><br></pre></td></tr></table></figure><blockquote><p>视频详细讲解了RNN变种模型，如GRU、LSTM、双向RNN、多层RNN等等</p></blockquote><h3 id="08-Machine-Translation-Sequence-to-sequence-and-Attention"><a href="#08-Machine-Translation-Sequence-to-sequence-and-Attention" class="headerlink" title="08 Machine Translation, Sequence-to-sequence and Attention"></a>08 Machine Translation, Sequence-to-sequence and Attention</h3><p>课程大纲：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">引入新任务：机器翻译</span><br><span class="line">引入一种新的神经结构：</span><br><span class="line">  sequence-to-sequence</span><br><span class="line">  机器翻译是sequence-to-sequence的一个主要用例</span><br><span class="line">引入一种新的神经技术：注意力</span><br><span class="line">  sequence-to-sequence通过attention得到提升</span><br></pre></td></tr></table></figure><blockquote><p>课程详细讲解了<code>sequence-to-sequence</code>模型和<code>attention</code>注意力机制</p></blockquote><h3 id="09-Practical-Tips-for-Final-Projects"><a href="#09-Practical-Tips-for-Final-Projects" class="headerlink" title="09 Practical Tips for Final Projects"></a>09 Practical Tips for Final Projects</h3><blockquote><p>课程详细讲解了工程化项目时，项目流程、需要注意的点、小技巧等等</p></blockquote><h3 id="10-Question-Answering-and-the-Default-Final-Project"><a href="#10-Question-Answering-and-the-Default-Final-Project" class="headerlink" title="10 Question Answering and the Default Final Project"></a>10 Question Answering and the Default Final Project</h3><blockquote><p>课程详细讲解了问答系统的算法及搭建</p></blockquote><h3 id="11-ConvNets-for-NLP"><a href="#11-ConvNets-for-NLP" class="headerlink" title="11 ConvNets for NLP"></a>11 ConvNets for NLP</h3><blockquote><p>课程详细讲解了CNN网络在NLP任务中的应用</p></blockquote><h3 id="12-Information-from-parts-of-words-Subword-Models"><a href="#12-Information-from-parts-of-words-Subword-Models" class="headerlink" title="12 Information from parts of words Subword Models"></a>12 Information from parts of words Subword Models</h3><blockquote><p>课程介绍了Subword Model、fastText</p></blockquote><h3 id="13-Modeling-contexts-of-use-Contextual-Representations-and-Pretraining"><a href="#13-Modeling-contexts-of-use-Contextual-Representations-and-Pretraining" class="headerlink" title="13 Modeling contexts of use Contextual Representations and Pretraining"></a>13 Modeling contexts of use Contextual Representations and Pretraining</h3><blockquote><p>课程介绍了<code>Transformers</code>、<code>BERT</code>及其在NLP任务中的应用</p></blockquote><h3 id="14-Transformers-and-Self-Attention-For-Generative-Models"><a href="#14-Transformers-and-Self-Attention-For-Generative-Models" class="headerlink" title="14 Transformers and Self-Attention For Generative Models"></a>14 Transformers and Self-Attention For Generative Models</h3><blockquote><p>课程介绍了<code>Self-Attention</code>的原理以及在生成式模型中的运用</p></blockquote><h3 id="15-Natural-Language-Generation"><a href="#15-Natural-Language-Generation" class="headerlink" title="15 Natural Language Generation"></a>15 Natural Language Generation</h3><blockquote><p>课程介绍了NLG tasks（NLG：Natural Language Generation）<br>自然语言生成指的是我们生成（即写入）新文本的任何设置</p></blockquote><p>NLG 包括以下任务：</p><ul><li>机器翻译</li><li>摘要</li><li>对话（闲聊和基于任务）</li><li>创意写作：讲故事，诗歌创作</li><li>自由形式问答（即生成答案，从文本或知识库中提取）</li><li>图像字幕</li></ul><h3 id="16-Coreference-Resolution"><a href="#16-Coreference-Resolution" class="headerlink" title="16 Coreference Resolution"></a>16 Coreference Resolution</h3><p>Coreference Resolution是“指代消解”任务，识别所有涉及到相同现实世界实体。</p><h3 id="17-Multitask-Learning"><a href="#17-Multitask-Learning" class="headerlink" title="17 Multitask Learning"></a>17 Multitask Learning</h3><ol><li>多任务学习是一般NLP系统的阻碍</li><li>统一模型可以决定如何转移知识（领域适应，权重分享，转移和零射击学习）</li><li>统一的多任务模型可以</li></ol><ul><li>更容易适应新任务</li><li>简化部署到生产的时间</li><li>降低标准，让更多人解决新任务</li><li>潜在地转向持续学习</li></ul><h3 id="18-Tree-Recursive-Neural-Networks-Constituency-Parsing-and-Sentiment"><a href="#18-Tree-Recursive-Neural-Networks-Constituency-Parsing-and-Sentiment" class="headerlink" title="18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment"></a>18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment</h3><h3 id="19-Safety-Bias-and-Fairness"><a href="#19-Safety-Bias-and-Fairness" class="headerlink" title="19 Safety, Bias, and Fairness"></a>19 Safety, Bias, and Fairness</h3><h3 id="20-The-Future-of-NLP-Deep-Learning"><a href="#20-The-Future-of-NLP-Deep-Learning" class="headerlink" title="20 The Future of NLP + Deep Learning"></a>20 The Future of NLP + Deep Learning</h3><ol><li>利用无标签数据</li></ol><ul><li>Back-translation 和 无监督机器翻译</li><li>提高预训练和GPT-2</li></ul><ol start="2"><li>接下来呢？</li></ol><ul><li>NLP技术的风险和社会影响</li><li>未来的研究方向</li></ul></div><footer class="article-footer"><div class="post-share"><a href="javascript:;" id="share-sub" class="post-share-fab"><i class="fa fa-share-alt"></i></a><div class="post-share-list" id="share-list"><ul class="share-icons"><li><a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2020/08/06/CS224n课程笔记整理/&title=《CS224n课程笔记整理》 — blog&pic=/images/banner.jpg" data-title="微博"><i class="fa fa-weibo"></i></a></li><li><a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信"><i class="fa fa-weixin"></i></a></li><li><a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://localhost:4000/2020/08/06/CS224n课程笔记整理/&title=《CS224n课程笔记整理》 — blog&source=一个专注前端智能化开发技术的网站" data-title="QQ"><i class="fa fa-qq"></i></a></li><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2020/08/06/CS224n课程笔记整理/" data-title="Facebook"><i class="fa fa-facebook"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《CS224n课程笔记整理》 — blog&url=http://localhost:4000/2020/08/06/CS224n课程笔记整理/&via=http://localhost:4000" data-title="Twitter"><i class="fa fa-twitter"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://localhost:4000/2020/08/06/CS224n课程笔记整理/" data-title="Google+"><i class="fa fa-google-plus"></i></a></li></ul></div></div><div class="post-modal wx-share" id="wxShare"><a class="close" href="javascript:;" id="wxShare-close">×</a><p>扫一扫，分享到微信</p><img src="//api.qrserver.com/v1/create-qr-code/?data=http://localhost:4000/2020/08/06/CS224n课程笔记整理/" alt="微信分享二维码"></div><div class="mask"></div><ul class="article-footer-menu"></ul></footer></div></article><aside class="post-toc-pos post-toc-top" id="post-toc"><nav class="post-toc-wrap"><ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#01-02-Word-Vectors"><span class="post-toc-text">01-02 Word Vectors</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CBOW、skip-gram"><span class="post-toc-text">CBOW、skip-gram</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#GloVe"><span class="post-toc-text">GloVe</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#03-Word-Window-Classification-Neural-Networks-and-Matrix-Calculus"><span class="post-toc-text">03 Word Window Classification,Neural Networks, and Matrix Calculus</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Named-Entity-Recognition-NER"><span class="post-toc-text">Named Entity Recognition (NER)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#神经网络"><span class="post-toc-text">神经网络</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#04-Backpropagation-and-Computation-Graphs"><span class="post-toc-text">04 Backpropagation and Computation Graphs</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#05-Linguistic-Structure-Dependency-Parsing"><span class="post-toc-text">05 Linguistic Structure Dependency Parsing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#依存语法"><span class="post-toc-text">依存语法</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#06-The-probability-of-a-sentence-Recurrent-Neural-Networks-and-Language-Models"><span class="post-toc-text">06 The probability of a sentence Recurrent Neural Networks and Language Models</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#07-Vanishing-Gradients-and-Fancy-RNNs"><span class="post-toc-text">07 Vanishing Gradients and Fancy RNNs</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#08-Machine-Translation-Sequence-to-sequence-and-Attention"><span class="post-toc-text">08 Machine Translation, Sequence-to-sequence and Attention</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#09-Practical-Tips-for-Final-Projects"><span class="post-toc-text">09 Practical Tips for Final Projects</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#10-Question-Answering-and-the-Default-Final-Project"><span class="post-toc-text">10 Question Answering and the Default Final Project</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#11-ConvNets-for-NLP"><span class="post-toc-text">11 ConvNets for NLP</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#12-Information-from-parts-of-words-Subword-Models"><span class="post-toc-text">12 Information from parts of words Subword Models</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#13-Modeling-contexts-of-use-Contextual-Representations-and-Pretraining"><span class="post-toc-text">13 Modeling contexts of use Contextual Representations and Pretraining</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#14-Transformers-and-Self-Attention-For-Generative-Models"><span class="post-toc-text">14 Transformers and Self-Attention For Generative Models</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#15-Natural-Language-Generation"><span class="post-toc-text">15 Natural Language Generation</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#16-Coreference-Resolution"><span class="post-toc-text">16 Coreference Resolution</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#17-Multitask-Learning"><span class="post-toc-text">17 Multitask Learning</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#18-Tree-Recursive-Neural-Networks-Constituency-Parsing-and-Sentiment"><span class="post-toc-text">18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#19-Safety-Bias-and-Fairness"><span class="post-toc-text">19 Safety, Bias, and Fairness</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#20-The-Future-of-NLP-Deep-Learning"><span class="post-toc-text">20 The Future of NLP + Deep Learning</span></a></li></ol></nav></aside><nav id="article-nav"><a href="/2020/08/06/NLP技术图谱/" id="article-nav-older" class="article-nav-link-wrap"><span class="article-nav-title">NLP技术图谱</span> <i class="fa fa-hand-o-right" aria-hidden="true"></i></a></nav><div id="lv-container" data-id="city" data-uid="MTAyMC81MDE5Mi8yNjY4Mg=="><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><noscript>为正常使用来必力评论功能请激活JavaScript</noscript></div></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner"><p><span id="busuanzi_container_site_uv" style="display:none">总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">总访问量：<span id="busuanzi_value_site_pv"></span></span></p><p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a> &copy; 2020 Yang Pei<br></p></div></div></footer><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><script>var mihoConfig={root:"http://localhost:4000",animate:"true",isHome:"false",share:"true"}</script><div class="sidebar"><div id="sidebar-top"><span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span></div></div><div class="sidebar-menu-box" id="sidebar-menu-box"><div class="sidebar-menu-box-container"><div id="sidebar-menu-box-categories"><a class="category-link" href="/categories/AI/">AI</a><a class="category-link" href="/categories/专题/">专题</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/计算机/">计算机</a><a class="category-link" href="/categories/语言/">语言</a></div><div id="sidebar-menu-box-tags"></div></div><a href="javascript:;" class="sidebar-menu-box-close">&times;</a></div>ß<div class="mobile-header-menu-nav" id="mobile-header-menu-nav"><div class="mobile-header-menu-container"><span class="title">Menus</span><ul class="mobile-header-menu-navbar"><li><a href="/"><i class="fa fa-home"></i><span>主页</span></a></li><li><a href="/archives"><i class="fa fa-archive"></i><span>全部</span></a></li><li><a href="/categories/AI/"><i class="fa fa-AI"></i><span>AI</span></a></li><li><a href="/categories/前端/"><i class="fa fa-前端"></i><span>前端</span></a></li><li><a href="/categories/计算机/"><i class="fa fa-计算机"></i><span>计算机</span></a></li><li><a href="/categories/语言/"><i class="fa fa-语言"></i><span>语言</span></a></li><li><a href="/categories/专题/"><i class="fa fa-专题"></i><span>专题</span></a></li></ul></div><div class="mobile-header-tag-container"><span class="title">Tags</span><div id="mobile-header-container-tags"></div></div></div><div class="search-wrap"><span class="search-close">&times;</span> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input class="search-field" placeholder="Search..." id="keywords"> <a id="search-submit" href="javascript:;"><i class="fa fa-search"></i></a><div class="search-container" id="search-container"><ul class="search-result" id="search-result"></ul></div></div><div id="search-tpl"><li class="search-result-item"><a href="{url}" class="search-item-li"><span class="search-item-li-title" title="{title}">{title}</span></a></li></div><script src="/js/search.js"></script><script src="/js/main.js"></script><script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script><div id="particles"></div><script src="/js/particles.js"></script><link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css"><script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script><script src="/js/animate.js"></script><script src="/js/pop-img.js"></script><script>$(".article-entry p img").popImg()</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({model:{jsonPath:"/live2dw/assets/Epsilon2.1.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body>