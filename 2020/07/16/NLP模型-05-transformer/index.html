<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>NLP模型-05-transformer | blog</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="YP's Blog"><meta name="description" content="NLP模型-01-预训练语言模型NLP模型-02-HMMNLP模型-03-GMMNLP模型-04-CRFNLP模型-05-transformerNLP模型-06-BertNLP模型-07-LDANLP模型-08-fastTextNLP模型-09-GloveNLP模型-10-textRNN &amp;amp; textCNNNLP模型-11-seq2seqNLP模型-12-attentionNLP模型-13"><meta name="keywords" content="web"><meta property="og:type" content="article"><meta property="og:title" content="NLP模型-05-transformer"><meta property="og:url" content="http://localhost:4000/2020/07/16/NLP模型-05-transformer/index.html"><meta property="og:site_name" content="blog"><meta property="og:description" content="NLP模型-01-预训练语言模型NLP模型-02-HMMNLP模型-03-GMMNLP模型-04-CRFNLP模型-05-transformerNLP模型-06-BertNLP模型-07-LDANLP模型-08-fastTextNLP模型-09-GloveNLP模型-10-textRNN &amp;amp; textCNNNLP模型-11-seq2seqNLP模型-12-attentionNLP模型-13"><meta property="og:locale" content="default"><meta property="og:image" content="https://i.loli.net/2020/07/12/SdtenNQgPkjm6iJ.jpg"><meta property="og:image" content="https://i.loli.net/2020/07/12/2N6PSW1F5ORb4zD.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/LvVNyuGleXH84ba.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/NLS2CcavgdDGwUA.png"><meta property="og:image" content="https://i.loli.net/2020/07/13/FnO2htRbZYKJcux.png"><meta property="og:image" content="https://i.loli.net/2020/07/13/Wyf6jEcHhu5I9dQ.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/cV5HT1NZDBjloUf.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/HkTUR4qDAmXW3C8.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/KWYHyGko3eEFTmB.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/ryFezKHq2cv5TJl.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/oM3ZXHFuBzD67Wn.png"><meta property="og:image" content="https://i.loli.net/2020/07/12/UzZWQCG64Et9y5P.png"><meta property="og:image" content="https://i.loli.net/2020/07/22/JXgR19pBNy7xior.png"><meta property="og:updated_time" content="2020-07-31T08:19:27.028Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="NLP模型-05-transformer"><meta name="twitter:description" content="NLP模型-01-预训练语言模型NLP模型-02-HMMNLP模型-03-GMMNLP模型-04-CRFNLP模型-05-transformerNLP模型-06-BertNLP模型-07-LDANLP模型-08-fastTextNLP模型-09-GloveNLP模型-10-textRNN &amp;amp; textCNNNLP模型-11-seq2seqNLP模型-12-attentionNLP模型-13"><meta name="twitter:image" content="https://i.loli.net/2020/07/12/SdtenNQgPkjm6iJ.jpg"><link rel="icon" href="/favicon.ico"><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/css/style.css"><script src="/js/pace.min.js"></script></head></html><body><div id="container"><header id="header"><div id="banner"></div><div id="header-outer"><div id="header-menu" class="header-menu-pos animated"><div class="header-menu-container"><a href="/" class="left"><span class="site-title">YP&#39;s Blog</span></a><nav id="header-menu-nav" class="right"><a href="/"><i class="fa fa-home"></i> <span>主页</span> </a><a href="/archives"><i class="fa fa-archive"></i> <span>全部</span> </a><a href="/categories/AI/"><i class="fa fa-AI"></i> <span>AI</span> </a><a href="/categories/前端/"><i class="fa fa-前端"></i> <span>前端</span> </a><a href="/categories/计算机/"><i class="fa fa-计算机"></i> <span>计算机</span> </a><a href="/categories/语言/"><i class="fa fa-语言"></i> <span>语言</span> </a><a href="/categories/专题/"><i class="fa fa-专题"></i> <span>专题</span></a></nav><a class="mobile-header-menu-button"><i class="fa fa-bars"></i></a></div></div><div id="header-row"><div id="logo"><a href="/"><img src="/images/logo.png" alt="logo"></a></div><div class="header-info"><div id="header-title"><h2>YP&#39;s Blog</h2></div><div id="header-description"><h3>一个专注前端智能化开发的技术博客</h3></div></div><nav class="header-nav"><div class="social"><a title="Blog" target="_blank" href="https://iloveyou11.github.io/"><i class="fa fa-home fa-2x"></i></a> <a title="Github" target="_blank" href="https://github.com/iloveyou11"><i class="fa fa-github fa-2x"></i></a></div></nav></div></div></header><div class="outer"><section id="main" class="body-wrap"><article id="post-NLP模型-05-transformer" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="post-title" itemprop="name">NLP模型-05-transformer</h1><div class="post-title-bar"><ul><li><i class="fa fa-book"></i> <a href="/categories/AI/">AI</a></li><li><i class="fa fa-calendar"></i> 2020-07-16</li><li><i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span></li></ul></div><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">1.9k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">6分</span></span></span></div></header><div class="article-entry post-content" itemprop="articleBody"><div id="toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是transformer"><span class="toc-text">什么是transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer结构解读"><span class="toc-text">transformer结构解读</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#概念详解"><span class="toc-text">概念详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer为何计算速度快"><span class="toc-text">transformer为何计算速度快</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型改进"><span class="toc-text">模型改进</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Universal-Transformer"><span class="toc-text">Universal Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer-XL"><span class="toc-text">Transformer-XL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reformer"><span class="toc-text">Reformer</span></a></li></ol></li></ol></div><a id="more"></a><p><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-01-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">NLP模型-01-预训练语言模型</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-02-HMM/" target="_blank" rel="noopener">NLP模型-02-HMM</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-03-GMM/" target="_blank" rel="noopener">NLP模型-03-GMM</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-04-CRF/" target="_blank" rel="noopener">NLP模型-04-CRF</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-05-transformer/" target="_blank" rel="noopener">NLP模型-05-transformer</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-06-Bert/" target="_blank" rel="noopener">NLP模型-06-Bert</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-07-LDA/" target="_blank" rel="noopener">NLP模型-07-LDA</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-08-fastText/" target="_blank" rel="noopener">NLP模型-08-fastText</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-09-Glove/" target="_blank" rel="noopener">NLP模型-09-Glove</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-10-textRNN%20&%20textCNN/" target="_blank" rel="noopener">NLP模型-10-textRNN &amp; textCNN</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-11-seq2seq/" target="_blank" rel="noopener">NLP模型-11-seq2seq</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-12-attention/" target="_blank" rel="noopener">NLP模型-12-attention</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-13-XLnet/" target="_blank" rel="noopener">NLP模型-13-XLnet</a><br><a href="https://iloveyou11.github.io/2020/07/28/NLP%E6%A8%A1%E5%9E%8B-14-%E6%A0%B8%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">NLP模型-14-核方法</a></p><h3 id="什么是transformer"><a href="#什么是transformer" class="headerlink" title="什么是transformer"></a>什么是transformer</h3><p>一句话解释：<code>transformer</code>是带有<code>self-attention</code>的<code>seq2seq</code>，输出能同时计算，可以代替RNN结构（必须按输入的顺序计算），因此transformer对于sequence to sequence的应用场景更为高效。</p><p>transformer是首个完全抛弃RNN的recurrence，CNN的convolution，仅用attention来做特征抽取的模型。</p><p><strong>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。</strong></p><!-- more --><h3 id="transformer结构解读"><a href="#transformer结构解读" class="headerlink" title="transformer结构解读"></a>transformer结构解读</h3><p>transformer的模型结构图如下：</p><img src="https://i.loli.net/2020/07/12/SdtenNQgPkjm6iJ.jpg" width="40%" alt="transformer计算流程1"><p>上图中，左半部分是Encoder，右半部分是Decoder。</p><ol><li>Encoder</li></ol><ul><li><code>input</code>会通过<code>input embedding layer</code>变成一个<code>vector</code></li><li>这个<code>vector</code>会加上<code>positional encoding</code></li><li><code>positional encoding</code>接下来会进入灰色的block（灰色block会重复N次）</li><li>其中，灰色block的第一层是<code>Muti-head Attention</code>，sequence通过此层会得到另一个sequence</li><li>接下来通过<code>Add &amp; Norm</code>，作用是将<code>Muti-Head Attention</code>的<code>output</code>和<code>Muti-head Attention</code>的<code>input</code>加起来再做layer normalization（和batch normalization不同）</li><li>接下来通过一个<code>Feed Forward layer</code>，会将sequence的每一个vector进行处理</li><li>然后再接上另一个<code>Add &amp; Norm</code></li></ul><ol start="2"><li>Decoder</li></ol><ul><li><code>outputs</code>会通过<code>output embedding layer</code>变成一个<code>vector</code></li><li>这个<code>vector</code>会加上<code>positional encoding</code></li><li><code>positional encoding</code>接下来会进入灰色的block（灰色block会重复N次）</li><li>其中，灰色block的第一层是<code>Masked Muti-head Attention</code>（masked：attend on the generated sequence），sequence通过此层会得到另一个sequence</li><li>接下来通过<code>Add &amp; Norm</code></li><li>再进入<code>Muti-Head Attention layer</code>接到之前<code>Encoder</code>部分的输出</li><li>接下来通过一个<code>Add &amp; Norm</code>、<code>Feed Forward layer</code>、<code>Add &amp; Norm</code></li><li>再经过<code>Linear</code>、<code>Softmax</code>得到注重的输出。</li></ul><p><strong>计算的具体流程如下图：</strong></p><p>第1步：根据Wq、Wk、Wv，计算每个vector对应的q、k、v，再根据右上角的公式计算αi</p><p>【PS】q、k、v分别代表Query向量（要去查询的，与每个词之间的关系），Key向量（等着被查的词）和Value向量（实际的特征信息），它们是通过3个不同的权值矩阵由嵌入向量X乘以三个不同的权值矩阵Wq、Wk、Wv得到</p><img src="https://i.loli.net/2020/07/12/2N6PSW1F5ORb4zD.png" width="50%" alt="transformer计算流程1"><p>第2步：对每个αi计算softmax值</p><img src="https://i.loli.net/2020/07/12/LvVNyuGleXH84ba.png" width="50%" alt="transformer计算流程2"><p>第3步：使用右上角的公式，计算每个vector对应的输出b</p><img src="https://i.loli.net/2020/07/12/NLS2CcavgdDGwUA.png" width="50%" alt="transformer计算流程3"><p>【注意】以上从x1到xn的运算是可以同时进行的，通过attention值的大小，可以与其他的vector产生上下文关联。</p><p><strong>transformer计算的范围</strong></p><p>Transformer的可处理文本长度定为512，而不是更大的值，一个很重要的因素是，在计算attention的过程中，需要计算（Multi-head attention并不会减少计算量），这也是为什么Transformer处理长距离依赖问题并不太好的原因之一。</p><h3 id="概念详解"><a href="#概念详解" class="headerlink" title="概念详解"></a>概念详解</h3><p><strong>multi-headed机制</strong>（通常都是多层，一层的不够的）</p><p>多组Q、K、V能得到多组表达（类似CNN中的filter），总综合多种特征提取方案，得到综合的特征表达</p><ul><li>通过不同的head得到多个特征表达（一般是8个）</li><li>将所有特征拼接在一起</li><li>可以通过再一层全连接来降维（乘上全连接矩阵）</li></ul><img src="https://i.loli.net/2020/07/13/FnO2htRbZYKJcux.png" width="90%" alt="multi-headed机制"><p><strong>位置信息如何表达？</strong></p><p>加入了位置信息的编码，每个词的位置发生变化，那就不同了。位置信息编码的方法非常多，常见的是加上一个周期信号。</p><p><strong>什么是layer normalization？</strong></p><img src="https://i.loli.net/2020/07/13/Wyf6jEcHhu5I9dQ.png" width="90%" alt="layer norm"><p>LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差。</p><p>Add &amp; Norm做了<strong>残差连接</strong>（多条路的选择给模型自己选择）</p><p><strong>decoder的特点</strong></p><ul><li>attention计算方式不同（不只有self-attention，还多了encoder-decoder attention）</li><li>加入了mask机制（decoder的输出只能一个个输出，需要依赖前面的vector，因此需要使用mask机制，就是不能预知未来输出的东西）</li><li>连softmax层，使用cross-entropy计算loss即可</li></ul><h3 id="transformer为何计算速度快"><a href="#transformer为何计算速度快" class="headerlink" title="transformer为何计算速度快"></a>transformer为何计算速度快</h3><p>我们可以将以上的计算过程使用一连串的矩阵乘法解决，而矩阵乘法可以轻易地使用GPU来加速。如下图所示：</p><p><strong>1. 每个vector的q、k、v的计算都可以使用矩阵相乘Wq<em>a、Wk</em>a、Wv*a加以计算：</strong></p><img src="https://i.loli.net/2020/07/12/cV5HT1NZDBjloUf.png" width="50%" alt="transformer矩阵计算1"><p><strong>2. α的计算也可以通过矩阵相乘表示不，如下图所示：</strong></p><p>（图一）</p><img src="https://i.loli.net/2020/07/12/HkTUR4qDAmXW3C8.png" width="50%" alt="transformer矩阵计算2"><p>（图二）</p><img src="https://i.loli.net/2020/07/12/KWYHyGko3eEFTmB.png" width="50%" alt="transformer矩阵计算3"><p>（图三）</p><img src="https://i.loli.net/2020/07/12/ryFezKHq2cv5TJl.png" width="50%" alt="transformer矩阵计算4"><p><strong>3. b的计算也可以通过矩阵相乘来解决：</strong></p><img src="https://i.loli.net/2020/07/12/oM3ZXHFuBzD67Wn.png" width="50%" alt="transformer矩阵计算5"><p>一下是综合矩阵运算的图解：</p><img src="https://i.loli.net/2020/07/12/UzZWQCG64Et9y5P.png" width="50%" alt="transformer矩阵计算6"><p>具体解析详见<a href="https://iloveyou11.github.io/2020/04/05/NLP-06/" target="_blank" rel="noopener">NLP-06</a>中的self-attention内容。</p><p><strong>attention整体计算流程：</strong></p><ol><li>每个词的Q会跟每个K计算得分（重要性越大，得分越高）</li><li>softmax后会得到整个加权结果（相当于归一化）</li><li>此时每个词看的不只是他前面的序列，而是整个输入序列</li><li>同一时间能计算出所有词的表达结果（利用矩阵乘法）</li></ol><h3 id="模型改进"><a href="#模型改进" class="headerlink" title="模型改进"></a>模型改进</h3><p>原始版的Transformer虽然并不成熟，层数固定不够灵活、算力需求过大导致的不适合处理超长序列等缺陷限制了其实际应用前景。但是其优秀的特征抽取能力吸引了很多学者的关注。很多人提出了不同的变种Transformer来改进或者规避它的缺陷。其中，Universal Transformer、Transformer-XL、Reformer就是典型的代表。</p><h4 id="Universal-Transformer"><a href="#Universal-Transformer" class="headerlink" title="Universal Transformer"></a>Universal Transformer</h4><p>在Transformer中，输入经过Attention后，会进入全连接层进行运算，而Universal Transformer模型则会进入一个共享权重的transition function继续循环计算。这里Transition function可以和之前一样是全连接层，也可以是其他函数层。</p><p>为了控制循环的次数，模型引入了Adaptive Computation Time（ACT）机制。ACT可以调整计算步数，加入ACT机制的Universal transformer被称为Adaptive universal transformer。引入ACT机制后，模型对于文本中更重要的token会进行更多次数的循环，而对于相对不重要的单词会减少计算资源的投入。</p><img src="https://i.loli.net/2020/07/22/JXgR19pBNy7xior.png" alt="Universal Transformer" width="80%"><h4 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h4><p>Transformer通常会将本文分割成长度小于等于d（默认是512）的segment，每个segment之间互相独立，互不干涉。距离超过512的token之间的依赖关系就完全无法建模抽取。同时，这还会带来一个<code>context fragmentation</code>的问题，因为segment的划分并不是根据语义边界，而是根据长度进行划分的，可能会将一个完整的句子拆成两个。</p><p>Transformer-XL提出了<code>segment-level Recurrence</code>来解决这个问题。在对当前segment进行处理的时候，缓存并利用上一个segment中所有layer的隐向量，而且上一个segment的所有隐向量只参与前向计算，不再进行反向传播。</p><p>Transformer-XL在没有大幅度提高算力需求的情况下，一定程度上解决了长距离依赖问题。</p><h4 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h4><p>针对transformer处理文本长度过短（512）的问题，提出了两个机制分别解决这两个问题，它们是<code>locality-sensitve hashing(LSH) attention</code>和<code>Reversible transformer</code>。</p><p>首先用LSH来对每个segment进行分桶，将相似的部分放在同一个桶里面。然后我们将每一个桶并行化计算其中两两之间的点乘。还考虑到有一定的概率相似的向量会被分到不同的桶里，因此采用了多轮hashing来降低这个概率。Reformer在减少了attention计算量的情况下，还减少了模型的内存占用，为未来大型预训练模型的落地奠定了基础。</p><hr><p>推荐阅读：<a href="http://mantchs.com/2019/09/26/NLP/Transformer/" target="_blank" rel="noopener">【NLP】Transformer</a></p></div><footer class="article-footer"><div class="post-share"><a href="javascript:;" id="share-sub" class="post-share-fab"><i class="fa fa-share-alt"></i></a><div class="post-share-list" id="share-list"><ul class="share-icons"><li><a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2020/07/16/NLP模型-05-transformer/&title=《NLP模型-05-transformer》 — blog&pic=/images/banner.jpg" data-title="微博"><i class="fa fa-weibo"></i></a></li><li><a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信"><i class="fa fa-weixin"></i></a></li><li><a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://localhost:4000/2020/07/16/NLP模型-05-transformer/&title=《NLP模型-05-transformer》 — blog&source=一个专注前端智能化开发技术的网站" data-title="QQ"><i class="fa fa-qq"></i></a></li><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2020/07/16/NLP模型-05-transformer/" data-title="Facebook"><i class="fa fa-facebook"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《NLP模型-05-transformer》 — blog&url=http://localhost:4000/2020/07/16/NLP模型-05-transformer/&via=http://localhost:4000" data-title="Twitter"><i class="fa fa-twitter"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://localhost:4000/2020/07/16/NLP模型-05-transformer/" data-title="Google+"><i class="fa fa-google-plus"></i></a></li></ul></div></div><div class="post-modal wx-share" id="wxShare"><a class="close" href="javascript:;" id="wxShare-close">×</a><p>扫一扫，分享到微信</p><img src="//api.qrserver.com/v1/create-qr-code/?data=http://localhost:4000/2020/07/16/NLP模型-05-transformer/" alt="微信分享二维码"></div><div class="mask"></div><ul class="article-footer-menu"></ul></footer></div></article><aside class="post-toc-pos post-toc-top" id="post-toc"><nav class="post-toc-wrap"><ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#什么是transformer"><span class="post-toc-text">什么是transformer</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#transformer结构解读"><span class="post-toc-text">transformer结构解读</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#概念详解"><span class="post-toc-text">概念详解</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#transformer为何计算速度快"><span class="post-toc-text">transformer为何计算速度快</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模型改进"><span class="post-toc-text">模型改进</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Universal-Transformer"><span class="post-toc-text">Universal Transformer</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Transformer-XL"><span class="post-toc-text">Transformer-XL</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Reformer"><span class="post-toc-text">Reformer</span></a></li></ol></li></ol></nav></aside><nav id="article-nav"><a href="/2020/07/17/NLP模型-09-Glove/" id="article-nav-newer" class="article-nav-link-wrap"><span class="article-nav-title"><i class="fa fa-hand-o-left" aria-hidden="true"></i> NLP模型-09-Glove </span></a><a href="/2020/07/16/NLP模型-06-Bert/" id="article-nav-older" class="article-nav-link-wrap"><span class="article-nav-title">NLP模型-06-Bert</span> <i class="fa fa-hand-o-right" aria-hidden="true"></i></a></nav><div id="lv-container" data-id="city" data-uid="MTAyMC81MDE5Mi8yNjY4Mg=="><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><noscript>为正常使用来必力评论功能请激活JavaScript</noscript></div></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner"><p><span id="busuanzi_container_site_uv" style="display:none">总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">总访问量：<span id="busuanzi_value_site_pv"></span></span></p><p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a> &copy; 2020 Yang Pei<br></p></div></div></footer><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><script>var mihoConfig={root:"http://localhost:4000",animate:"false",isHome:"false",share:"true"}</script><div class="sidebar"><div id="sidebar-top"><span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span></div></div><div class="sidebar-menu-box" id="sidebar-menu-box"><div class="sidebar-menu-box-container"><div id="sidebar-menu-box-categories"><a class="category-link" href="/categories/AI/">AI</a><a class="category-link" href="/categories/专题/">专题</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/计算机/">计算机</a><a class="category-link" href="/categories/语言/">语言</a></div><div id="sidebar-menu-box-tags"></div></div><a href="javascript:;" class="sidebar-menu-box-close">&times;</a></div>ß<div class="mobile-header-menu-nav" id="mobile-header-menu-nav"><div class="mobile-header-menu-container"><span class="title">Menus</span><ul class="mobile-header-menu-navbar"><li><a href="/"><i class="fa fa-home"></i><span>主页</span></a></li><li><a href="/archives"><i class="fa fa-archive"></i><span>全部</span></a></li><li><a href="/categories/AI/"><i class="fa fa-AI"></i><span>AI</span></a></li><li><a href="/categories/前端/"><i class="fa fa-前端"></i><span>前端</span></a></li><li><a href="/categories/计算机/"><i class="fa fa-计算机"></i><span>计算机</span></a></li><li><a href="/categories/语言/"><i class="fa fa-语言"></i><span>语言</span></a></li><li><a href="/categories/专题/"><i class="fa fa-专题"></i><span>专题</span></a></li></ul></div><div class="mobile-header-tag-container"><span class="title">Tags</span><div id="mobile-header-container-tags"></div></div></div><div class="search-wrap"><span class="search-close">&times;</span> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input class="search-field" placeholder="Search..." id="keywords"> <a id="search-submit" href="javascript:;"><i class="fa fa-search"></i></a><div class="search-container" id="search-container"><ul class="search-result" id="search-result"></ul></div></div><div id="search-tpl"><li class="search-result-item"><a href="{url}" class="search-item-li"><span class="search-item-li-title" title="{title}">{title}</span></a></li></div><script src="/js/search.js"></script><script src="/js/main.js"></script><script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script><div id="particles"></div><script src="/js/particles.js"></script><script src="/js/pop-img.js"></script><script>$(".article-entry p img").popImg()</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({model:{jsonPath:"/live2dw/assets/Epsilon2.1.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body>