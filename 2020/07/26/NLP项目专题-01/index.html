<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>NLP项目专题-01 | blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="YP's Blog">
  
  <meta name="description" content="NLP项目专题-01-文本分类NLP项目专题-02-自动补全NLP项目专题-03-语义消歧 文本分类方法例如，需求详细描述文档中哪一部分属于数据描述，哪一部分属于UI描述。将这些信息分类后，可以方便后续的文本理解任务(针对不同类别的部分，机器理解的模型是不同的)，这样可以明显提升文本理解与匹配的准确度。 文本分类常用方法如下：  基于词典模板的规则分类 基于过往日志匹配（适用于搜索引擎） 基于传统">
<meta name="keywords" content="web">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP项目专题-01">
<meta property="og:url" content="http://localhost:4000/2020/07/26/NLP项目专题-01/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="NLP项目专题-01-文本分类NLP项目专题-02-自动补全NLP项目专题-03-语义消歧 文本分类方法例如，需求详细描述文档中哪一部分属于数据描述，哪一部分属于UI描述。将这些信息分类后，可以方便后续的文本理解任务(针对不同类别的部分，机器理解的模型是不同的)，这样可以明显提升文本理解与匹配的准确度。 文本分类常用方法如下：  基于词典模板的规则分类 基于过往日志匹配（适用于搜索引擎） 基于传统">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://i.loli.net/2020/07/28/RLT4XoQv86d1Mif.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/HwRrnAq7vzYgX3a.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/4zd5ngprFLqZoSO.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/vwRmjkPyd9n2rCc.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/kH46Et2SXnhgyKC.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/DO7XyQK9RcYx5oC.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/QfMCjdWyJ9Ht1zc.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/4eENa93WGr7yoRz.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/S8lkDgTtLbEsFvn.png">
<meta property="og:image" content="https://i.loli.net/2020/07/28/pQKLzkWH7intaEU.png">
<meta property="og:updated_time" content="2020-07-28T03:06:03.750Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP项目专题-01">
<meta name="twitter:description" content="NLP项目专题-01-文本分类NLP项目专题-02-自动补全NLP项目专题-03-语义消歧 文本分类方法例如，需求详细描述文档中哪一部分属于数据描述，哪一部分属于UI描述。将这些信息分类后，可以方便后续的文本理解任务(针对不同类别的部分，机器理解的模型是不同的)，这样可以明显提升文本理解与匹配的准确度。 文本分类常用方法如下：  基于词典模板的规则分类 基于过往日志匹配（适用于搜索引擎） 基于传统">
<meta name="twitter:image" content="https://i.loli.net/2020/07/28/RLT4XoQv86d1Mif.png">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  <script src="/js/pace.min.js"></script>
  

  
  

</head>
</html>
<body>
  <div id="container">
      <header id="header">
    <div id="banner"></div>
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">YP&#39;s Blog</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a  href="/">
                        <i class="fa fa-home"></i>
                        <span>主页</span>
                    </a>
                    
                    <a  href="/archives">
                        <i class="fa fa-archive"></i>
                        <span>全部</span>
                    </a>
                    
                    <a  href="/categories/AI/">
                        <i class="fa fa-AI"></i>
                        <span>AI</span>
                    </a>
                    
                    <a  href="/categories/前端/">
                        <i class="fa fa-前端"></i>
                        <span>前端</span>
                    </a>
                    
                    <a  href="/categories/计算机/">
                        <i class="fa fa-计算机"></i>
                        <span>计算机</span>
                    </a>
                    
                    <a  href="/categories/语言/">
                        <i class="fa fa-语言"></i>
                        <span>语言</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
        <div id="header-row">
            <div id="logo">
                <a href="/">
                    <img src="/images/logo.png" alt="logo">
                </a>
            </div>
            <div class="header-info">
                <div id="header-title">
                    
                    <h2>
                        YP&#39;s Blog
                    </h2>
                    
                </div>
                <div id="header-description">
                    
                    <h3>
                        一个专注前端智能化开发的技术博客
                    </h3>
                    
                </div>
            </div>
            <nav class="header-nav">
                <div class="social">
                    
                        <a title="Blog" target="_blank" href="https://iloveyou11.github.io/">
                            <i class="fa fa-home fa-2x"></i></a>
                    
                        <a title="Github" target="_blank" href="https://github.com/iloveyou11">
                            <i class="fa fa-github fa-2x"></i></a>
                    
                </div>
            </nav>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-NLP项目专题-01" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
  
    <h1 class="post-title" itemprop="name">
      NLP项目专题-01
    </h1>
    <div class="post-title-bar">
      <ul>
          
              <li>
                  <i class="fa fa-book"></i>
                  
                      <a href="/categories/AI/">AI</a>
                  
              </li>
          
        <li>
          <i class="fa fa-calendar"></i>  2020-07-26
        </li>
        <li>
          <i class="fa fa-eye"></i>
          <span id="busuanzi_value_page_pv"></span>
        </li>
      </ul>
    </div>
  

                    
            </header>
            
                <div class="article-entry post-content" itemprop="articleBody">
                    
                            
                                
    <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#文本分类方法"><span class="toc-text">文本分类方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文本分类实践"><span class="toc-text">文本分类实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#fastText"><span class="toc-text">fastText</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TextCNN"><span class="toc-text">TextCNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TextRNN"><span class="toc-text">TextRNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TextRNN-Attention"><span class="toc-text">TextRNN+Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TextRCNN"><span class="toc-text">TextRCNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HAN"><span class="toc-text">HAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BERT"><span class="toc-text">BERT</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VDCNN"><span class="toc-text">VDCNN</span></a></li></ol></li></ol>
    </div>
    
                                    <p><a href>NLP项目专题-01-文本分类</a><br><a href>NLP项目专题-02-自动补全</a><br><a href>NLP项目专题-03-语义消歧</a></p>
<h3 id="文本分类方法"><a href="#文本分类方法" class="headerlink" title="文本分类方法"></a>文本分类方法</h3><p>例如，需求详细描述文档中哪一部分属于数据描述，哪一部分属于UI描述。将这些信息分类后，可以方便后续的文本理解任务(针对不同类别的部分，机器理解的模型是不同的)，这样可以明显提升文本理解与匹配的准确度。</p>
<p><strong>文本分类常用方法如下：</strong></p>
<ol>
<li>基于词典模板的规则分类</li>
<li>基于过往日志匹配（适用于搜索引擎）</li>
<li>基于传统机器学习模型(特征工程+算法，如NaiveBayes/SVM/LR/KNN……)</li>
<li>基于深度学习模型:词向量+模型(FastText/TextCNN/TextRNN/TextRCNN/DPCNN/BERT/VDCNN)</li>
</ol>
<p>这几种方式基本上是目前比较主流的方法，现在进行意图识别的难点主要是两点，一点是数据来源的匮乏，因为方法已经比较固定，基本都是有监督学习，需要很多的标记数据，现在我们常用的数据要么就是找专业标记团队去买，要么就是自己去爬。第二点是尽管是分类工作，但是意图识别分类种类很多，并且要求的准确性，拓展性都不是之前的分类可比的，这一点也是很困难的。</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">fastText</td>
<td align="left">fastText原理是把句子中所有的词进行lookup得到词向量之后，对向量进行平均（某种意义上可以理解为只有一个avgpooling特殊CNN），然后直接接softmax层预测label。在label比较多的时候，为了降低计算量，论文最后一层采用了层次softmax的方法，既根据label的频次建立哈夫曼树，每个label对应一个哈夫曼编码，每个哈夫曼树节点具有一个向量作为参数进行更新，预测的时候隐层输出与每个哈夫曼树节点向量做点乘，根据结果决定向左右哪个方向移动，最终落到某个label对应的节点上。</td>
</tr>
<tr>
<td align="left">TextCNN</td>
<td align="left">首先，对句子做padding或者截断，保证句子长度为固定值s=7,单词embedding成d=5维度的向量，这样句子被表示为(s,d)(s,d)大小的矩阵（类比图像中的像素）。然后经过有filter_size=(2,3,4)的一维卷积层，每个filter_size有两个输出channel。第三层是一个1-maxpooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的softmax层，输出每个类别的概率。</td>
</tr>
<tr>
<td align="left">TextRNN</td>
<td align="left">对于英文，都是基于词的。对于中文，首先要确定是基于字的还是基于词的。如果是基于词，要先对句子进行分词。之后，每个字/词对应RNN的一个时刻，隐层输出作为下一时刻的输入。最后时刻的隐层输出h_ThTcatch住整个句子的抽象特征，再接一个softmax进行分类。</td>
</tr>
<tr>
<td align="left">TextRNN+Attention</td>
<td align="left">在TextRNN的基础上加入了attention机制</td>
</tr>
<tr>
<td align="left">TextRCNN</td>
<td align="left">利用前向和后向RNN得到每个词的前向和后向上下文的表示，词的表示就变成词向量和前向后向上下文向量concat起来的形式，最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层filter_size=1就可以了，不再需要更大filter_size获得更大视野，这里词的表示也可以只用双向RNN输出</td>
</tr>
<tr>
<td align="left">HAN</td>
<td align="left">HAN模型首先利用Bi-GRU捕捉单词级别的上下文信息。由于句子中的每个单词对于句子表示并不是同等的贡献，因此，作者引入注意力机制来提取对句子表示有重要意义的词汇，并将这些信息词汇的表征聚合起来形成句子向量。</td>
</tr>
<tr>
<td align="left">BERT</td>
<td align="left">BERT的模型架构是一个多层的双向Transformer编码器(Transformer的原理及细节可以参考Attentionisallyouneed)。作者采用两套参数分别生成BERTBASE模型和BERTLARGE模型(细节描述可以参考原论文)，所有下游任务可以在这两套模型进行微调。</td>
</tr>
<tr>
<td align="left">VDCNN</td>
<td align="left">目前NLP领域的模型，无论是机器翻译、文本分类、序列标注等问题大都使用浅层模型。VDCNN探究的是深层模型在文本分类任务中的有效性，最优性能网络达到了29层。</td>
</tr>
</tbody></table>
<h3 id="文本分类实践"><a href="#文本分类实践" class="headerlink" title="文本分类实践"></a>文本分类实践</h3><h4 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h4><p>fastText是一个快速文本分类算法，FastText 算法能获得和深度模型相同的精度，但是计算时间却要远远小于深度学习模型。fastText 可以作为一个文本分类的 baseline 模型。fastText不需要训练好的词向量模型，它会自己训练词向量模型。fastText两个重要的优化：<code>Hierarchical Softmax</code>、<code>N-gram</code>。fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，模型架构类似但是模型任务不同。</p>
<p>fastText模型架构:其中x1,x2,…,xN−1,xN表示一个文本中的<code>n-gram</code>向量，每个特征是词向量的平均值。这和前文中提到的cbow相似，cbow用上下文去预测中心词，而此处用全部的n-gram去预测指定类别。如下图所示：</p>
<img src="https://i.loli.net/2020/07/28/RLT4XoQv86d1Mif.png" alt="fastText" width="50%">

<p><strong>为什么要使用层次softmax模型？</strong><br>在标准的softmax中，计算一个类别的softmax概率时，我们需要对所有类别概率做归一化，在这类别很大情况下非常耗时，因此提出了分层softmax(Hierarchical Softmax),思想是根据类别的频率构造霍夫曼树来代替标准softmax，通过分层softmax可以将复杂度从N降低到logN，下图给出分层softmax示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这里补充一下什么是haffman树：</span><br><span class="line">给定N个权值作为N个叶子结点，构造一棵二叉树，若该树的带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman Tree)。</span><br><span class="line"></span><br><span class="line">haffman树的构造过程：</span><br><span class="line">假设有n个权值，则构造出的哈夫曼树有n个叶子结点。 n个权值分别设为 w1、w2、…、wn，则哈夫曼树的构造规则为：</span><br><span class="line">(1) 将w1、w2、…，wn看成是有n 棵树的森林(每棵树仅有一个结点)；</span><br><span class="line">(2) 在森林中选出两个根结点的权值最小的树合并，作为一棵新树的左、右子树，且新树的根结点权值为其左、右子树根结点权值之和；</span><br><span class="line">(3)从森林中删除选取的两棵树，并将新树加入森林；</span><br><span class="line">(4)重复(2)、(3)步，直到森林中只剩一棵树为止，该树即为所求得的哈夫曼树。</span><br></pre></td></tr></table></figure>

<img src="https://i.loli.net/2020/07/28/HwRrnAq7vzYgX3a.png" alt="层次softmax" width="50%">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> fasttext <span class="keyword">as</span> ft</span><br><span class="line"><span class="keyword">from</span> skllearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">““</span><br><span class="line">分词</span><br><span class="line">去停用词</span><br><span class="line">把处理过后的词写入文本</span><br><span class="line">””</span><br><span class="line"><span class="comment"># 有监督的学习，训练分类器</span></span><br><span class="line">classifier = ft.supervised(filePath, <span class="string">"classifier.model"</span>)</span><br><span class="line">result = classifier.test(filePath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测文档类别</span></span><br><span class="line">labels = classifier.predict(texts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测类别+概率</span></span><br><span class="line">labelProb = classifier.predict_proba(texts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到前k个类别</span></span><br><span class="line">labels = classifier.predict(texts, k=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到前k个类别+概率</span></span><br><span class="line">labelProb = classifier.predict_prob(texts, k=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h4 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h4><p>将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性。</p>
<p>TextCNN的详细流程：</p>
<ul>
<li><code>Embedding</code>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li>
<li><code>Convolution</code>：然后经过 kernel_sizes=(2,3,4) 的一维卷积层，每个kernel_size 有两个输出 channel。</li>
<li><code>MaxPolling</code>：第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li>
<li><code>FullConnection and Softmax</code>：最后接一层全连接的 softmax 层，输出每个类别的概率。</li>
</ul>
<p>TextCNN的模型结构如下：</p>
<img src="https://i.loli.net/2020/07/28/4zd5ngprFLqZoSO.png" alt="TextCNN" width="90%">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> Input</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv1D, MaxPool1D, Dense, Flatten, concatenate, Embedding</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textcnn</span><span class="params">(max_sequence_length, max_token_num, embedding_dim, output_dim, model_img_path=None, embedding_matrix=None)</span>:</span></span><br><span class="line">    <span class="string">""" TextCNN: 1. embedding layers, 2.convolution layer, 3.max-pooling, 4.softmax layer. """</span></span><br><span class="line">    x_input = Input(shape=(max_sequence_length,))</span><br><span class="line">    logging.info(<span class="string">"x_input.shape: %s"</span> % str(x_input.shape))  <span class="comment"># (?, 60)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> embedding_matrix <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        x_emb = Embedding(input_dim=max_token_num, output_dim=embedding_dim, input_length=max_sequence_length)(x_input)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x_emb = Embedding(input_dim=max_token_num, output_dim=embedding_dim, input_length=max_sequence_length,</span><br><span class="line">                          weights=[embedding_matrix], trainable=<span class="literal">True</span>)(x_input)</span><br><span class="line">    logging.info(<span class="string">"x_emb.shape: %s"</span> % str(x_emb.shape))  <span class="comment"># (?, 60, 300)</span></span><br><span class="line"></span><br><span class="line">    pool_output = []</span><br><span class="line">    kernel_sizes = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>] </span><br><span class="line">    <span class="keyword">for</span> kernel_size <span class="keyword">in</span> kernel_sizes:</span><br><span class="line">        c = Conv1D(filters=<span class="number">2</span>, kernel_size=kernel_size, strides=<span class="number">1</span>)(x_emb)</span><br><span class="line">        p = MaxPool1D(pool_size=int(c.shape[<span class="number">1</span>]))(c)</span><br><span class="line">        pool_output.append(p)</span><br><span class="line">        logging.info(<span class="string">"kernel_size: %s \t c.shape: %s \t p.shape: %s"</span> % (kernel_size, str(c.shape), str(p.shape)))</span><br><span class="line">    pool_output = concatenate([p <span class="keyword">for</span> p <span class="keyword">in</span> pool_output])</span><br><span class="line">    logging.info(<span class="string">"pool_output.shape: %s"</span> % str(pool_output.shape))  <span class="comment"># (?, 1, 6)</span></span><br><span class="line"></span><br><span class="line">    x_flatten = Flatten()(pool_output)  <span class="comment"># (?, 6)</span></span><br><span class="line">    y = Dense(output_dim, activation=<span class="string">'softmax'</span>)(x_flatten)  <span class="comment"># (?, 2)</span></span><br><span class="line">    logging.info(<span class="string">"y.shape: %s \n"</span> % str(y.shape))</span><br><span class="line"></span><br><span class="line">    model = Model([x_input], outputs=[y])</span><br><span class="line">    <span class="keyword">if</span> model_img_path:</span><br><span class="line">        plot_model(model, to_file=model_img_path, show_shapes=<span class="literal">True</span>, show_layer_names=<span class="literal">False</span>)</span><br><span class="line">    model.summary()</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h4 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h4><ol>
<li>结构1<br>流程：embedding—&gt;BiLSTM—&gt;concat final output/average all output—–&gt;softmax layer</li>
</ol>
<img src="https://i.loli.net/2020/07/28/vwRmjkPyd9n2rCc.png" alt="TextRNN1" width="60%">

<ol start="2">
<li>结构2<br>流程：embedding–&gt;BiLSTM—-&gt;(dropout)–&gt;concat ouput—&gt;UniLSTM—&gt;(droput)–&gt;softmax layer</li>
</ol>
<img src="https://i.loli.net/2020/07/28/kH46Et2SXnhgyKC.png" alt="TextRNN2" width="80%">



<p><a href="https://www.cnblogs.com/Luv-GEM/p/10836454.html" target="_blank" rel="noopener">中文文本分类之TextRNN</a></p>
<h4 id="TextRNN-Attention"><a href="#TextRNN-Attention" class="headerlink" title="TextRNN+Attention"></a>TextRNN+Attention</h4><p>在textRNN的基础上，加入了attention机制，模型结构如下：</p>
<img src="https://i.loli.net/2020/07/28/DO7XyQK9RcYx5oC.png" alt="TextRNN+Attention" width="60%">

<p>加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。</p>
<h4 id="TextRCNN"><a href="#TextRCNN" class="headerlink" title="TextRCNN"></a>TextRCNN</h4><p>TextRCNN是TextRNN + CNN。结构如下：</p>
<img src="https://i.loli.net/2020/07/28/QfMCjdWyJ9Ht1zc.png" alt="TextRCNN" width="80%">

<h4 id="HAN"><a href="#HAN" class="headerlink" title="HAN"></a>HAN</h4><p>HAN是Hierarchical Attention Network的简称，结构如下：</p>
<img src="https://i.loli.net/2020/07/28/4eENa93WGr7yoRz.png" alt="HAN1" width="60%">

<img src="https://i.loli.net/2020/07/28/S8lkDgTtLbEsFvn.png" alt="HAN2" width="60%">

<p>模型结构分为了以下四个部分：</p>
<ol>
<li>word encoder （BiGRU layer）<br>将每个单词转化为词向量，然后输入到双向的GRU网络中，获得该单词对应的隐藏输出</li>
<li>word attention （Attention layer）<br>根据重要性给单词赋予权重——定义一个随机上下文向量，计算其与句子中每个单词的相似度，然后经过一个softmax操作获得了一个归一化的attention权重矩阵</li>
<li>sentence encoder （BiGRU layer）<br>得到每个句子的向量表示，同理得到文档的向量表示</li>
<li>sentence attention （Attention layer）<br>将文档向量输入到softmax层进行分类</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HAN</span><span class="params">(object)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_sentence_num, max_sentence_length, num_classes, vocab_size,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 embedding_size, learning_rate, decay_steps, decay_rate,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, l2_lambda, grad_clip, is_training=False,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 initializer=tf.random_normal_initializer<span class="params">(stddev=<span class="number">0.1</span>)</span>)</span>:</span>  </span><br><span class="line">        self.vocab_size = vocab_size  </span><br><span class="line">        self.max_sentence_num = max_sentence_num  </span><br><span class="line">        self.max_sentence_length = max_sentence_length  </span><br><span class="line">        self.num_classes = num_classes  </span><br><span class="line">        self.embedding_size = embedding_size  </span><br><span class="line">        self.hidden_size = hidden_size  </span><br><span class="line">        self.learning_rate = learning_rate  </span><br><span class="line">        self.decay_rate = decay_rate  </span><br><span class="line">        self.decay_steps = decay_steps  </span><br><span class="line">        self.l2_lambda = l2_lambda  </span><br><span class="line">        self.grad_clip = grad_clip  </span><br><span class="line">        self.initializer = initializer  </span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>, name=<span class="string">'global_step'</span>)  </span><br><span class="line">        <span class="comment"># placeholder  </span></span><br><span class="line">        self.input_x = tf.placeholder(tf.int32, [<span class="literal">None</span>, max_sentence_num, max_sentence_length], name=<span class="string">'input_x'</span>)  </span><br><span class="line">        self.input_y = tf.placeholder(tf.int32, [<span class="literal">None</span>, num_classes], name=<span class="string">'input_y'</span>)  </span><br><span class="line">        self.dropout_keep_prob = tf.placeholder(tf.float32, name=<span class="string">'dropout_keep_prob'</span>)  </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_training:  </span><br><span class="line">            <span class="keyword">return</span>  </span><br><span class="line">        word_embedding = self.word2vec()  </span><br><span class="line">        sen_vec = self.sen2vec(word_embedding)  </span><br><span class="line">        doc_vec = self.doc2vec(sen_vec)  </span><br><span class="line">        self.logits = self.inference(doc_vec)  </span><br><span class="line">        self.loss_val = self.loss(self.input_y, self.logits)  </span><br><span class="line">        self.train_op = self.train()  </span><br><span class="line">        self.prediction = tf.argmax(self.logits, axis=<span class="number">1</span>, name=<span class="string">'prediction'</span>)  </span><br><span class="line">        self.pred_min = tf.reduce_min(self.prediction)  </span><br><span class="line">        self.pred_max = tf.reduce_max(self.prediction)  </span><br><span class="line">        self.pred_cnt = tf.bincount(tf.cast(self.prediction, dtype=tf.int32))  </span><br><span class="line">        self.label_cnt = tf.bincount(tf.cast(tf.argmax(self.input_y, axis=<span class="number">1</span>), dtype=tf.int32))  </span><br><span class="line">        self.accuracy = self.accuracy(self.logits, self.input_y)</span><br></pre></td></tr></table></figure>

<h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>详见<a href="https://iloveyou11.github.io/2020/07/16/Bert%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E5%8F%98%E7%A7%8D/" target="_blank" rel="noopener">Bert模型及其变种</a></p>
<p><a href="https://blog.csdn.net/qq_20989105/article/details/89492442" target="_blank" rel="noopener">NLP之BERT中文文本分类超详细教程</a></p>
<h4 id="VDCNN"><a href="#VDCNN" class="headerlink" title="VDCNN"></a>VDCNN</h4><p>VDCNN是<code>Very Deep Convolutional Networks for Text Classiﬁcation</code>的缩写，是一个非常深度的卷积神经网络，论文中给出的实现有<code>9 layer，17 layer， 29 layer 以及49 layer</code>，真的是很非常深了。</p>
<p>VDCNN参照VGG和ResNet网络的特征，构建的网络具有如下特征:</p>
<ol>
<li>Convolutional Block， 每个卷积块有两个卷积层，加上batchnorm、relu。卷积核尺寸size=3，池化步长pool_size=2；</li>
<li>经过池化层pooling后，filters翻倍，也就是说，filters是成2^n层次增长的</li>
</ol>
<p>VDCNN模型的结构如下：</p>
<img src="https://i.loli.net/2020/07/28/pQKLzkWH7intaEU.png" alt="VDCNN" width="100%">


<p>使用keras搭建VDCNN模型实现的文本分类代码如下：<a href="https://github.com/yongzhuo/Keras-TextClassification/tree/master/keras_textclassification/m08_TextVDCNN" target="_blank" rel="noopener">Keras-TextClassification</a>，其中，VDCNN模型架构详见<a href="https://github.com/yongzhuo/Keras-TextClassification/blob/master/keras_textclassification/m08_TextVDCNN/graph.py" target="_blank" rel="noopener">graph.py</a></p>

                                        
                </div>
                <footer class="article-footer">
                    
                        
<div class="post-share">
    <a href="javascript:;" id="share-sub" class="post-share-fab">
        <i class="fa fa-share-alt"></i>
    </a>
    <div class="post-share-list" id="share-list">
        <ul class="share-icons">
          <li>
            <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2020/07/26/NLP项目专题-01/&title=《NLP项目专题-01》 — blog&pic=/images/banner.jpg" data-title="微博">
              <i class="fa fa-weibo"></i>
            </a>
          </li>
          <li>
            <a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
          <li>
            <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://localhost:4000/2020/07/26/NLP项目专题-01/&title=《NLP项目专题-01》 — blog&source=一个专注前端智能化开发技术的网站" data-title="QQ">
              <i class="fa fa-qq"></i>
            </a>
          </li>
          <li>
            <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2020/07/26/NLP项目专题-01/" data-title="Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          </li>
          <li>
            <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《NLP项目专题-01》 — blog&url=http://localhost:4000/2020/07/26/NLP项目专题-01/&via=http://localhost:4000" data-title="Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
          <li>
            <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://localhost:4000/2020/07/26/NLP项目专题-01/" data-title="Google+">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        </ul>
     </div>
</div>
<div class="post-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" id="wxShare-close">×</a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=http://localhost:4000/2020/07/26/NLP项目专题-01/" alt="微信分享二维码">
</div>

<div class="mask"></div>

                            
                                <ul class="article-footer-menu">
                                    
                                            
                                </ul>
                                
                </footer>
    </div>
</article>

    
    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#文本分类方法"><span class="post-toc-text">文本分类方法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#文本分类实践"><span class="post-toc-text">文本分类实践</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#fastText"><span class="post-toc-text">fastText</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#TextCNN"><span class="post-toc-text">TextCNN</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#TextRNN"><span class="post-toc-text">TextRNN</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#TextRNN-Attention"><span class="post-toc-text">TextRNN+Attention</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#TextRCNN"><span class="post-toc-text">TextRCNN</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#HAN"><span class="post-toc-text">HAN</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#BERT"><span class="post-toc-text">BERT</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#VDCNN"><span class="post-toc-text">VDCNN</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
        
<nav id="article-nav">
  
    <a href="/2020/07/27/NLP项目专题-02/" id="article-nav-newer" class="article-nav-link-wrap">

      <span class="article-nav-title">
        <i class="fa fa-hand-o-left" aria-hidden="true"></i>
        
          NLP项目专题-02
        
      </span>
    </a>
  
  
    <a href="/2020/07/23/HMM详解/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-title">HMM原理详解</span>
      <i class="fa fa-hand-o-right" aria-hidden="true"></i>
    </a>
  
</nav>

            
                
                    
                                                    </section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


      <p>
        Powered by  <a href="http://hexo.io/" target="_blank">Hexo</a>
        Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a>
      &copy; 2020 Yang Pei<br>
      </p>
    </div>
  </div>
</footer>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
    var mihoConfig = {
        root: "http://localhost:4000",
        animate: "false" ,
        isHome: "false" ,
        share: "true"
    }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            <a class="category-link" href="/categories/AI/">AI</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/计算机/">计算机</a><a class="category-link" href="/categories/语言/">语言</a>
        </div>
        <div id="sidebar-menu-box-tags">
            
        </div>
    </div>
    <a href="javascript:;" class="sidebar-menu-box-close">&times;</a>
</div>ß

<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a  href="/">
                    <i class="fa fa-home"></i><span>主页</span>
                </a>
            </li>
            
            <li>
                <a  href="/archives">
                    <i class="fa fa-archive"></i><span>全部</span>
                </a>
            </li>
            
            <li>
                <a  href="/categories/AI/">
                    <i class="fa fa-AI"></i><span>AI</span>
                </a>
            </li>
            
            <li>
                <a  href="/categories/前端/">
                    <i class="fa fa-前端"></i><span>前端</span>
                </a>
            </li>
            
            <li>
                <a  href="/categories/计算机/">
                    <i class="fa fa-计算机"></i><span>计算机</span>
                </a>
            </li>
            
            <li>
                <a  href="/categories/语言/">
                    <i class="fa fa-语言"></i><span>语言</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            
        </div>
    </div>
</div>
    <div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>
    <script src="/js/search.js"></script>
        <script src="/js/main.js"></script>

            
                <script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script>
                <div id="particles"></div>
                <script src="/js/particles.js"></script>
                    

                        

                                

                                                
                                                                
                                                                    <script src="/js/pop-img.js"></script>
                                                                        <script>
                                                                            $(".article-entry p img").popImg();
                                                                        </script>
                                                                        
  </div>
</body>
</html>