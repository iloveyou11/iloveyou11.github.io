<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><link rel="stylesheet" href="/js/fancybox/dist/jquery.fancybox.min.css"><title>NLP模型-01-预训练语言模型 | blog</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="YP's Blog"><meta name="description" content="NLP模型-01-预训练语言模型NLP模型-02-HMMNLP模型-03-GMMNLP模型-04-CRFNLP模型-05-transformerNLP模型-06-BertNLP模型-07-LDANLP模型-08-fastTextNLP模型-09-GloveNLP模型-10-textRNN &amp;amp; textCNNNLP模型-11-seq2seqNLP模型-12-attentionNLP模型-13"><meta name="keywords" content="web"><meta property="og:type" content="article"><meta property="og:title" content="NLP模型-01-预训练语言模型"><meta property="og:url" content="http://localhost:4000/2020/07/14/NLP模型-01-预训练语言模型/index.html"><meta property="og:site_name" content="blog"><meta property="og:description" content="NLP模型-01-预训练语言模型NLP模型-02-HMMNLP模型-03-GMMNLP模型-04-CRFNLP模型-05-transformerNLP模型-06-BertNLP模型-07-LDANLP模型-08-fastTextNLP模型-09-GloveNLP模型-10-textRNN &amp;amp; textCNNNLP模型-11-seq2seqNLP模型-12-attentionNLP模型-13"><meta property="og:locale" content="default"><meta property="og:image" content="https://i.loli.net/2020/07/20/pJaOoIxtEYBemMW.png"><meta property="og:updated_time" content="2020-08-04T03:52:11.021Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="NLP模型-01-预训练语言模型"><meta name="twitter:description" content="NLP模型-01-预训练语言模型NLP模型-02-HMMNLP模型-03-GMMNLP模型-04-CRFNLP模型-05-transformerNLP模型-06-BertNLP模型-07-LDANLP模型-08-fastTextNLP模型-09-GloveNLP模型-10-textRNN &amp;amp; textCNNNLP模型-11-seq2seqNLP模型-12-attentionNLP模型-13"><meta name="twitter:image" content="https://i.loli.net/2020/07/20/pJaOoIxtEYBemMW.png"><link rel="icon" href="/favicon.ico"><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/css/style.css"><script src="/js/pace.min.js"></script></head><script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="/js/fancybox/dist/jquery.fancybox.min.js"></script><script src="/js/wrapimg.js"></script></html><body><div id="container"><header id="header"><div id="banner"></div><div id="header-outer"><div id="header-menu" class="header-menu-pos animated"><div class="header-menu-container"><a href="/" class="left"><span class="site-title">YP&#39;s Blog</span></a><nav id="header-menu-nav" class="right"><a href="/"><i class="fa fa-home"></i> <span>主页</span> </a><a href="/archives"><i class="fa fa-archive"></i> <span>全部</span> </a><a href="/categories/AI/"><i class="fa fa-AI"></i> <span>AI</span> </a><a href="/categories/前端/"><i class="fa fa-前端"></i> <span>前端</span> </a><a href="/categories/计算机/"><i class="fa fa-计算机"></i> <span>计算机</span> </a><a href="/categories/语言/"><i class="fa fa-语言"></i> <span>语言</span> </a><a href="/categories/专题/"><i class="fa fa-专题"></i> <span>专题</span></a></nav><a class="mobile-header-menu-button"><i class="fa fa-bars"></i></a></div></div><div id="header-row"><div id="logo"><a href="/"><img src="/images/logo.png" alt="logo"></a></div><div class="header-info"><div id="header-title"><h2>YP&#39;s Blog</h2></div><div id="header-description"><h3>一个专注前端智能化开发的技术博客</h3></div></div><nav class="header-nav"><div class="social"><a title="Blog" target="_blank" href="https://iloveyou11.github.io/"><i class="fa fa-home fa-2x"></i></a> <a title="Github" target="_blank" href="https://github.com/iloveyou11"><i class="fa fa-github fa-2x"></i></a></div></nav></div></div></header><div class="outer"><section id="main" class="body-wrap"><article id="post-NLP模型-01-预训练语言模型" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="post-title" itemprop="name">NLP模型-01-预训练语言模型</h1><div class="post-title-bar"><ul><li><i class="fa fa-book"></i> <a href="/categories/AI/">AI</a></li><li><i class="fa fa-calendar"></i> 2020-07-14</li><li><i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span></li></ul></div><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">2.1k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">7分</span></span></span></div></header><div class="article-entry post-content" itemprop="articleBody"><div id="toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是预训练模型"><span class="toc-text">什么是预训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分布式表示"><span class="toc-text">分布式表示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#EMLo"><span class="toc-text">EMLo</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPT"><span class="toc-text">GPT</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BERT"><span class="toc-text">BERT</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RoBERTa"><span class="toc-text">RoBERTa</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ALBERT"><span class="toc-text">ALBERT</span></a></li></ol></li></ol></div><p><a href="https://iloveyou11.github.io/2020/07/14/NLP%E6%A8%A1%E5%9E%8B-01-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">NLP模型-01-预训练语言模型</a><br><a href="https://iloveyou11.github.io/2020/07/14/NLP%E6%A8%A1%E5%9E%8B-02-HMM/" target="_blank" rel="noopener">NLP模型-02-HMM</a><br><a href="https://iloveyou11.github.io/2020/07/15/NLP%E6%A8%A1%E5%9E%8B-03-GMM/" target="_blank" rel="noopener">NLP模型-03-GMM</a><br><a href="https://iloveyou11.github.io/2020/07/15/NLP%E6%A8%A1%E5%9E%8B-04-CRF/" target="_blank" rel="noopener">NLP模型-04-CRF</a><br><a href="https://iloveyou11.github.io/2020/07/16/NLP%E6%A8%A1%E5%9E%8B-05-transformer/" target="_blank" rel="noopener">NLP模型-05-transformer</a><br><a href="https://iloveyou11.github.io/2020/07/16/NLP%E6%A8%A1%E5%9E%8B-06-Bert/" target="_blank" rel="noopener">NLP模型-06-Bert</a><br><a href="https://iloveyou11.github.io/2020/07/17/NLP%E6%A8%A1%E5%9E%8B-07-LDA/" target="_blank" rel="noopener">NLP模型-07-LDA</a><br><a href="https://iloveyou11.github.io/2020/07/17/NLP%E6%A8%A1%E5%9E%8B-08-fastText/" target="_blank" rel="noopener">NLP模型-08-fastText</a><br><a href="https://iloveyou11.github.io/2020/07/17/NLP%E6%A8%A1%E5%9E%8B-09-Glove/" target="_blank" rel="noopener">NLP模型-09-Glove</a><br><a href="https://iloveyou11.github.io/2020/07/18/NLP%E6%A8%A1%E5%9E%8B-10-textRNN%20&%20textCNN/" target="_blank" rel="noopener">NLP模型-10-textRNN &amp; textCNN</a><br><a href="https://iloveyou11.github.io/2020/07/18/NLP%E6%A8%A1%E5%9E%8B-11-seq2seq/" target="_blank" rel="noopener">NLP模型-11-seq2seq</a><br><a href="https://iloveyou11.github.io/2020/07/18/NLP%E6%A8%A1%E5%9E%8B-12-attention/" target="_blank" rel="noopener">NLP模型-12-attention</a><br><a href="https://iloveyou11.github.io/2020/07/19/NLP%E6%A8%A1%E5%9E%8B-13-XLnet/" target="_blank" rel="noopener">NLP模型-13-XLnet</a><br><a href="https://iloveyou11.github.io/2020/07/22/NLP%E6%A8%A1%E5%9E%8B-14-%E6%A0%B8%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">NLP模型-14-核方法</a></p><h3 id="什么是预训练模型"><a href="#什么是预训练模型" class="headerlink" title="什么是预训练模型"></a>什么是预训练模型</h3><p>预训练模型实际上就是已经训练过的模型或模型组件。它已经在大型训练集上经过了很好的训练，学习到了每个词的表示，并且保存下来所有的模型参数。</p><p>预训练模型的本质是用来文本特征提取，可以适用于迁移学习，减少前期训练词向量的成本。</p><p>预训练模型在word embedding就开始出现，相当于AlexNet在卷积神经网络的地位（雏形）。word embedding是将文字进行向量化，探索字词句的向量表示，如one-hot、tfidf等表示方法，但是这种表达方式非常稀疏，而且词的含义并没有得到表达，因此我们要探索稠密向量。</p><p>在自然语言处理领域的背景下，预训练技术通过使用大规模无标注的文本语料来训练深层网络结构，从而得到一组模型参数，这种深层网络结构通常被称为“预训练模型”;将预训练好的模型参数应用到后续的其他特定任务上，这些特定任务通常被称为“下游任务”。</p><p>预训练技术取得的巨大成功，很大程度上归功于其实现了<code>迁移学习</code>的概念。迁移学习本质上是在一个数据集上训练基础模型，通过微调等方式，使得模型可以在其他不同的数据集上处理不同的任务。预训练的过程如上文所述，是将预训练好的模型的相应结构和权重直接应用到下游任务上，从而实现“迁移学习”的概念，即将预训练模型“迁移”到下游任务。</p><p>词向量的表达应该符合以下特征：</p><ol><li>在分布空间中，含义越类似的词语应该距离越近（相同类型的词语应该在同一簇）</li><li>不同语言，相同含义的词语应该出现在分布空间的同一位置</li><li>词语应该支持加减法，如<code>king-queue=man-woman</code></li></ol><p><strong>预训练模型应该如何使用呢？</strong></p><ol><li>直接将预训练模型当做特征提取来使用</li><li>只采用预训练模型的结构，所有权重随机初始化，使用自己的数据集进行重新训练</li><li>Fine-tune，冻结一部分结构，重新训练剩下的层，或者加上一些层</li></ol><p><strong>预训练模型分为以下三类：</strong></p><ol><li>自回归语言模型，如GPT</li><li>去噪自编码DAE，如Bert</li><li>排列语言模型，如XLNet</li></ol><h3 id="分布式表示"><a href="#分布式表示" class="headerlink" title="分布式表示"></a>分布式表示</h3><p>以下是词向量表示法的分类：</p><img src="https://i.loli.net/2020/07/20/pJaOoIxtEYBemMW.png" alt="分布式表示" width="100%"><p>从上图可以看出，词语的分布式表示分为<code>欧式空间</code>和<code>非欧式空间</code>，其中<code>非欧式空间</code>常常是高维的表达，如黎曼空间、球形空间等等，距离计算也均不相同，我们常见的是欧式空间的表达。</p><p>欧式空间可以分为<code>global</code>方法和<code>local</code>方法，<code>global</code>方法会考虑整个句子词与词之间的相关性，从而衡量词语的向量表达，如MF（矩阵分解）、LDA（主题模型）。这些方法需要考虑到整个语料库的信息，但是有个缺点，就是在训练好词向量后，如果加入了新词或者改变了上下文，整个词向量矩阵均需要重新计算，这个计算量还是不容小觑的。因此，基于这个不足，后面衍生出了<code>local</code>的词向量表达——只考虑了一篇文档、一句话或者是上下文的几个词语，而不是完整的语料库，这样能加快更新的速度。</p><p><code>local</code>方法分为<code>基于LM的训练</code>和<code>基于非LM的训练</code>，其本质区分点在于是否使用了语言模型。这里通俗地解释一下语言模型——就是衡量一句话是否是人话的程度。<br><code>基于非LM的训练</code>包括了Skip-Gram、CBOW（连续词袋模型）、Bert、ALBert等等，CBOW是使用上下文的词语预测中心词语，Skip-Gram是使用中心词语预测上下文的词语。两者在训练的过程中，运用了一些小技巧，如层次softmax、Negative Sampling等等，后文会有详细解释。CBOW和Skip-Gram依然存在一些问题，例如碰到一词多义的情景就不知道该怎么办了，总不能用一个固定的词向量表示吧。</p><p>于是就有了<code>local&gt;基于非LM的训练&gt;考虑上下文</code>的模型，如Bert、ALBert等等。transformer是在LSTM之后效果最佳的特征提取器，模型中使用到了attention机制。Bert就是transformer中的Encoder。</p><p><code>local&gt;基于LM的训练</code>中考虑上下文的模型有EMLo、XLNet，不考虑上下文的模型有NNLM。</p><h4 id="EMLo"><a href="#EMLo" class="headerlink" title="EMLo"></a>EMLo</h4><p>EMLo能够很好地解决一词多义的问题，它的basic unit是LSTM，底层使用已经训练好的静态词向量（如Glove），再接上两层双向LSTM双向网络，通过多层进行特征抽取（如单词特征、句法特征、语义特征），最后每个词语都会得到3个向量，分别为底层、第一层LSTM、第二层LSTM，使用在任务中学习到的权重，对这三项进行加权平均，从而得到最终的词向量。</p><p>EMLo虽然指标做到了当时的SOTA，但是仍然存在一些问题：</p><ol><li>EMLo的LSTM抽取特征的能力是远低于如今的transformer的</li><li>使用拼接方式实现双向网络，特征融合的能力还是较弱的，其本质上还是利用了从左到右、从右到左的两个网络</li></ol><h4 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h4><p>GPT（Improving Language Understanding by Generative Pre-Training）就是Transformer的decoder，采用了masked self-attention的训练方式。它的核心思想，是利用<code>一个网络模型</code>去做<code>预训练任务</code>和<code>下游任务</code>，将下游任务的输入全部处理成满足预训练模型的输入格式，这样可以直接使用预训练模型去做任务了。我们只需要在一句话的前后添加两个token标志即可。</p><p>不同于EMLo的LSTM结构，GPT使用transformer能够捕捉到更长范围内的语义信息，且transformer的效果要优于LSTM。</p><p>GPT2相对于GPT来说，并没有太大实质性的改动，增加了更高质量、更广泛、数量更大的数据集，采用了更巨大的transformer结构，且对transformer结构做了一些微调。GPT2的目标，是<strong>使用一个大量和质量好的无标签数据，去完成NLP下游任务（使用无监督模型去做监督任务）</strong>。</p><p>GPT2是一种生成模型，拥有了15亿的参数，在阅读理解任务上能够获得惊人的效果，但是在文本摘要生成上并没有取得理想的效果。</p><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>Bert是transformer结构的encoder，具体的详细解释见<a href="https://iloveyou11.github.io/2020/07/16/Bert%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E5%8F%98%E7%A7%8D/" target="_blank" rel="noopener">《Bert模型及其变种》</a></p><p>目前，Bert模型在诸多NLP任务中都取得了不错的效果，如：</p><ul><li>分类：文本分类、情感分类……</li><li>序列标注：分词、实体识别、语义标注……</li><li>句子关系：问答、推理……</li><li>生成任务：文本摘要、机器翻译……</li></ul><p>Bert仍然存在一些缺点：</p><ul><li>训练和测试的数据格式不一致（测试阶段没有mask token）</li><li>词语之间保持了独立</li><li>无法处理长文本</li></ul><h4 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h4><p>RoBERTa（a Robustly Optimized BERT Pretraining Approach）是Bert的一个强劲改进版本，它使用了更大的数据、更大的batch size、更长的训练时间、更大的学习率（超参数调整），并且将静态的mask改为了动态的mask，获得了不错的效果（超越了之前出现的模型，当时的SOTA）。</p><h4 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h4><p>在RoBERTa出现后的几天，ALBERT（A Lite BERT For Self-Supervised Learning Of Language Representations）就超越了RoBERTa成为当时最先进的算法。ALBERT有以下这些创新点：</p><ul><li>词嵌入向量参数的因式分解</li><li>跨层参数共享</li><li>NSP 预训练任务</li><li>去掉dropout、LAMB优化器、更大的batch-size</li><li>N-gram mask</li></ul></div><footer class="article-footer"><div class="post-share"><a href="javascript:;" id="share-sub" class="post-share-fab"><i class="fa fa-share-alt"></i></a><div class="post-share-list" id="share-list"><ul class="share-icons"><li><a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2020/07/14/NLP模型-01-预训练语言模型/&title=《NLP模型-01-预训练语言模型》 — blog&pic=/images/banner.jpg" data-title="微博"><i class="fa fa-weibo"></i></a></li><li><a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信"><i class="fa fa-weixin"></i></a></li><li><a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://localhost:4000/2020/07/14/NLP模型-01-预训练语言模型/&title=《NLP模型-01-预训练语言模型》 — blog&source=一个专注前端智能化开发技术的网站" data-title="QQ"><i class="fa fa-qq"></i></a></li><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2020/07/14/NLP模型-01-预训练语言模型/" data-title="Facebook"><i class="fa fa-facebook"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《NLP模型-01-预训练语言模型》 — blog&url=http://localhost:4000/2020/07/14/NLP模型-01-预训练语言模型/&via=http://localhost:4000" data-title="Twitter"><i class="fa fa-twitter"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://localhost:4000/2020/07/14/NLP模型-01-预训练语言模型/" data-title="Google+"><i class="fa fa-google-plus"></i></a></li></ul></div></div><div class="post-modal wx-share" id="wxShare"><a class="close" href="javascript:;" id="wxShare-close">×</a><p>扫一扫，分享到微信</p><img src="//api.qrserver.com/v1/create-qr-code/?data=http://localhost:4000/2020/07/14/NLP模型-01-预训练语言模型/" alt="微信分享二维码"></div><div class="mask"></div><ul class="article-footer-menu"></ul></footer></div></article><aside class="post-toc-pos post-toc-top" id="post-toc"><nav class="post-toc-wrap"><ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#什么是预训练模型"><span class="post-toc-text">什么是预训练模型</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#分布式表示"><span class="post-toc-text">分布式表示</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#EMLo"><span class="post-toc-text">EMLo</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#GPT"><span class="post-toc-text">GPT</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#BERT"><span class="post-toc-text">BERT</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#RoBERTa"><span class="post-toc-text">RoBERTa</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ALBERT"><span class="post-toc-text">ALBERT</span></a></li></ol></li></ol></nav></aside><nav id="article-nav"><a href="/2020/07/15/NLP模型-04-CRF/" id="article-nav-newer" class="article-nav-link-wrap"><span class="article-nav-title"><i class="fa fa-hand-o-left" aria-hidden="true"></i> NLP模型-04-CRF </span></a><a href="/2020/07/14/NLP模型-02-HMM/" id="article-nav-older" class="article-nav-link-wrap"><span class="article-nav-title">NLP模型-02-HMM</span> <i class="fa fa-hand-o-right" aria-hidden="true"></i></a></nav></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner"><p><span id="busuanzi_container_site_uv" style="display:none">总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">总访问量：<span id="busuanzi_value_site_pv"></span></span></p><p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a> &copy; 2020 Yang Pei<br></p></div></div></footer><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><script>var mihoConfig={root:"http://localhost:4000",animate:"true",isHome:"false",share:"true"}</script><div class="sidebar"><div id="sidebar-top"><span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span></div></div><div class="sidebar-menu-box" id="sidebar-menu-box"><div class="sidebar-menu-box-container"><div id="sidebar-menu-box-categories"><a class="category-link" href="/categories/AI/">AI</a><a class="category-link" href="/categories/专题/">专题</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/计算机/">计算机</a><a class="category-link" href="/categories/语言/">语言</a></div><div id="sidebar-menu-box-tags"></div></div><a href="javascript:;" class="sidebar-menu-box-close">&times;</a></div>ß<div class="mobile-header-menu-nav" id="mobile-header-menu-nav"><div class="mobile-header-menu-container"><span class="title">Menus</span><ul class="mobile-header-menu-navbar"><li><a href="/"><i class="fa fa-home"></i><span>主页</span></a></li><li><a href="/archives"><i class="fa fa-archive"></i><span>全部</span></a></li><li><a href="/categories/AI/"><i class="fa fa-AI"></i><span>AI</span></a></li><li><a href="/categories/前端/"><i class="fa fa-前端"></i><span>前端</span></a></li><li><a href="/categories/计算机/"><i class="fa fa-计算机"></i><span>计算机</span></a></li><li><a href="/categories/语言/"><i class="fa fa-语言"></i><span>语言</span></a></li><li><a href="/categories/专题/"><i class="fa fa-专题"></i><span>专题</span></a></li></ul></div><div class="mobile-header-tag-container"><span class="title">Tags</span><div id="mobile-header-container-tags"></div></div></div><div class="search-wrap"><span class="search-close">&times;</span> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input class="search-field" placeholder="Search..." id="keywords"> <a id="search-submit" href="javascript:;"><i class="fa fa-search"></i></a><div class="search-container" id="search-container"><ul class="search-result" id="search-result"></ul></div></div><div id="search-tpl"><li class="search-result-item"><a href="{url}" class="search-item-li"><span class="search-item-li-title" title="{title}">{title}</span></a></li></div><script src="/js/search.js"></script><script src="/js/main.js"></script><script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script><div id="particles"></div><script src="/js/particles.js"></script><link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css"><script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script><script src="/js/animate.js"></script><script src="/js/pop-img.js"></script><script>$(".article-entry p img").popImg()</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/Epsilon2.1.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>