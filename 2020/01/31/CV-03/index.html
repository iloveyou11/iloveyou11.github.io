<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><link rel="stylesheet" href="/js/fancybox/dist/jquery.fancybox.min.css"><title>CV系列3：卷积神经网络代码实现 | blog</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="YP's Blog"><meta name="description" content="本文主要介绍各类典型卷积神经网络的代码实现。"><meta name="keywords" content="web"><meta property="og:type" content="article"><meta property="og:title" content="CV系列3：卷积神经网络代码实现"><meta property="og:url" content="http://localhost:4000/2020/01/31/CV-03/index.html"><meta property="og:site_name" content="blog"><meta property="og:description" content="本文主要介绍各类典型卷积神经网络的代码实现。"><meta property="og:locale" content="default"><meta property="og:updated_time" content="2020-08-26T14:08:07.325Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CV系列3：卷积神经网络代码实现"><meta name="twitter:description" content="本文主要介绍各类典型卷积神经网络的代码实现。"><link rel="icon" href="/favicon.ico"><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/css/style.css"><script src="/js/pace.min.js"></script></head><script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="/js/fancybox/dist/jquery.fancybox.min.js"></script><script src="/js/wrapimg.js"></script></html><body><div id="container"><header id="header"><div id="banner"></div><div id="header-outer"><div id="header-menu" class="header-menu-pos animated"><div class="header-menu-container"><a href="/" class="left"><span class="site-title">YP&#39;s Blog</span></a><nav id="header-menu-nav" class="right"><a href="/"><i class="fa fa-home"></i> <span>主页</span> </a><a href="/categories/前端/"><i class="fa fa-前端"></i> <span>前端</span> </a><a href="/categories/源码/"><i class="fa fa-源码"></i> <span>源码</span> </a><a href="/categories/ML/"><i class="fa fa-ML"></i> <span>ML</span> </a><a href="/categories/CV/"><i class="fa fa-CV"></i> <span>CV</span> </a><a href="/categories/NLP/"><i class="fa fa-NLP"></i> <span>NLP</span> </a><a href="/categories/计算机/"><i class="fa fa-计算机"></i> <span>计算机</span> </a><a href="/categories/专题/"><i class="fa fa-专题"></i> <span>专题</span> </a><a href="/categories/感想/"><i class="fa fa-感想"></i> <span>感想</span> </a><a href="/categories/语言/"><i class="fa fa-语言"></i> <span>语言</span></a></nav><a class="mobile-header-menu-button"><i class="fa fa-bars"></i></a></div></div><div id="header-row"><div id="logo"><a href="/"><img src="/images/logo.png" alt="logo"></a></div><div class="header-info"><div id="header-title"><h2>YP&#39;s Blog</h2></div><div id="header-description"><h3>一个专注前端智能化领域的技术博客</h3></div></div><nav class="header-nav"><div class="social"><a title="Blog" target="_blank" href="https://iloveyou11.github.io/"><i class="fa fa-home fa-2x"></i></a> <a title="Github" target="_blank" href="https://github.com/iloveyou11"><i class="fa fa-github fa-2x"></i></a></div></nav></div></div></header><div class="outer"><section id="main" class="body-wrap"><article id="post-CV-03" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="post-title" itemprop="name">CV系列3：卷积神经网络代码实现</h1><div class="post-title-bar"><ul><li><i class="fa fa-book"></i> <a href="/categories/CV/">CV</a></li><li><i class="fa fa-calendar"></i> 2020-01-31</li><li><i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span></li></ul></div><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">6.8k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">37分</span></span></span></div></header><div class="article-entry post-content" itemprop="articleBody"><div id="toc"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#LeNet-雏形网络"><span class="toc-text">LeNet-雏形网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AlexLet"><span class="toc-text">AlexLet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SPPNet-空间金字塔"><span class="toc-text">SPPNet-空间金字塔</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VGGNet"><span class="toc-text">VGGNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GoogLeNet"><span class="toc-text">GoogLeNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet-残差网络"><span class="toc-text">ResNet-残差网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DenseNet-密连网络"><span class="toc-text">DenseNet-密连网络</span></a></li></ol></div><p>本文主要介绍各类典型卷积神经网络的代码实现。</p><a id="more"></a><h4 id="LeNet-雏形网络"><a href="#LeNet-雏形网络" class="headerlink" title="LeNet-雏形网络"></a>LeNet-雏形网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  </span><br><span class="line"><span class="keyword">import</span> input_data  </span><br><span class="line">  </span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">sess = tf.InteractiveSession()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 训练数据</span></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="literal">None</span>, <span class="number">784</span>])  </span><br><span class="line"><span class="comment"># 训练标签数据</span></span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="literal">None</span>, <span class="number">10</span>])  </span><br><span class="line"><span class="comment"># 把x更改为4维张量，第1维代表样本数量，第2维和第3维代表图像长宽， 第4维代表图像通道数, 1表示黑白</span></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 第一层：卷积层</span></span><br><span class="line"><span class="comment"># 过滤器大小为5*5, 当前层深度为1， 过滤器的深度为32</span></span><br><span class="line">conv1_weights = tf.get_variable(<span class="string">"conv1_weights"</span>, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">conv1_biases = tf.get_variable(<span class="string">"conv1_biases"</span>, [<span class="number">32</span>], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment"># 移动步长为1, 使用全0填充</span></span><br><span class="line">conv1 = tf.nn.conv2d(x_image, conv1_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># 激活函数Relu去线性化</span></span><br><span class="line">relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line">  </span><br><span class="line"><span class="comment">#第二层：最大池化层  </span></span><br><span class="line"><span class="comment">#池化层过滤器的大小为2*2, 移动步长为2，使用全0填充  </span></span><br><span class="line">pool1 = tf.nn.max_pool(relu1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#第三层：卷积层  </span></span><br><span class="line">conv2_weights = tf.get_variable(<span class="string">"conv2_weights"</span>, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>)) <span class="comment">#过滤器大小为5*5, 当前层深度为32， 过滤器的深度为64  </span></span><br><span class="line">conv2_biases = tf.get_variable(<span class="string">"conv2_biases"</span>, [<span class="number">64</span>], initializer=tf.constant_initializer(<span class="number">0.0</span>))  </span><br><span class="line">conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) <span class="comment">#移动步长为1, 使用全0填充  </span></span><br><span class="line">relu2 = tf.nn.relu( tf.nn.bias_add(conv2, conv2_biases) )  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#第四层：最大池化层  </span></span><br><span class="line"><span class="comment">#池化层过滤器的大小为2*2, 移动步长为2，使用全0填充  </span></span><br><span class="line">pool2 = tf.nn.max_pool(relu2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#第五层：全连接层  </span></span><br><span class="line">fc1_weights = tf.get_variable(<span class="string">"fc1_weights"</span>, [<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>)) <span class="comment">#7*7*64=3136把前一层的输出变成特征向量  </span></span><br><span class="line">fc1_baises = tf.get_variable(<span class="string">"fc1_baises"</span>, [<span class="number">1024</span>], initializer=tf.constant_initializer(<span class="number">0.1</span>))  </span><br><span class="line">pool2_vector = tf.reshape(pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])  </span><br><span class="line">fc1 = tf.nn.relu(tf.matmul(pool2_vector, fc1_weights) + fc1_baises)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#为了减少过拟合，加入Dropout层  </span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)  </span><br><span class="line">fc1_dropout = tf.nn.dropout(fc1, keep_prob)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#第六层：全连接层  </span></span><br><span class="line">fc2_weights = tf.get_variable(<span class="string">"fc2_weights"</span>, [<span class="number">1024</span>, <span class="number">10</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>)) <span class="comment">#神经元节点数1024, 分类节点10  </span></span><br><span class="line">fc2_biases = tf.get_variable(<span class="string">"fc2_biases"</span>, [<span class="number">10</span>], initializer=tf.constant_initializer(<span class="number">0.1</span>))  </span><br><span class="line">fc2 = tf.matmul(fc1_dropout, fc2_weights) + fc2_biases  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#第七层：输出层  </span></span><br><span class="line"><span class="comment"># softmax  </span></span><br><span class="line">y_conv = tf.nn.softmax(fc2)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#定义交叉熵损失函数  </span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[<span class="number">1</span>]))  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#选择优化器，并让优化器最小化损失函数/收敛, 反向传播  </span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># tf.argmax()返回的是某一维度上其数据最大所在的索引值，在这里即代表预测值和真实值  </span></span><br><span class="line"><span class="comment"># 判断预测值y和真实值y_中最大数的索引是否一致，y的值为1-10概率  </span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 用平均值来统计测试准确率  </span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#开始训练  </span></span><br><span class="line">sess.run(tf.global_variables_initializer())  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):  </span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">100</span>)  </span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:  </span><br><span class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;) <span class="comment">#评估阶段不使用Dropout  </span></span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))  </span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;) <span class="comment">#训练阶段使用50%的Dropout  </span></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#在测试数据上测试准确率  </span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure><h4 id="AlexLet"><a href="#AlexLet" class="headerlink" title="AlexLet"></a>AlexLet</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">第一层：卷积层</span><br><span class="line"></span><br><span class="line">该层的输入是原始图像的像素值，以MNIST数据集为例，则是28x28x1，第一层过滤器尺寸为5x5，深度设置为6，不适用0去填充，因此该层的输出尺寸是28-5+1=24，深度也为6.</span><br><span class="line"></span><br><span class="line">第二层：池化层</span><br><span class="line"></span><br><span class="line">接受第一层的输出作为输入，过滤器大小选为2x2，步长2.</span><br><span class="line"></span><br><span class="line">第三层：卷积层</span><br><span class="line"></span><br><span class="line">卷积和大小5x5，深度为16，同样不使用0填充，步长为1.</span><br><span class="line"></span><br><span class="line">第四层：池化层</span><br><span class="line"></span><br><span class="line">卷积核采用2x2,步长2</span><br><span class="line"></span><br><span class="line">第五层：全连接</span><br><span class="line"></span><br><span class="line">卷积核为5x5，输出节点为120</span><br><span class="line"></span><br><span class="line">第六层：全连接层</span><br><span class="line"></span><br><span class="line">输入节点数120，输出节点数84</span><br><span class="line"></span><br><span class="line">第七层：全连接层</span><br><span class="line"></span><br><span class="line">输入84，输出10</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义网络超参数</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">training_iters = <span class="number">300000</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">20</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义网络参数</span></span><br><span class="line">n_input = <span class="number">784</span> <span class="comment"># 输入的维度</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># 标签的维度</span></span><br><span class="line">dropout = <span class="number">0.5</span> <span class="comment"># Dropout 的概率</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 占位符输入</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, n_input])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, n_classes])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 卷积操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(name, l_input, w, b, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input,</span><br><span class="line">                                                  w, strides=[<span class="number">1</span>, k, k, <span class="number">1</span>],</span><br><span class="line">                                                  padding=<span class="string">'SAME'</span>), b), name=name)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 最大下采样操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(name, l_input, k1, k2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(l_input, ksize=[<span class="number">1</span>, k1, k1, <span class="number">1</span>], strides=[<span class="number">1</span>, k2, k2, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=name)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 归一化操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm</span><span class="params">(name, l_input, lsize=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.lrn(l_input, lsize, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>, name=name)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义整个网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">alex_net</span><span class="params">(_X, _weights, _biases, _dropout)</span>:</span></span><br><span class="line">    <span class="comment"># 向量转为矩阵</span></span><br><span class="line">    _X = tf.reshape(_X, shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 卷积层</span></span><br><span class="line">    conv1 = conv2d(<span class="string">'conv1'</span>, _X, _weights[<span class="string">'wc1'</span>], _biases[<span class="string">'bc1'</span>], <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化层</span></span><br><span class="line">    norm1 = norm(<span class="string">'norm1'</span>, conv1, lsize=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 下采样层</span></span><br><span class="line">    pool1 = max_pool(<span class="string">'pool1'</span>, norm1, k1=<span class="number">3</span>, k2=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Dropout</span></span><br><span class="line">    norm1 = tf.nn.dropout(pool1, _dropout)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 卷积</span></span><br><span class="line">    conv2 = conv2d(<span class="string">'conv2'</span>, norm1, _weights[<span class="string">'wc2'</span>], _biases[<span class="string">'bc2'</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    norm2 = norm(<span class="string">'norm2'</span>, conv2, lsize=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 下采样</span></span><br><span class="line">    pool2 = max_pool(<span class="string">'pool2'</span>, norm2, k1=<span class="number">3</span>, k2=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Dropout</span></span><br><span class="line">    norm2 = tf.nn.dropout(pool2, _dropout)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 卷积</span></span><br><span class="line">    conv3 = conv2d(<span class="string">'conv3'</span>, norm2, _weights[<span class="string">'wc3'</span>], _biases[<span class="string">'bc3'</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 归一化384</span></span><br><span class="line">    norm3 = norm(<span class="string">'norm3'</span>, conv3, lsize=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 下采样</span></span><br><span class="line">    <span class="comment"># pool3 = max_pool('pool3', norm3, k=2)</span></span><br><span class="line">    <span class="comment"># Dropoutize of tensor shape you provided is 150528 : 224x224x</span></span><br><span class="line">    norm3 = tf.nn.dropout(norm3, _dropout)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    # 卷积</span></span><br><span class="line"><span class="string">    conv4 = conv2d('conv4', norm3, _weights['wc4'], _biases['bc4'], 1)</span></span><br><span class="line"><span class="string">    # 归一化</span></span><br><span class="line"><span class="string">    norm4 = norm('norm4', conv4, lsize=4)</span></span><br><span class="line"><span class="string">    # 下采样</span></span><br><span class="line"><span class="string">    # pool3 = max_pool('pool3', norm3, k=2)</span></span><br><span class="line"><span class="string">    # Dropout</span></span><br><span class="line"><span class="string">    norm4 = tf.nn.dropout(norm4, _dropout)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    # 卷积</span></span><br><span class="line"><span class="string">    conv5 = conv2d('conv5', norm4, _weights['wc5'], _biases['bc5'], 1)</span></span><br><span class="line"><span class="string">    # 归一化256</span></span><br><span class="line"><span class="string">    norm5 = norm('norm5', conv5, lsize=4)</span></span><br><span class="line"><span class="string">    # 下采样</span></span><br><span class="line"><span class="string">    pool5 = max_pool('pool5', norm5, k1=3, k2=2)</span></span><br><span class="line"><span class="string">    # Dropout</span></span><br><span class="line"><span class="string">    norm5 = tf.nn.dropout(pool5, _dropout)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 全连接层，先把特征图转为向量</span></span><br><span class="line">    dense1 = tf.reshape(norm3, [<span class="number">-1</span>, _weights[<span class="string">'wd1'</span>].get_shape().as_list()[<span class="number">0</span>]])</span><br><span class="line">    dense1 = tf.nn.dropout(tf.nn.relu(tf.matmul(dense1, _weights[<span class="string">'wd1'</span>]) + _biases[<span class="string">'bd1'</span>], name=<span class="string">'fc1'</span>), _dropout)</span><br><span class="line">    <span class="comment"># 全连接层4096</span></span><br><span class="line">    dense2 = tf.nn.relu(tf.matmul(dense1, _weights[<span class="string">'wd2'</span>]) + _biases[<span class="string">'bd2'</span>], name=<span class="string">'fc2'</span>) <span class="comment"># Relu activation</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 网络输出层384</span></span><br><span class="line">    out = tf.matmul(dense2, _weights[<span class="string">'out'</span>]) + _biases[<span class="string">'out'</span>]</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储所有的网络参数48</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">weights = &#123;</span></span><br><span class="line"><span class="string">    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),</span></span><br><span class="line"><span class="string">    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),</span></span><br><span class="line"><span class="string">    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),</span></span><br><span class="line"><span class="string">    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),</span></span><br><span class="line"><span class="string">    'wd2': tf.Variable(tf.random_normal([1024, 1024])),</span></span><br><span class="line"><span class="string">    'out': tf.Variable(tf.random_normal([1024, 10]))</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">biases = &#123;</span></span><br><span class="line"><span class="string">    'bc1': tf.Variable(tf.random_normal([64])),</span></span><br><span class="line"><span class="string">    'bc2': tf.Variable(tf.random_normal([128])),</span></span><br><span class="line"><span class="string">    'bc3': tf.Variable(tf.random_normal([256])),</span></span><br><span class="line"><span class="string">    'bd1': tf.Variable(tf.random_normal([1024])),</span></span><br><span class="line"><span class="string">    'bd2': tf.Variable(tf.random_normal([1024])),</span></span><br><span class="line"><span class="string">    'out': tf.Variable(tf.random_normal([n_classes]))</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 以字典的形式设置权重和偏置</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'wc1'</span>: tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">64</span>])),</span><br><span class="line">    <span class="string">'wc2'</span>: tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>])),</span><br><span class="line">    <span class="string">'wc3'</span>: tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>])),</span><br><span class="line">    <span class="string">'wd1'</span>: tf.Variable(tf.random_normal([<span class="number">4</span>*<span class="number">4</span>*<span class="number">256</span>, <span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'wd2'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>, <span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>, <span class="number">10</span>]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'bc1'</span>: tf.Variable(tf.random_normal([<span class="number">64</span>])),</span><br><span class="line">    <span class="string">'bc2'</span>: tf.Variable(tf.random_normal([<span class="number">128</span>])),</span><br><span class="line">    <span class="string">'bc3'</span>: tf.Variable(tf.random_normal([<span class="number">256</span>])),</span><br><span class="line">    <span class="string">'bd1'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'bd2'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">pred = alex_net(x, weights, biases, keep_prob)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义损失函数和学习步骤</span></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cost)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 测试网络</span></span><br><span class="line">correct_pred = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化所有的共享变量</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 开启一个训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    step = <span class="number">1</span></span><br><span class="line">    <span class="comment"># Keep training until reach max iterations</span></span><br><span class="line">    <span class="keyword">while</span> step * batch_size &lt; training_iters:</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># 获取批数据</span></span><br><span class="line">        sess.run(optimizer, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: dropout&#125;)</span><br><span class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 计算精度</span></span><br><span class="line">            acc = sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">            <span class="comment"># 计算损失值</span></span><br><span class="line">            loss = sess.run(cost, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">            print(<span class="string">"Iter "</span> + str(step*batch_size) + <span class="string">", Minibatch Loss= "</span> + <span class="string">"&#123;:.6f&#125;"</span>.format(loss) +</span><br><span class="line">                   <span class="string">", Training Accuracy = "</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(acc))</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">    <span class="comment"># 计算测试精度</span></span><br><span class="line">    print(<span class="string">"Testing Accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: mnist.test.images[:<span class="number">256</span>],</span><br><span class="line">                                                             y: mnist.test.labels[:<span class="number">256</span>],</span><br><span class="line">                                                             keep_prob: <span class="number">0.5</span>&#125;))</span><br><span class="line">    print(<span class="string">'**********************'</span>)</span><br><span class="line">    print(<span class="string">"Testing Accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: mnist.test.images[:<span class="number">256</span>],</span><br><span class="line">                                                             y: mnist.test.labels[:<span class="number">256</span>],</span><br><span class="line">                                                             keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure><h4 id="SPPNet-空间金字塔"><a href="#SPPNet-空间金字塔" class="headerlink" title="SPPNet-空间金字塔"></a>SPPNet-空间金字塔</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spp_layer</span><span class="params">(input_, levels=<span class="number">4</span>, name = <span class="string">'SPP_layer'</span>,pool_type = <span class="string">'max_pool'</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Multiple Level SPP layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Works for levels=[1, 2, 3, 6].</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    shape = input_.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(levels):</span><br><span class="line">        <span class="comment">#设置池化参数</span></span><br><span class="line">            l = l + <span class="number">1</span></span><br><span class="line">            ksize = [<span class="number">1</span>, np.ceil(shape[<span class="number">1</span>]/ l + <span class="number">1</span>).astype(np.int32), np.ceil(shape[<span class="number">2</span>] / l + <span class="number">1</span>).astype(np.int32), <span class="number">1</span>]</span><br><span class="line">            strides = [<span class="number">1</span>, np.floor(shape[<span class="number">1</span>] / l + <span class="number">1</span>).astype(np.int32), np.floor(shape[<span class="number">2</span>] / l + <span class="number">1</span>).astype(np.int32), <span class="number">1</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> pool_type == <span class="string">'max_pool'</span>:</span><br><span class="line">                pool = tf.nn.max_pool(input_, ksize=ksize, strides=strides, padding=<span class="string">'SAME'</span>)</span><br><span class="line">                pool = tf.reshape(pool,(shape[<span class="number">0</span>],<span class="number">-1</span>),)</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                pool = tf.nn.avg_pool(input_, ksize=ksize, strides=strides, padding=<span class="string">'SAME'</span>)</span><br><span class="line">                pool = tf.reshape(pool,(shape[<span class="number">0</span>],<span class="number">-1</span>))</span><br><span class="line">            print(<span class="string">"Pool Level &#123;:&#125;: shape &#123;:&#125;"</span>.format(l, pool.get_shape().as_list()))</span><br><span class="line">            <span class="keyword">if</span> l == <span class="number">1</span>：</span><br><span class="line">                x_flatten = tf.reshape(pool,(shape[<span class="number">0</span>],<span class="number">-1</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x_flatten = tf.concat((x_flatten,pool),axis=<span class="number">1</span>) <span class="comment">#四种尺度进行拼接</span></span><br><span class="line">            print(<span class="string">"Pool Level &#123;:&#125;: shape &#123;:&#125;"</span>.format(l, x_flatten.get_shape().as_list()))</span><br><span class="line">            <span class="comment"># pool_outputs.append(tf.reshape(pool, [tf.shape(pool)[1], -1]))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_flatten</span><br></pre></td></tr></table></figure><h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span>  datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"> </span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_batches = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 用来创建卷积层并把本层的参数存入参数列表</span></span><br><span class="line"><span class="comment"># input_op:输入的tensor name:该层的名称 kh:卷积层的高 kw:卷积层的宽 n_out:输出通道数，dh:步长的高 dw:步长的宽，p是参数列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_op</span><span class="params">(input_op,name,kh,kw,n_out,dh,dw,p)</span>:</span></span><br><span class="line">    <span class="comment"># 输入的通道数</span></span><br><span class="line">    n_in = input_op.get_shape()[<span class="number">-1</span>].value</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        kernel = tf.get_variable(scope + <span class="string">"w"</span>,shape=[kh,kw,n_in,n_out],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer_conv2d())</span><br><span class="line">        conv = tf.nn.conv2d(input_op, kernel, (<span class="number">1</span>,dh,dw,<span class="number">1</span>),padding=<span class="string">'SAME'</span>)</span><br><span class="line">        bias_init_val = tf.constant(<span class="number">0.0</span>, shape=[n_out],dtype=tf.float32)</span><br><span class="line">        biases = tf.Variable(bias_init_val , trainable=<span class="literal">True</span> , name=<span class="string">'b'</span>)</span><br><span class="line">        z = tf.nn.bias_add(conv,biases)</span><br><span class="line">        activation = tf.nn.relu(z,name=scope)</span><br><span class="line">        p += [kernel,biases]</span><br><span class="line">        <span class="keyword">return</span> activation</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义全连接层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_op</span><span class="params">(input_op,name,n_out,p)</span>:</span></span><br><span class="line">    n_in = input_op.get_shape()[<span class="number">-1</span>].value</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        kernel = tf.get_variable(scope+<span class="string">'w'</span>,shape=[n_in,n_out],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer_conv2d())</span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[n_out],dtype=tf.float32),name=<span class="string">'b'</span>)</span><br><span class="line">        <span class="comment"># tf.nn.relu_layer()用来对输入变量input_op与kernel做乘法并且加上偏置b</span></span><br><span class="line">        activation = tf.nn.relu_layer(input_op,kernel,biases,name=scope)</span><br><span class="line">        p += [kernel,biases]</span><br><span class="line">        <span class="keyword">return</span> activation</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义最大池化层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mpool_op</span><span class="params">(input_op,name,kh,kw,dh,dw)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(input_op,ksize=[<span class="number">1</span>,kh,kw,<span class="number">1</span>],strides=[<span class="number">1</span>,dh,dw,<span class="number">1</span>],padding=<span class="string">'SAME'</span>,name=name)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#定义网络结构</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference_op</span><span class="params">(input_op,keep_prob)</span>:</span></span><br><span class="line">    p = []</span><br><span class="line">    conv1_1 = conv_op(input_op,name=<span class="string">'conv1_1'</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">64</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv1_2 = conv_op(conv1_1,name=<span class="string">'conv1_2'</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">64</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool1 = mpool_op(conv1_2,name=<span class="string">'pool1'</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dw=<span class="number">2</span>,dh=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    conv2_1 = conv_op(pool1,name=<span class="string">'conv2_1'</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">128</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv2_2 = conv_op(conv2_1,name=<span class="string">'conv2_2'</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">128</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool2 = mpool_op(conv2_2, name=<span class="string">'pool2'</span>, kh=<span class="number">2</span>, kw=<span class="number">2</span>, dw=<span class="number">2</span>, dh=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    conv3_1 = conv_op(pool2, name=<span class="string">'conv3_1'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">256</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    conv3_2 = conv_op(conv3_1, name=<span class="string">'conv3_2'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">256</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    conv3_3 = conv_op(conv3_2, name=<span class="string">'conv3_3'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">256</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    pool3 = mpool_op(conv3_3, name=<span class="string">'pool3'</span>, kh=<span class="number">2</span>, kw=<span class="number">2</span>, dw=<span class="number">2</span>, dh=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    conv4_1 = conv_op(pool3, name=<span class="string">'conv4_1'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">512</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    conv4_2 = conv_op(conv4_1, name=<span class="string">'conv4_2'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">512</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    conv4_3 = conv_op(conv4_2, name=<span class="string">'conv4_3'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">512</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    pool4 = mpool_op(conv4_3, name=<span class="string">'pool4'</span>, kh=<span class="number">2</span>, kw=<span class="number">2</span>, dw=<span class="number">2</span>, dh=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    conv5_1 = conv_op(pool4, name=<span class="string">'conv5_1'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">512</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    conv5_2 = conv_op(conv5_1, name=<span class="string">'conv5_2'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">512</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    conv5_3 = conv_op(conv5_2, name=<span class="string">'conv5_3'</span>, kh=<span class="number">3</span>, kw=<span class="number">3</span>, n_out=<span class="number">512</span>, dh=<span class="number">1</span>, dw=<span class="number">1</span>, p=p)</span><br><span class="line">    pool5 = mpool_op(conv5_3, name=<span class="string">'pool5'</span>, kh=<span class="number">2</span>, kw=<span class="number">2</span>, dw=<span class="number">2</span>, dh=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    shp = pool5.get_shape()</span><br><span class="line">    flattened_shape = shp[<span class="number">1</span>].value * shp[<span class="number">2</span>].value * shp[<span class="number">3</span>].value</span><br><span class="line">    resh1 = tf.reshape(pool5,[<span class="number">-1</span>,flattened_shape],name=<span class="string">"resh1"</span>)</span><br><span class="line"> </span><br><span class="line">    fc6 = fc_op(resh1,name=<span class="string">"fc6"</span>,n_out=<span class="number">4096</span>,p=p)</span><br><span class="line">    fc6_drop = tf.nn.dropout(fc6,keep_prob,name=<span class="string">'fc6_drop'</span>)</span><br><span class="line">    fc7 = fc_op(fc6_drop,name=<span class="string">"fc7"</span>,n_out=<span class="number">4096</span>,p=p)</span><br><span class="line">    fc7_drop = tf.nn.dropout(fc7,keep_prob,name=<span class="string">"fc7_drop"</span>)</span><br><span class="line">    fc8 = fc_op(fc7_drop,name=<span class="string">"fc8"</span>,n_out=<span class="number">1000</span>,p=p)</span><br><span class="line">    softmax = tf.nn.softmax(fc8)</span><br><span class="line">    predictions = tf.argmax(softmax,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions,softmax,fc8,p</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session,target,feed,info_string)</span>:</span></span><br><span class="line">    num_steps_burn_in = <span class="number">10</span>  <span class="comment"># 预热轮数</span></span><br><span class="line">    total_duration = <span class="number">0.0</span>  <span class="comment"># 总时间</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span>  <span class="comment"># 总时间的平方和用以计算方差</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches + num_steps_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target,feed_dict=feed)</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_steps_burn_in:  <span class="comment"># 只考虑预热轮数之后的时间</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span>:</span><br><span class="line">                print(<span class="string">'%s:step %d,duration = %.3f'</span> % (datetime.now(), i - num_steps_burn_in, duration))</span><br><span class="line">                total_duration += duration</span><br><span class="line">                total_duration_squared += duration * duration</span><br><span class="line">    mn = total_duration / num_batches  <span class="comment"># 平均每个batch的时间</span></span><br><span class="line">    vr = total_duration_squared / num_batches - mn * mn  <span class="comment"># 方差</span></span><br><span class="line">    sd = math.sqrt(vr)  <span class="comment"># 标准差</span></span><br><span class="line">    print(<span class="string">'%s: %s across %d steps, %.3f +/- %.3f sec/batch'</span> % (datetime.now(), info_string, num_batches, mn, sd))</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_benchmark</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        image_size = <span class="number">224</span>  <span class="comment"># 输入图像尺寸</span></span><br><span class="line">        images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, <span class="number">3</span>], dtype=tf.float32, stddev=<span class="number">1e-1</span>))</span><br><span class="line">        keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">        prediction,softmax,fc8,p = inference_op(images,keep_prob)</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        time_tensorflow_run(sess, prediction,&#123;keep_prob:<span class="number">1.0</span>&#125;, <span class="string">"Forward"</span>)</span><br><span class="line">        <span class="comment"># 用以模拟训练的过程</span></span><br><span class="line">        objective = tf.nn.l2_loss(fc8)  <span class="comment"># 给一个loss</span></span><br><span class="line">        grad = tf.gradients(objective, p)  <span class="comment"># 相对于loss的 所有模型参数的梯度</span></span><br><span class="line">        time_tensorflow_run(sess, grad, &#123;keep_prob:<span class="number">0.5</span>&#125;,<span class="string">"Forward-backward"</span>)</span><br></pre></td></tr></table></figure><h4 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h4><p>这里实现的是Inception V3卷积网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line">slim=tf.contrib.slim</span><br><span class="line"><span class="comment">#产生截断的正态分布</span></span><br><span class="line">trunc_normal =<span class="keyword">lambda</span> stddev:tf.truncated_normal_initializer(<span class="number">0.0</span>,stddev)</span><br><span class="line">parameters =[] <span class="comment">#储存参数</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#why？为什么要定义这个函数？</span></span><br><span class="line"><span class="comment">#因为若事先定义好slim.conv2d各种默认参数，包括激活函数、标准化器，后面定义卷积层将会非常容易：</span></span><br><span class="line"><span class="comment"># 1.代码整体美观</span></span><br><span class="line"><span class="comment"># 2.网络设计的工作量会大大减轻</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3_arg_scope</span><span class="params">(weight_decay=<span class="number">0.00004</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                           stddev=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                           batch_norm_var_collection=<span class="string">'moving_vars'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    #定义inception_v3_arg_scope(),</span></span><br><span class="line"><span class="string">    #用来生成网络中经常用到的函数的默认参数（卷积的激活函数、权重初始化方式、标准化器等）</span></span><br><span class="line"><span class="string">    :param weight_decay: 权值衰减系数</span></span><br><span class="line"><span class="string">    :param stddev: 标准差</span></span><br><span class="line"><span class="string">    :param batch_norm_var_collection:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    batch_norm_params=&#123;</span><br><span class="line">        <span class="string">'decay'</span>:<span class="number">0.9997</span>, <span class="comment">#衰减系数decay</span></span><br><span class="line">        <span class="string">'epsilon'</span>:<span class="number">0.001</span>, <span class="comment">#极小值</span></span><br><span class="line">        <span class="string">'updates_collections'</span>:tf.GraphKeys.UPDATE_OPS,</span><br><span class="line">        <span class="string">'variables_collections'</span>:&#123;</span><br><span class="line">            <span class="string">'beta'</span>:<span class="literal">None</span>,</span><br><span class="line">            <span class="string">'gamma'</span>:<span class="literal">None</span>,</span><br><span class="line">            <span class="string">'moving_mean'</span>:[batch_norm_var_collection],</span><br><span class="line">            <span class="string">'moving_variance'</span>:[batch_norm_var_collection],</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.fully_connected],</span><br><span class="line">                        weights_regularizer=slim.l2_regularizer(weight_decay)):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        slim.arg_scope()是一个非常有用的工具，可以给函数的参数自动赋予某些默认值</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        例如：</span></span><br><span class="line"><span class="string">        slim.arg_scope([slim.conv2d,slim.fully_connected],weights_regularizer=slim.l2_regularizer(weight_decay)):</span></span><br><span class="line"><span class="string">        会对[slim.conv2d,slim.fully_connected]这两个函数的参数自动赋值，</span></span><br><span class="line"><span class="string">        将参数weights_regularizer的默认值设为slim.l2_regularizer(weight_decay)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        备注：使用了slim.arg_scope后就不需要每次重复设置参数，只需在修改时设置即可。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 设置默认值：对slim.conv2d函数的几个参数赋予默认值</span></span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">            [slim.conv2d],</span><br><span class="line">            weights_initializer=tf.truncated_normal_initializer(stddev=stddev), <span class="comment">#权重初始化</span></span><br><span class="line">            activation_fn=tf.nn.relu,  <span class="comment">#激励函数</span></span><br><span class="line">            normalizer_fn=slim.batch_norm,  <span class="comment">#标准化器</span></span><br><span class="line">            normalizer_params=batch_norm_params ) <span class="keyword">as</span> sc: <span class="comment">#normalizer_params标准化器的参数</span></span><br><span class="line"> </span><br><span class="line">            <span class="keyword">return</span> sc  <span class="comment">#返回定义好的scope</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_V3_base</span><span class="params">(input,scope=None)</span>:</span></span><br><span class="line"> </span><br><span class="line">    end_points= &#123;&#125;</span><br><span class="line">    <span class="comment"># 第一部分--基础部分：卷积和池化交错</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">'inception_V3'</span>,[input]):</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],</span><br><span class="line">                            stride=<span class="number">1</span>,padding=<span class="string">'VALID'</span>):</span><br><span class="line">            net1=slim.conv2d(input,<span class="number">32</span>,[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">'conv2d_1a_3x3'</span>)</span><br><span class="line">            net2 = slim.conv2d(net1, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>],scope=<span class="string">'conv2d_2a_3x3'</span>)</span><br><span class="line">            net3 = slim.conv2d(net2, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">'SAME'</span>,</span><br><span class="line">                               scope=<span class="string">'conv2d_2b_3x3'</span>)</span><br><span class="line">            net4=slim.max_pool2d(net3,[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">'maxPool_3a_3x3'</span>)</span><br><span class="line">            net5 = slim.conv2d(net4, <span class="number">80</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_4a_3x3'</span>)</span><br><span class="line">            net6 = slim.conv2d(net5, <span class="number">192</span>, [<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">'SAME'</span>,</span><br><span class="line">                               scope=<span class="string">'conv2d_4b_3x3'</span>)</span><br><span class="line">            net = slim.max_pool2d(net6, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'maxPool_5a_3x3'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">#第二部分--Inception模块组：inception_1\inception_2\inception_2</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],</span><br><span class="line">                            stride=<span class="number">1</span>,padding=<span class="string">'SAME'</span>):</span><br><span class="line"><span class="comment">#inception_1：第一个模块组（共含3个inception_module）</span></span><br><span class="line">        <span class="comment">#inception_1_m1: 第一组的1号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_1_m1'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0=slim.conv2d(net,<span class="number">64</span>,[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">48</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">64</span>, [<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_1b_5x5'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2b_3x3'</span>)</span><br><span class="line">                branch2_3 = slim.conv2d(branch2_2, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2c_3x3'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line">            <span class="comment">#使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net=tf.concat([branch_0,branch1_2,branch2_3,branch3_2],<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># inception_1_m2: 第一组的 2号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_1_m2'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">48</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">64</span>, [<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_5x5'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_2b_3x3'</span>)</span><br><span class="line">                branch2_3 = slim.conv2d(branch2_2, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_2c_3x3'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_2, branch2_3, branch3_2], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># inception_1_m2: 第一组的 3号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_1_m3'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">48</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">64</span>, [<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_5x5'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_2b_3x3'</span>)</span><br><span class="line">                branch2_3 = slim.conv2d(branch2_2, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_2c_3x3'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_2, branch2_3, branch3_2], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#inception_2：第2个模块组（共含5个inception_module）</span></span><br><span class="line">        <span class="comment"># inception_2_m1: 第2组的 1号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_2_m1'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">3</span>],stride=<span class="number">2</span>,</span><br><span class="line">                                       padding=<span class="string">'VALID'</span>,scope=<span class="string">'conv2d_0a_3x3'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_3x3'</span>)</span><br><span class="line">                branch1_3 = slim.conv2d(branch1_2, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                        stride=<span class="number">2</span>,</span><br><span class="line">                                        padding=<span class="string">'VALID'</span>,</span><br><span class="line">                                        scope=<span class="string">'conv2d_1c_3x3'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.max_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                        stride=<span class="number">2</span>,</span><br><span class="line">                                        padding=<span class="string">'VALID'</span>,</span><br><span class="line">                                        scope=<span class="string">'maxPool_2a_3x3'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_3, branch2_1], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># inception_2_m2: 第2组的 2号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_2_m2'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_1x7'</span>)</span><br><span class="line">                branch1_3 = slim.conv2d(branch1_2, <span class="number">128</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_1c_7x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">128</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2b_7x1'</span>)</span><br><span class="line">                branch2_3 = slim.conv2d(branch2_2, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2c_1x7'</span>)</span><br><span class="line">                branch2_4 = slim.conv2d(branch2_3, <span class="number">128</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2d_7x1'</span>)</span><br><span class="line">                branch2_5 = slim.conv2d(branch2_4, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2e_1x7'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># inception_2_m3: 第2组的 3号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_2_m3'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_1x7'</span>)</span><br><span class="line">                branch1_3 = slim.conv2d(branch1_2, <span class="number">192</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_1c_7x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">160</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2b_7x1'</span>)</span><br><span class="line">                branch2_3 = slim.conv2d(branch2_2, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2c_1x7'</span>)</span><br><span class="line">                branch2_4 = slim.conv2d(branch2_3, <span class="number">160</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2d_7x1'</span>)</span><br><span class="line">                branch2_5 = slim.conv2d(branch2_4, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2e_1x7'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># inception_2_m4: 第2组的 4号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_2_m4'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_1x7'</span>)</span><br><span class="line">                branch1_3 = slim.conv2d(branch1_2, <span class="number">192</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_1c_7x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">160</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2b_7x1'</span>)</span><br><span class="line">                branch2_3 = slim.conv2d(branch2_2, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2c_1x7'</span>)</span><br><span class="line">                branch2_4 = slim.conv2d(branch2_3, <span class="number">160</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2d_7x1'</span>)</span><br><span class="line">                branch2_5 = slim.conv2d(branch2_4, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2e_1x7'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># inception_2_m5: 第2组的 5号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_2_m5'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_1x7'</span>)</span><br><span class="line">                branch1_3 = slim.conv2d(branch1_2, <span class="number">192</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_1c_7x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">160</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2b_7x1'</span>)</span><br><span class="line">                branch2_3 = slim.conv2d(branch2_2, <span class="number">160</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2c_1x7'</span>)</span><br><span class="line">                branch2_4 = slim.conv2d(branch2_3, <span class="number">160</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2d_7x1'</span>)</span><br><span class="line">                branch2_5 = slim.conv2d(branch2_4, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2e_1x7'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], <span class="number">3</span>)</span><br><span class="line">        <span class="comment">#将inception_2_m5存储到end_points中，作为Auxiliary Classifier辅助模型的分类</span></span><br><span class="line">        end_points[<span class="string">'inception_2_m5'</span>]=net</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 第3组</span></span><br><span class="line"><span class="comment"># inception_3_m1: 第3组的 1号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_3_m1'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">                branch_0 = slim.conv2d(branch_0,<span class="number">320</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                       stride=<span class="number">2</span>,</span><br><span class="line">                                       padding=<span class="string">'VALID'</span>,</span><br><span class="line">                                       scope=<span class="string">'conv2d_0b_3x3'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line">                branch1_2 = slim.conv2d(branch1_1, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">                                            scope=<span class="string">'conv2d_1b_1x7'</span>)</span><br><span class="line">                branch1_3 = slim.conv2d(branch1_2, <span class="number">192</span>, [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_1c_7x1'</span>)</span><br><span class="line">                branch1_4 = slim.conv2d(branch1_3, <span class="number">192</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                        stride=<span class="number">2</span>,</span><br><span class="line">                                        padding=<span class="string">'VALID'</span>,</span><br><span class="line">                                        scope=<span class="string">'conv2d_1c_3x3'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.max_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                            stride=<span class="number">2</span>,</span><br><span class="line">                                            padding=<span class="string">'VALID'</span>,</span><br><span class="line">                                            scope=<span class="string">'maxPool_3a_3x3'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_4, branch2_1], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># inception_3_m2: 第3组的 2号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_3_m2'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">320</span>, [<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line"><span class="comment">#特殊</span></span><br><span class="line">                branch1_2 = tf.concat([</span><br><span class="line">                    slim.conv2d(branch1_1, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">3</span>], scope=<span class="string">'conv2d_1a_1x3'</span>),</span><br><span class="line">                    slim.conv2d(branch1_1, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_3x1'</span>)</span><br><span class="line">                    ], <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">488</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2b_3x3'</span>)</span><br><span class="line">                branch2_3 = tf.concat([</span><br><span class="line">                    slim.conv2d(branch2_2, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">3</span>], scope=<span class="string">'conv2d_1a_1x3'</span>),</span><br><span class="line">                    slim.conv2d(branch2_2, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_3x1'</span>)</span><br><span class="line">                    ], <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_2, branch2_3,branch3_2], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># inception_3_m3: 第3组的 3号module</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inception_3_m3'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(net, <span class="number">320</span>, [<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">'conv2d_0a_1x1'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch1_1 = slim.conv2d(net, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_1x1'</span>)</span><br><span class="line"><span class="comment">#特殊</span></span><br><span class="line">                branch1_2 = tf.concat([</span><br><span class="line">                    slim.conv2d(branch1_1, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">3</span>], scope=<span class="string">'conv2d_1a_1x3'</span>),</span><br><span class="line">                    slim.conv2d(branch1_1, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_3x1'</span>)</span><br><span class="line">                    ], <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch2_1 = slim.conv2d(net, <span class="number">488</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_2a_1x1'</span>)</span><br><span class="line">                branch2_2 = slim.conv2d(branch2_1, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_2b_3x3'</span>)</span><br><span class="line">                branch2_3 = tf.concat([</span><br><span class="line">                    slim.conv2d(branch2_2, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">3</span>], scope=<span class="string">'conv2d_1a_1x3'</span>),</span><br><span class="line">                    slim.conv2d(branch2_2, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">1</span>], scope=<span class="string">'conv2d_1a_3x1'</span>)</span><br><span class="line">                    ], <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                branch3_1 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'avgPool_3a_3x3'</span>)</span><br><span class="line">                branch3_2 = slim.conv2d(branch3_1, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        scope=<span class="string">'conv2d_3b_1x1'</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）</span></span><br><span class="line">            net = tf.concat([branch_0, branch1_2, branch2_3,branch3_2], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> net,end_points</span><br><span class="line"><span class="comment">##############################  卷积部分完成  ########################################</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#第三部分：全局平均池化、softmax、Auxiliary Logits</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3</span><span class="params">(input,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_classes=<span class="number">1000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 is_training=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout_keep_prob=<span class="number">0.8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 prediction_fn=slim.softmax,</span></span></span><br><span class="line"><span class="function"><span class="params">                 spatial_squeeze=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                 reuse=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 scope=<span class="string">'inceptionV3'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">'inceptionV3'</span>,[input,num_classes],</span><br><span class="line">                       reuse=reuse) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.batch_norm,slim.dropout],</span><br><span class="line">                            is_training=is_training):</span><br><span class="line">             net,end_points=inception_V3_base(input,scope=scope)</span><br><span class="line"> </span><br><span class="line">            <span class="comment">#Auxiliary  Logits</span></span><br><span class="line">             <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],</span><br><span class="line">                            stride=<span class="number">1</span>,padding=<span class="string">'SAME'</span>):</span><br><span class="line">                aux_logits=end_points[<span class="string">'inception_2_m5'</span>]</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">'Auxiliary_Logits'</span>):</span><br><span class="line">                    aux_logits=slim.avg_pool2d(</span><br><span class="line">                        aux_logits,[<span class="number">5</span>,<span class="number">5</span>],stride=<span class="number">3</span>,padding=<span class="string">'VALID'</span>,</span><br><span class="line">                        scope=<span class="string">'AvgPool_1a_5x5'</span> )</span><br><span class="line">                    aux_logits=slim.conv2d(aux_logits,<span class="number">128</span>,[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                                       scope=<span class="string">'conv2d_1b_1x1'</span>)</span><br><span class="line">                    aux_logits=slim.conv2d(aux_logits,<span class="number">768</span>,[<span class="number">5</span>,<span class="number">5</span>],</span><br><span class="line">                                       weights_initializer=trunc_normal(<span class="number">0.01</span>),</span><br><span class="line">                                       padding=<span class="string">'VALID'</span>,</span><br><span class="line">                                       scope=<span class="string">'conv2d_2a_5x5'</span>)</span><br><span class="line">                    aux_logits = slim.conv2d(aux_logits, num_classes, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                         activation_fn=<span class="literal">None</span>,</span><br><span class="line">                                         normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                                         weights_initializer=trunc_normal(<span class="number">0.001</span>),</span><br><span class="line">                                         scope=<span class="string">'conv2d_2b_1x1'</span>)</span><br><span class="line">                    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">                        aux_logits =tf.squeeze(aux_logits,[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">'SpatialSqueeze'</span>)</span><br><span class="line">                    end_points[<span class="string">'Auxiliary_Logits'</span>]=aux_logits</span><br><span class="line"> </span><br><span class="line">             <span class="keyword">with</span> tf.variable_scope(<span class="string">'Logits'</span>):</span><br><span class="line">                 net=slim.avg_pool2d(net,[<span class="number">8</span>,<span class="number">8</span>],padding=<span class="string">'VALID'</span>,</span><br><span class="line">                                scope=<span class="string">'avgPool_1a_8x8'</span>)</span><br><span class="line">                 net=slim.dropout(net,keep_prob=dropout_keep_prob,</span><br><span class="line">                             scope=<span class="string">'dropout_1b'</span>)</span><br><span class="line">                 end_points[<span class="string">'PreLogits'</span>]=net</span><br><span class="line">                 logits=slim.conv2d(net,num_classes,[<span class="number">1</span>,<span class="number">1</span>],activation_fn=<span class="literal">None</span>,</span><br><span class="line">                               normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                               scope=<span class="string">'conv2d_1c_1x1'</span>)</span><br><span class="line">                 <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">                     logits=tf.squeeze(logits,[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">'SpatialSqueeze'</span>)</span><br><span class="line">             end_points[<span class="string">'Logits'</span>]=logits</span><br><span class="line">             end_points[<span class="string">'Predictions'</span>]=prediction_fn(logits,scope=<span class="string">'Predictions'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> logits,end_points</span><br><span class="line"> </span><br><span class="line"><span class="comment">########################### 构建完成</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_compute</span><span class="params">(session, target, info_string)</span>:</span></span><br><span class="line">    num_batch = <span class="number">100</span> <span class="comment">#100</span></span><br><span class="line">    num_step_burn_in = <span class="number">10</span>  <span class="comment"># 预热轮数，头几轮迭代有显存加载、cache命中等问题可以因此跳过</span></span><br><span class="line">    total_duration = <span class="number">0.0</span>  <span class="comment"># 总时间</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batch + num_step_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target )</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_step_burn_in:</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每迭代10次显示一次duration</span></span><br><span class="line">                print(<span class="string">"%s: step %d,duration=%.5f "</span> % (datetime.now(), i - num_step_burn_in, duration))</span><br><span class="line">            total_duration += duration</span><br><span class="line">            total_duration_squared += duration * duration</span><br><span class="line">    time_mean = total_duration / num_batch</span><br><span class="line">    time_variance = total_duration_squared / num_batch - time_mean * time_mean</span><br><span class="line">    time_stddev = math.sqrt(time_variance)</span><br><span class="line">    <span class="comment"># 迭代完成，输出</span></span><br><span class="line">    print(<span class="string">"%s: %s across %d steps,%.3f +/- %.3f sec per batch "</span> %</span><br><span class="line">          (datetime.now(), info_string, num_batch, time_mean, time_stddev))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        batch_size=<span class="number">32</span></span><br><span class="line">        height,weight=<span class="number">299</span>,<span class="number">299</span></span><br><span class="line">        input=tf.random_uniform( (batch_size,height,weight,<span class="number">3</span>) )</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope(inception_v3_arg_scope()):</span><br><span class="line">            logits,end_points=inception_v3(input,is_training=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">        init=tf.global_variables_initializer()</span><br><span class="line">        sess=tf.Session()</span><br><span class="line">        <span class="comment"># 将网络结构图写到文件中</span></span><br><span class="line">        writer = tf.summary.FileWriter(<span class="string">'logs/'</span>, sess.graph)</span><br><span class="line">        sess.run(init)</span><br><span class="line">        num_batches=<span class="number">100</span></span><br><span class="line">        time_compute(sess,logits,<span class="string">'Forward'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="ResNet-残差网络"><a href="#ResNet-残差网络" class="headerlink" title="ResNet-残差网络"></a>ResNet-残差网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> resnet_config</span><br><span class="line"><span class="keyword">from</span> data_loader <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> eval.evaluate <span class="keyword">import</span> accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 depth=resnet_config.depth,</span></span></span><br><span class="line"><span class="function"><span class="params">                 height=config.height,</span></span></span><br><span class="line"><span class="function"><span class="params">                 width=config.width,</span></span></span><br><span class="line"><span class="function"><span class="params">                 channel=config.channel,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_classes=config.num_classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate=resnet_config.learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_decay_rate=resnet_config.learning_decay_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_decay_steps=resnet_config.learning_decay_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                 epoch=resnet_config.epoch,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size=resnet_config.batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 model_path=resnet_config.model_path,</span></span></span><br><span class="line"><span class="function"><span class="params">                 summary_path=resnet_config.summary_path)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param depth:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.height = height</span><br><span class="line">        self.width = width</span><br><span class="line">        self.channel = channel</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.learning_decay_rate = learning_decay_rate</span><br><span class="line">        self.learning_decay_steps = learning_decay_steps</span><br><span class="line">        self.epoch = epoch</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.model_path = model_path</span><br><span class="line">        self.summary_path = summary_path</span><br><span class="line">        self.num_block_dict = &#123;<span class="number">18</span>: [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                               <span class="number">34</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                               <span class="number">50</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                               <span class="number">101</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>]&#125;</span><br><span class="line">        self.bottleneck_dict = &#123;<span class="number">18</span>: <span class="literal">False</span>,</span><br><span class="line">                                <span class="number">34</span>: <span class="literal">False</span>,</span><br><span class="line">                                <span class="number">50</span>: <span class="literal">True</span>,</span><br><span class="line">                                <span class="number">101</span>: <span class="literal">True</span>&#125;</span><br><span class="line">        self.filter_out = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">        self.filter_out_last_layer = [<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>]</span><br><span class="line">        self.conv_out_depth = self.filter_out[<span class="number">-1</span>] <span class="keyword">if</span> self.depth &lt; <span class="number">50</span> <span class="keyword">else</span> self.filter_out_last_layer[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">assert</span> self.depth <span class="keyword">in</span> self.num_block_dict, <span class="string">'depth should be in [18,34,50,101]'</span></span><br><span class="line">        self.num_block = self.num_block_dict[self.depth]</span><br><span class="line">        self.bottleneck = self.bottleneck_dict[self.depth]</span><br><span class="line">        self.input_x = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, self.height, self.width, self.channel], name=<span class="string">'input_x'</span>)</span><br><span class="line">        self.input_y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, self.num_classes], name=<span class="string">'input_y'</span>)</span><br><span class="line">        self.prediction = <span class="literal">None</span></span><br><span class="line">        self.loss = <span class="literal">None</span></span><br><span class="line">        self.acc = <span class="literal">None</span></span><br><span class="line">        self.global_step = <span class="literal">None</span></span><br><span class="line">        self.data_loader = DataLoader()</span><br><span class="line">        self.model()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># first convolution layers</span></span><br><span class="line">        x = self.conv(x=self.input_x, k_size=<span class="number">7</span>, filters_out=<span class="number">64</span>, strides=<span class="number">2</span>, activation=<span class="literal">True</span>, name=<span class="string">'First_Conv'</span>)</span><br><span class="line">        x = tf.layers.max_pooling2d(x, pool_size=[<span class="number">3</span>, <span class="number">3</span>], strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, name=<span class="string">'max_pool'</span>)</span><br><span class="line">        x = self.stack_block(x)</span><br><span class="line">        x = tf.layers.average_pooling2d(x, pool_size=x.get_shape()[<span class="number">1</span>:<span class="number">3</span>], strides=<span class="number">1</span>, name=<span class="string">'average_pool'</span>)</span><br><span class="line">        x = tf.reshape(x, [<span class="number">-1</span>, <span class="number">1</span> * <span class="number">1</span> * self.conv_out_depth])</span><br><span class="line">        fc_W = tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>)</span><br><span class="line">        logits = tf.layers.dense(inputs=x, units=self.num_classes,kernel_initializer=fc_W)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预测值</span></span><br><span class="line">        self.prediction = tf.argmax(logits,axis=<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># 计算准确率</span></span><br><span class="line">        self.acc = accuracy(logits, self.input_y)</span><br><span class="line">        <span class="comment"># 损失值</span></span><br><span class="line">        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.input_y))</span><br><span class="line">        <span class="comment"># 全局步数</span></span><br><span class="line">        self.global_step = tf.train.get_or_create_global_step()</span><br><span class="line">        <span class="comment"># 递减学习率</span></span><br><span class="line">        learning_rate = tf.train.exponential_decay(learning_rate=self.learning_rate,</span><br><span class="line">                                                   global_step=self.global_step,</span><br><span class="line">                                                   decay_rate=self.learning_decay_rate,</span><br><span class="line">                                                   decay_steps=self.learning_decay_steps,</span><br><span class="line">                                                   staircase=<span class="literal">True</span>)</span><br><span class="line">        self.optimize = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stack_block</span><span class="params">(self, input_x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> stack <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            stack_strides = <span class="number">1</span> <span class="keyword">if</span> stack == <span class="number">0</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">            stack_name = <span class="string">'stack_%s'</span> % stack</span><br><span class="line">            <span class="keyword">with</span> tf.name_scope(stack_name):</span><br><span class="line">                <span class="keyword">for</span> block <span class="keyword">in</span> range(self.num_block[stack]):</span><br><span class="line">                    shortcut = input_x</span><br><span class="line">                    block_strides = stack_strides <span class="keyword">if</span> block == <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                    block_name = stack_name + <span class="string">'_block_%s'</span> % block</span><br><span class="line">                    <span class="keyword">with</span> tf.name_scope(block_name):</span><br><span class="line">                        <span class="keyword">if</span> self.bottleneck:</span><br><span class="line">                            <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">                                <span class="keyword">with</span> tf.name_scope(block_name + <span class="string">'_layer_%s'</span> % layer):</span><br><span class="line">                                    filters = self.filter_out[stack] <span class="keyword">if</span> layer &lt; <span class="number">2</span> <span class="keyword">else</span> self.filter_out_last_layer[stack]</span><br><span class="line">                                    k_size = <span class="number">3</span> <span class="keyword">if</span> layer == <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                                    layer_strides = block_strides <span class="keyword">if</span> layer &lt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                                    activation = <span class="literal">True</span> <span class="keyword">if</span> layer &lt; <span class="number">2</span> <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">                                    layer_name = block_name + <span class="string">'_conv_%s'</span> % layer</span><br><span class="line">                                    input_x = self.conv(x=input_x, filters_out=filters, k_size=k_size,</span><br><span class="line">                                                        strides=layer_strides, activation=activation, name=layer_name)</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                                <span class="keyword">with</span> tf.name_scope(block_name + <span class="string">'_layer_%s'</span> % layer):</span><br><span class="line">                                    filters = self.filter_out[stack]</span><br><span class="line">                                    k_size = <span class="number">3</span></span><br><span class="line">                                    layer_strides = block_strides <span class="keyword">if</span> layer &lt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                                    activation = <span class="literal">True</span> <span class="keyword">if</span> layer &lt; <span class="number">1</span> <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">                                    layer_name = block_name + <span class="string">'_conv_%s'</span> % layer</span><br><span class="line">                                    input_x = self.conv(x=input_x, filters_out=filters, k_size=k_size,</span><br><span class="line">                                                        strides=layer_strides, activation=activation, name=layer_name)</span><br><span class="line">                    shortcut_depth = shortcut.get_shape()[<span class="number">-1</span>]</span><br><span class="line">                    input_x_depth = input_x.get_shape()[<span class="number">-1</span>]</span><br><span class="line">                    <span class="keyword">with</span> tf.name_scope(<span class="string">'shortcut_connect'</span>):</span><br><span class="line">                        <span class="keyword">if</span> shortcut_depth != input_x_depth:</span><br><span class="line">                            connect_k_size = <span class="number">1</span></span><br><span class="line">                            connect_strides = block_strides</span><br><span class="line">                            connect_filter = filters</span><br><span class="line">                            shortcut_name = block_name + <span class="string">'_shortcut'</span></span><br><span class="line">                            shortcut = self.conv(x=shortcut, filters_out=connect_filter, k_size=connect_k_size,</span><br><span class="line">                                                 strides=connect_strides, activation=<span class="literal">False</span>, name=shortcut_name)</span><br><span class="line">                        input_x = tf.nn.relu(shortcut + input_x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> input_x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv</span><span class="params">(self, x, k_size, filters_out, strides, activation, name)</span>:</span></span><br><span class="line">        x = tf.layers.conv2d(x, filters=filters_out, kernel_size=k_size, strides=strides, padding=<span class="string">'same'</span>, name=name)</span><br><span class="line">        x = tf.layers.batch_normalization(x, name=name + <span class="string">'_BN'</span>)</span><br><span class="line">        <span class="keyword">if</span> activation:</span><br><span class="line">            x = tf.nn.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, train_id_list, valid_img, valid_label)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        training model</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 模型存储路径初始化</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model_path):</span><br><span class="line">            os.makedirs(self.model_path)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.summary_path):</span><br><span class="line">            os.makedirs(self.summary_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train_steps初始化</span></span><br><span class="line">        train_steps = <span class="number">0</span></span><br><span class="line">        best_valid_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># summary初始化</span></span><br><span class="line">        tf.summary.scalar(<span class="string">'loss'</span>, self.loss)</span><br><span class="line">        merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># session初始化</span></span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        writer = tf.summary.FileWriter(self.summary_path, sess.graph)</span><br><span class="line">        saver = tf.train.Saver(max_to_keep=<span class="number">10</span>)</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(self.epoch):</span><br><span class="line">            shuffle_id_list = random.sample(train_id_list.tolist(), len(train_id_list))</span><br><span class="line">            batch_num = int(np.ceil(len(shuffle_id_list) / self.batch_size))</span><br><span class="line">            train_id_batch = np.array_split(shuffle_id_list, batch_num)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line">                this_batch = train_id_batch[i]</span><br><span class="line">                batch_img, batch_label = self.data_loader.get_batch_data(this_batch)</span><br><span class="line">                train_steps += <span class="number">1</span></span><br><span class="line">                feed_dict = &#123;self.input_x: batch_img, self.input_y: batch_label&#125;</span><br><span class="line">                _, train_loss, train_acc = sess.run([self.optimize, self.loss, self.acc], feed_dict=feed_dict)</span><br><span class="line">                <span class="keyword">if</span> train_steps % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                    val_loss, val_acc = sess.run([self.loss, self.acc],</span><br><span class="line">                                                 feed_dict=&#123;self.input_x: valid_img, self.input_y: valid_label&#125;)</span><br><span class="line">                    msg = <span class="string">'epoch:%s | steps:%s | train_loss:%.4f | val_loss:%.4f | train_acc:%.4f | val_acc:%.4f'</span> % (</span><br><span class="line">                        epoch, train_steps, train_loss, val_loss, train_acc, val_acc)</span><br><span class="line">                    print(msg)</span><br><span class="line">                    summary = sess.run(merged, feed_dict=&#123;self.input_x: valid_img, self.input_y: valid_label&#125;)</span><br><span class="line">                    writer.add_summary(summary, global_step=train_steps)</span><br><span class="line">                    <span class="keyword">if</span> val_acc &gt;= best_valid_acc:</span><br><span class="line">                        best_valid_acc = val_acc</span><br><span class="line">                        saver.save(sess, save_path=self.model_path, global_step=train_steps)</span><br><span class="line"></span><br><span class="line">        sess.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        predicting</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">        ckpt = tf.train.get_checkpoint_state(self.model_path)</span><br><span class="line">        saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">        prediction = sess.run(self.prediction, feed_dict=&#123;self.input_x: x&#125;)</span><br><span class="line">        <span class="keyword">return</span> prediction</span><br></pre></td></tr></table></figure><h4 id="DenseNet-密连网络"><a href="#DenseNet-密连网络" class="headerlink" title="DenseNet-密连网络"></a>DenseNet-密连网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_layer</span><span class="params">(input, filters,kernel_size,stride=<span class="number">1</span>, layer_name=<span class="string">"conv"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(layer_name):</span><br><span class="line">        net = slim.conv2d(input, filters, kernel_size, scope=layer_name)</span><br><span class="line">        <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,x,nb_blocks, filters, sess)</span>:</span></span><br><span class="line">        self.nb_blocks = nb_blocks</span><br><span class="line">        self.filters = filters</span><br><span class="line">        self.model = self.build_model(x)</span><br><span class="line">        self.sess = sess</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bottleneck_layer</span><span class="params">(self,x, scope)</span>:</span></span><br><span class="line">        <span class="comment"># [BN --&gt; ReLU --&gt; conv11 --&gt; BN --&gt; ReLU --&gt;conv33]</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(scope):</span><br><span class="line">            x = slim.batch_norm(x)</span><br><span class="line">            x = tf.nn.relu(x)</span><br><span class="line">            x = conv_layer(x,self.filters,kernel_size=(<span class="number">1</span>,<span class="number">1</span>), layer_name=scope+<span class="string">'_conv1'</span>)</span><br><span class="line">            x = slim.batch_norm(x)</span><br><span class="line">            x = tf.nn.relu(x)</span><br><span class="line">            x = conv_layer(x,self.filters,kernel_size=(<span class="number">3</span>,<span class="number">3</span>), layer_name=scope+<span class="string">'_conv2'</span>)</span><br><span class="line">            <span class="keyword">return</span> x </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transition_layer</span><span class="params">(self,x, scope)</span>:</span></span><br><span class="line">        <span class="comment"># [BN --&gt; conv11 --&gt; avg_pool2]</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(scope):</span><br><span class="line">            x = slim.batch_norm(x)</span><br><span class="line">            x = conv_layer(x,self.filters,kernel_size=(<span class="number">1</span>,<span class="number">1</span>), layer_name=scope+<span class="string">'_conv1'</span>)</span><br><span class="line">            x = slim.avg_pool2d(x,<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> x </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dense_block</span><span class="params">(self,input_x, nb_layers, layer_name)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(layer_name):</span><br><span class="line">            layers_concat = []</span><br><span class="line">            layers_concat.append(input_x)</span><br><span class="line">            x = self.bottleneck_layer(input_x,layer_name +<span class="string">'_bottleN_'</span>+str(<span class="number">0</span>))</span><br><span class="line">            layers_concat.append(x)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> xrange(nb_layers):</span><br><span class="line">                x = tf.concat(layers_concat,axis=<span class="number">3</span>)</span><br><span class="line">                x = self.bottleneck_layer(x,layer_name+<span class="string">'_bottleN_'</span>+str(i+<span class="number">1</span>))</span><br><span class="line">                layers_concat.append(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(self,input_x)</span>:</span></span><br><span class="line">        x = conv_layer(input_x,self.filters,kernel_size=(<span class="number">7</span>,<span class="number">7</span>), layer_name=<span class="string">'conv0'</span>)</span><br><span class="line">        x = slim.max_pool2d(x,(<span class="number">3</span>,<span class="number">3</span>))        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(self.nb_blocks):</span><br><span class="line">            print(i)</span><br><span class="line">            x = self.dense_block(x,<span class="number">4</span>, <span class="string">'dense_'</span>+str(i))</span><br><span class="line">            x = self.transition_layer(x,<span class="string">'trans_'</span>+str(i))        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></div><footer class="article-footer"><div class="post-share"><a href="javascript:;" id="share-sub" class="post-share-fab"><i class="fa fa-share-alt"></i></a><div class="post-share-list" id="share-list"><ul class="share-icons"><li><a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2020/01/31/CV-03/&title=《CV系列3：卷积神经网络代码实现》 — blog&pic=/images/banner.jpg" data-title="微博"><i class="fa fa-weibo"></i></a></li><li><a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信"><i class="fa fa-weixin"></i></a></li><li><a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://localhost:4000/2020/01/31/CV-03/&title=《CV系列3：卷积神经网络代码实现》 — blog&source=本文主要介绍各类典型卷积神经网络的代码实现。" data-title="QQ"><i class="fa fa-qq"></i></a></li><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2020/01/31/CV-03/" data-title="Facebook"><i class="fa fa-facebook"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《CV系列3：卷积神经网络代码实现》 — blog&url=http://localhost:4000/2020/01/31/CV-03/&via=http://localhost:4000" data-title="Twitter"><i class="fa fa-twitter"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://localhost:4000/2020/01/31/CV-03/" data-title="Google+"><i class="fa fa-google-plus"></i></a></li></ul></div></div><div class="post-modal wx-share" id="wxShare"><a class="close" href="javascript:;" id="wxShare-close">×</a><p>扫一扫，分享到微信</p><img src="//api.qrserver.com/v1/create-qr-code/?data=http://localhost:4000/2020/01/31/CV-03/" alt="微信分享二维码"></div><div class="mask"></div><ul class="article-footer-menu"></ul></footer></div></article><aside class="post-toc-pos post-toc-top" id="post-toc"><nav class="post-toc-wrap"><ol class="post-toc"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#LeNet-雏形网络"><span class="post-toc-text">LeNet-雏形网络</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#AlexLet"><span class="post-toc-text">AlexLet</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SPPNet-空间金字塔"><span class="post-toc-text">SPPNet-空间金字塔</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#VGGNet"><span class="post-toc-text">VGGNet</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#GoogLeNet"><span class="post-toc-text">GoogLeNet</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ResNet-残差网络"><span class="post-toc-text">ResNet-残差网络</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#DenseNet-密连网络"><span class="post-toc-text">DenseNet-密连网络</span></a></li></ol></nav></aside><nav id="article-nav"><a href="/2020/02/01/jenkins实战/" id="article-nav-newer" class="article-nav-link-wrap"><span class="article-nav-title"><i class="fa fa-hand-o-left" aria-hidden="true"></i> jenkins实战 </span></a><a href="/2020/01/19/CV-02/" id="article-nav-older" class="article-nav-link-wrap"><span class="article-nav-title">CV系列2：卷积神经网络的演变</span> <i class="fa fa-hand-o-right" aria-hidden="true"></i></a></nav></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner"><p><span id="busuanzi_container_site_uv" style="display:none">总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">总访问量：<span id="busuanzi_value_site_pv"></span></span></p><p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a> &copy; 2020 Yang Pei<br></p></div></div></footer><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><script>var mihoConfig={root:"http://localhost:4000",animate:"true",isHome:"false",share:"true"}</script><div class="sidebar"><div id="sidebar-top"><span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span></div></div><div class="sidebar-menu-box" id="sidebar-menu-box"><div class="sidebar-menu-box-container"><div id="sidebar-menu-box-categories"><a class="category-link" href="/categories/AI/">AI</a><a class="category-link" href="/categories/CV/">CV</a><a class="category-link" href="/categories/ML/">ML</a><a class="category-link" href="/categories/NLP/">NLP</a><a class="category-link" href="/categories/专题/">专题</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/感想/">感想</a><a class="category-link" href="/categories/源码/">源码</a><a class="category-link" href="/categories/计算机/">计算机</a><a class="category-link" href="/categories/语言/">语言</a></div><div id="sidebar-menu-box-tags"></div></div><a href="javascript:;" class="sidebar-menu-box-close">&times;</a></div>ß<div class="mobile-header-menu-nav" id="mobile-header-menu-nav"><div class="mobile-header-menu-container"><span class="title">Menus</span><ul class="mobile-header-menu-navbar"><li><a href="/"><i class="fa fa-home"></i><span>主页</span></a></li><li><a href="/categories/前端/"><i class="fa fa-前端"></i><span>前端</span></a></li><li><a href="/categories/源码/"><i class="fa fa-源码"></i><span>源码</span></a></li><li><a href="/categories/ML/"><i class="fa fa-ML"></i><span>ML</span></a></li><li><a href="/categories/CV/"><i class="fa fa-CV"></i><span>CV</span></a></li><li><a href="/categories/NLP/"><i class="fa fa-NLP"></i><span>NLP</span></a></li><li><a href="/categories/计算机/"><i class="fa fa-计算机"></i><span>计算机</span></a></li><li><a href="/categories/专题/"><i class="fa fa-专题"></i><span>专题</span></a></li><li><a href="/categories/感想/"><i class="fa fa-感想"></i><span>感想</span></a></li><li><a href="/categories/语言/"><i class="fa fa-语言"></i><span>语言</span></a></li></ul></div><div class="mobile-header-tag-container"><span class="title">Tags</span><div id="mobile-header-container-tags"></div></div></div><div class="search-wrap"><span class="search-close">&times;</span> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input class="search-field" placeholder="Search..." id="keywords"> <a id="search-submit" href="javascript:;"><i class="fa fa-search"></i></a><div class="search-container" id="search-container"><ul class="search-result" id="search-result"></ul></div></div><div id="search-tpl"><li class="search-result-item"><a href="{url}" class="search-item-li"><span class="search-item-li-title" title="{title}">{title}</span></a></li></div><script src="/js/search.js"></script><script src="/js/main.js"></script><script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script><div id="particles"></div><script src="/js/particles.js"></script><link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css"><script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script><script src="/js/animate.js"></script><script src="/js/pop-img.js"></script><script>$(".article-entry p img").popImg()</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({model:{jsonPath:"/live2dw/assets/Epsilon2.1.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body>