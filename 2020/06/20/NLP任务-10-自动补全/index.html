<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>NLP任务-10-自动补全 | blog</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="YP's Blog"><meta name="description" content="NLP任务-01-词嵌入NLP任务-02-序列标注NLP任务-03-分词NLP任务-04-词性标注NLP任务-05-命名实体识别NLP任务-06-依存句法分析NLP任务-07-信息抽取NLP任务-08-文本分类NLP任务-09-文本聚类NLP任务-10-自动补全NLP任务-11-语义消歧自动补全是个基于NLP的搜索任务，主要包括了以下几个方面：语言理解文档召回和排序文本生成辅助搜索语言理解这块的任"><meta name="keywords" content="web"><meta property="og:type" content="article"><meta property="og:title" content="NLP任务-10-自动补全"><meta property="og:url" content="http://localhost:4000/2020/06/20/NLP任务-10-自动补全/index.html"><meta property="og:site_name" content="blog"><meta property="og:description" content="NLP任务-01-词嵌入NLP任务-02-序列标注NLP任务-03-分词NLP任务-04-词性标注NLP任务-05-命名实体识别NLP任务-06-依存句法分析NLP任务-07-信息抽取NLP任务-08-文本分类NLP任务-09-文本聚类NLP任务-10-自动补全NLP任务-11-语义消歧自动补全是个基于NLP的搜索任务，主要包括了以下几个方面：语言理解文档召回和排序文本生成辅助搜索语言理解这块的任"><meta property="og:locale" content="default"><meta property="og:image" content="https://i.loli.net/2020/07/23/MPQpLulqTESiIna.png"><meta property="og:image" content="https://i.loli.net/2020/07/27/qPQNLlFvnzTyUHr.png"><meta property="og:image" content="https://i.loli.net/2020/07/27/7sqMkygxn83UYJO.png"><meta property="og:image" content="https://i.loli.net/2020/07/27/aBxdrj6Nos7SOWK.png"><meta property="og:image" content="https://i.loli.net/2020/07/27/fu8t9FBAh6yQCHb.png"><meta property="og:image" content="https://i.loli.net/2020/07/27/MC2DtKFon9jUhPz.png"><meta property="og:image" content="https://i.loli.net/2020/07/27/smUq8pnOEoQVzPk.png"><meta property="og:image" content="https://i.loli.net/2020/07/27/WUANVdqfYw28HPs.png"><meta property="og:updated_time" content="2020-08-04T03:52:42.600Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="NLP任务-10-自动补全"><meta name="twitter:description" content="NLP任务-01-词嵌入NLP任务-02-序列标注NLP任务-03-分词NLP任务-04-词性标注NLP任务-05-命名实体识别NLP任务-06-依存句法分析NLP任务-07-信息抽取NLP任务-08-文本分类NLP任务-09-文本聚类NLP任务-10-自动补全NLP任务-11-语义消歧自动补全是个基于NLP的搜索任务，主要包括了以下几个方面：语言理解文档召回和排序文本生成辅助搜索语言理解这块的任"><meta name="twitter:image" content="https://i.loli.net/2020/07/23/MPQpLulqTESiIna.png"><link rel="icon" href="/favicon.ico"><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/css/style.css"><script src="/js/pace.min.js"></script></head></html><body><div id="container"><header id="header"><div id="banner"></div><div id="header-outer"><div id="header-menu" class="header-menu-pos animated"><div class="header-menu-container"><a href="/" class="left"><span class="site-title">YP&#39;s Blog</span></a><nav id="header-menu-nav" class="right"><a href="/"><i class="fa fa-home"></i> <span>主页</span> </a><a href="/archives"><i class="fa fa-archive"></i> <span>全部</span> </a><a href="/categories/AI/"><i class="fa fa-AI"></i> <span>AI</span> </a><a href="/categories/前端/"><i class="fa fa-前端"></i> <span>前端</span> </a><a href="/categories/计算机/"><i class="fa fa-计算机"></i> <span>计算机</span> </a><a href="/categories/语言/"><i class="fa fa-语言"></i> <span>语言</span> </a><a href="/categories/专题/"><i class="fa fa-专题"></i> <span>专题</span></a></nav><a class="mobile-header-menu-button"><i class="fa fa-bars"></i></a></div></div><div id="header-row"><div id="logo"><a href="/"><img src="/images/logo.png" alt="logo"></a></div><div class="header-info"><div id="header-title"><h2>YP&#39;s Blog</h2></div><div id="header-description"><h3>一个专注前端智能化开发的技术博客</h3></div></div><nav class="header-nav"><div class="social"><a title="Blog" target="_blank" href="https://iloveyou11.github.io/"><i class="fa fa-home fa-2x"></i></a> <a title="Github" target="_blank" href="https://github.com/iloveyou11"><i class="fa fa-github fa-2x"></i></a></div></nav></div></div></header><div class="outer"><section id="main" class="body-wrap"><article id="post-NLP任务-10-自动补全" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="post-title" itemprop="name">NLP任务-10-自动补全</h1><div class="post-title-bar"><ul><li><i class="fa fa-book"></i> <a href="/categories/AI/">AI</a></li><li><i class="fa fa-calendar"></i> 2020-06-20</li><li><i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span></li></ul></div><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">3k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">11分</span></span></span></div></header><div class="article-entry post-content" itemprop="articleBody"><div id="toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#语言理解"><span class="toc-text">语言理解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#命名实体识别（NER）"><span class="toc-text">命名实体识别（NER）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#HMM-隐马尔可夫"><span class="toc-text">HMM(隐马尔可夫)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#CRF-条件随机场"><span class="toc-text">CRF(条件随机场)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#LSTM-CRF"><span class="toc-text">LSTM+CRF</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#CNN-CRF"><span class="toc-text">CNN+CRF</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#BERT-（LSTM）-CRF"><span class="toc-text">BERT+（LSTM）+CRF</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#指代消歧"><span class="toc-text">指代消歧</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#基于二元分类的方法"><span class="toc-text">基于二元分类的方法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#端到端的神经共指消解"><span class="toc-text">端到端的神经共指消解</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#文本分类"><span class="toc-text">文本分类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文档召回和排序"><span class="toc-text">文档召回和排序</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#文档召回"><span class="toc-text">文档召回</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#文档排序"><span class="toc-text">文档排序</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#TextRank"><span class="toc-text">TextRank</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#其他"><span class="toc-text">其他</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文本生成辅助搜索"><span class="toc-text">文本生成辅助搜索</span></a></li></ol></div><p><a href="https://iloveyou11.github.io/2020/06/11/NLP%E4%BB%BB%E5%8A%A1-01-%E8%AF%8D%E5%B5%8C%E5%85%A5/" target="_blank" rel="noopener">NLP任务-01-词嵌入</a><br><a href="https://iloveyou11.github.io/2020/06/12/NLP%E4%BB%BB%E5%8A%A1-02-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/" target="_blank" rel="noopener">NLP任务-02-序列标注</a><br><a href="https://iloveyou11.github.io/2020/06/13/NLP%E4%BB%BB%E5%8A%A1-03-%E5%88%86%E8%AF%8D/" target="_blank" rel="noopener">NLP任务-03-分词</a><br><a href="https://iloveyou11.github.io/2020/06/14/NLP%E4%BB%BB%E5%8A%A1-04-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/" target="_blank" rel="noopener">NLP任务-04-词性标注</a><br><a href="https://iloveyou11.github.io/2020/06/15/NLP%E4%BB%BB%E5%8A%A1-05-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%ABNER/" target="_blank" rel="noopener">NLP任务-05-命名实体识别</a><br><a href="https://iloveyou11.github.io/2020/06/16/NLP%E4%BB%BB%E5%8A%A1-06-%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">NLP任务-06-依存句法分析</a><br><a href="https://iloveyou11.github.io/2020/06/17/NLP%E4%BB%BB%E5%8A%A1-07-%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/" target="_blank" rel="noopener">NLP任务-07-信息抽取</a><br><a href="https://iloveyou11.github.io/2020/06/18/NLP%E4%BB%BB%E5%8A%A1-08-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">NLP任务-08-文本分类</a><br><a href="https://iloveyou11.github.io/2020/06/19/NLP%E4%BB%BB%E5%8A%A1-09-%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">NLP任务-09-文本聚类</a><br><a href="https://iloveyou11.github.io/2020/06/20/NLP%E4%BB%BB%E5%8A%A1-10-%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8/" target="_blank" rel="noopener">NLP任务-10-自动补全</a><br><a href="https://iloveyou11.github.io/2020/06/21/NLP%E4%BB%BB%E5%8A%A1-11-%E8%AF%AD%E4%B9%89%E6%B6%88%E6%AD%A7/" target="_blank" rel="noopener">NLP任务-11-语义消歧</a></p><p>自动补全是个基于NLP的搜索任务，主要包括了以下几个方面：</p><ol><li>语言理解</li><li>文档召回和排序</li><li>文本生成辅助搜索</li></ol><h3 id="语言理解"><a href="#语言理解" class="headerlink" title="语言理解"></a>语言理解</h3><p>这块的任务都是以分类问题为准，所谓的理解，只是把语言抽象到某一个理解空间，将其进行标准化，以便进行批量化处理。主要步骤是：</p><ol><li>实体标注(即命名实体识别)</li><li>实体消歧或指代消歧(基于知识的预测)</li><li>意图识别(句子级别预测，甚至就是简单的文本分类)</li></ol><h4 id="命名实体识别（NER）"><a href="#命名实体识别（NER）" class="headerlink" title="命名实体识别（NER）"></a>命名实体识别（NER）</h4><p><code>实体标注方面</code>，传统的统计方法是HMM(隐马尔可夫)、MEMM(最大熵马尔可夫)、CRF(条件随机场)，基本就和命名实体识别类似了，而在深度学习引入后，形成了输入层、编码层、解码层的主要架构，同过预训练表征模型(如w2v)、深度学习结构(CNN、RNN等)以及输出层(CRF、softmax)等结构链接，完成最基本的结构。</p><p><strong>数据标注方式：</strong><br>主要有BIO和BIOES两种。BIOES如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">BIOES如下：</span><br><span class="line"></span><br><span class="line">B，即Begin，表示开始</span><br><span class="line">I，即Intermediate，表示中间</span><br><span class="line">E，即End，表示结尾</span><br><span class="line">S，即Single，表示单个字符</span><br><span class="line">O，即Other，表示其他，用于标记无关字符</span><br><span class="line"></span><br><span class="line">BIO如下：</span><br><span class="line"></span><br><span class="line">B，即Begin，表示开始</span><br><span class="line">I，即Intermediate，表示中间</span><br><span class="line">E，即End，表示结尾</span><br></pre></td></tr></table></figure><h5 id="HMM-隐马尔可夫"><a href="#HMM-隐马尔可夫" class="headerlink" title="HMM(隐马尔可夫)"></a>HMM(隐马尔可夫)</h5><p><strong>具体流程：</strong></p><img src="https://i.loli.net/2020/07/23/MPQpLulqTESiIna.png" alt="HMM参数" width="80%"><p>当获得了分好词的语料之后，三个概率<code>θ=(A,B,Π)</code>可以通过如下方式获得：<br>(1) 初始状态概率<code>Π</code>-<code>P(z1)</code><br>统计每个句子开头，序列标记分别为B，S的个数，最后除以总句子的个数，即得到了初始概率矩阵。<br>(2) 状态转移概率<code>A</code>-<code>(zi|zi-1)</code><br>根据语料，统计不同序列状态之间转化的个数，例如<code>count(yi=”E”|yi-1=”M”)</code>为语料中i-1时刻标为“M”时，i时刻标记为“E”出现的次数。得到一个<code>4*4</code>的矩阵，再将矩阵的每个元素除以语料中该标记字的个数，得到状态转移概率矩阵。<br>(3) 输出观测概率<code>B</code>-<code>P(xi|zi)</code><br>根据语料，统计由某个隐藏状态输出为某个观测状态的个数，例如<code>count(xi=”深”|yi=”B”)</code>为i时刻标记为“B”时，i时刻观测到字为“深”的次数。得到一个<code>4*N</code>的矩阵，再将矩阵的每个元素除以语料中该标记的个数，得到输出观测概率矩阵。</p><p>训练结束后，即可获得三个概率矩阵<code>θ=(A,B,Π)</code>，接下来需要使用维特比算法获得一个句子的最大概率分词标记序列。</p><img src="https://i.loli.net/2020/07/27/qPQNLlFvnzTyUHr.png" alt="NER-HMM" width="80%"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">第一个词为“我”，通过初始概率矩阵和输出观测概率矩阵分别计算delta1(&quot;B&quot;)=P(y1=”S”)P(x1=”我”|y1=”S”)，delta1(&quot;M&quot;)=P(y1=”B”)P(x1=”我”|y1=”B”)，delta1(&quot;E&quot;)=P(y1=”M”)P(x1=”我”|y1=”M”)，delta1(&quot;S&quot;)=P(y1=”E”)P(x1=”我”|y1=”E”)，并设kethe1(&quot;B&quot;)=kethe1(&quot;M&quot;)=kethe1(&quot;E&quot;)=kethe1(&quot;S&quot;)=0；</span><br><span class="line">同理利用公式分别计算：</span><br><span class="line">delta2(&quot;B&quot;)，delta2(&quot;M&quot;)，delta2(&quot;E&quot;)，delta2(&quot;S&quot;)。图中列出了delta2(&quot;S&quot;)的计算过程，就是计算：</span><br><span class="line">P(y2=”S”|y1=”B”)P(x2=”爱”|y2=”S”)</span><br><span class="line">P(y2=”S”|y1=”M”)P(x2=”爱”|y2=”S”)</span><br><span class="line">P(y2=”S”|y1=”E”)P(x2=”爱”|y2=”S”)</span><br><span class="line">P(y2=”S”|y1=”S”)P(x2=”爱”|y2=”S”)</span><br><span class="line">其中P(y2=”S”|y1=”S”)P(x2=”爱”|y2=”S”)的值最大，为0.034，因此delta2(&quot;S&quot;)，kethe2(&quot;S&quot;)=&quot;S&quot;，同理，可以计算出delta2(&quot;B&quot;)，delta2(&quot;M&quot;)，delta2(&quot;E&quot;)及kethe2(&quot;B&quot;)，kethe2(&quot;M&quot;)，kethe2(&quot;E&quot;)。</span><br><span class="line"></span><br><span class="line">同理可以获得第三个和第四个序列标记的delta和kethe。</span><br><span class="line">到最后一个序列，delta4(&quot;B&quot;)，delta4(&quot;M&quot;)，delta4(&quot;E&quot;)，delta4(&quot;S&quot;)中delta4(&quot;S&quot;)的值最大，因此，最后一个状态为”S”。</span><br><span class="line">最后，回退，</span><br><span class="line">i3 = kethe4(&quot;S&quot;) =&quot;B&quot;</span><br><span class="line">i2 =kethe3(&quot;B&quot;) = &quot;S&quot;</span><br><span class="line">i1 = kethe2(&quot;S&quot;) =&quot;S&quot;</span><br><span class="line">求得序列标记为：“SSBE”。</span><br></pre></td></tr></table></figure><p><strong>HMM解决序列标注问题的优势与不足：</strong><br>HMM时非常适合用于序列标注问题，但HMM引入了马尔科夫假设，即T时刻的状态仅仅与前一时刻的状态相关。但是，语言往往是前后文相互照应的，所以HMM可能会有它的局限和问题，我们可以思考一下，如何解决这个问题。</p><h5 id="CRF-条件随机场"><a href="#CRF-条件随机场" class="headerlink" title="CRF(条件随机场)"></a>CRF(条件随机场)</h5><p>NER任务特征提取的网路结构如下：</p><img src="https://i.loli.net/2020/07/27/7sqMkygxn83UYJO.png" alt="NER-CRF" width="80%"><p>句子经过双向LSTM进行特征提取之后，会得到一个特征输出。训练时，将这个特征和对应的label输入到条件随机场中，就可以计算损失了。预测时，将自然语言输入到该网络，经CRF就可以识别该句子中的实体了。</p><p><code>条件随机场(CRF)在现今NLP中序列标记任务中是不可或缺的存在。太多的实现基于此，例如LSTM+CRF，CNN+CRF，BERT+CRF。因此，这是一个必须要深入理解和吃透的模型。！！</code></p><h5 id="LSTM-CRF"><a href="#LSTM-CRF" class="headerlink" title="LSTM+CRF"></a>LSTM+CRF</h5><p>采用LSTM作为特征抽取器，再接一个CRF层来作为输出层，结构如下图所示：</p><img src="https://i.loli.net/2020/07/27/aBxdrj6Nos7SOWK.png" alt="NER-LSTM+CRF" width="80%"><h5 id="CNN-CRF"><a href="#CNN-CRF" class="headerlink" title="CNN+CRF"></a>CNN+CRF</h5><p>采用LSTM作为特征抽取器，再接一个CRF层来作为输出层，结构如下图所示：</p><img src="https://i.loli.net/2020/07/27/fu8t9FBAh6yQCHb.png" alt="NER-CNN+CRF" width="80%"><p>虽然CNN并不太擅长长序列的特征提取，但是CNN具有非常高效的并行运算能力，能够加快运算速度。</p><h5 id="BERT-（LSTM）-CRF"><a href="#BERT-（LSTM）-CRF" class="headerlink" title="BERT+（LSTM）+CRF"></a>BERT+（LSTM）+CRF</h5><p>利用预训练好的BERT模型，再用少量的标注数据进行fine tune，能够快速地实现NER任务。</p><img src="https://i.loli.net/2020/07/27/MC2DtKFon9jUhPz.png" alt="NER-BERT+CRF" width="80%"><h4 id="指代消歧"><a href="#指代消歧" class="headerlink" title="指代消歧"></a>指代消歧</h4><p><code>实体消歧或指代消歧</code>主要是解决在用户搜索的语句中出现的问题，例如”苹果”到底是水果还是手机等等，这个是依赖上下文信息和知识库合力完成的，例如一句话”我爱吃苹果”，这个”吃”其实就是一个上下文的信息，另一方面我们要通过这个”吃”推断出这个水果的含义，我们就需要借助知识库。</p><h5 id="基于二元分类的方法"><a href="#基于二元分类的方法" class="headerlink" title="基于二元分类的方法"></a>基于二元分类的方法</h5><p>共指消解需要考虑的特征主要分为以下几类：<code>词汇、距离、一致性、语法、语义等</code>。</p><ol><li>词汇特征主要考虑两个 Mention 的字符串的匹配程度，一般而言字符串相同程度越高的 Mention共指概率越大。</li><li>距离特征主要考察两个 Mention 的句子距离，这个主要依据是共指事实上也是一种局部性的替代关系， 越是临近的 Mention 之间共指概率越大。 一般而言，两个 Mention 相隔超过三个句子，共指的可能性就会很小了。</li><li>一致性特征详细可以分为性别、单复数、语义类别等是否一致。这组特征主要起到筛选的作用。</li><li>语法关系用来判断两个 Mention 的语法角色之间的关系，由于对句子深层的语法分析还很难办到，这里主要采用的是一些基于特定模板的方法，例如判断两个 Mention 之间是否被逗号格开或者相邻等来决定是否具有同位关系。</li><li>语义特征主要是考察两个 Mention 在语义类别不一致时是否满足上下位或者同义、近义关系。这种判断主要依赖于具体的语言学词典，例如英文上的 WordNet(Fellbaum, 1998)、中文上的 HowNet(董振东,董强, 2001)等。</li></ol><h5 id="端到端的神经共指消解"><a href="#端到端的神经共指消解" class="headerlink" title="端到端的神经共指消解"></a>端到端的神经共指消解</h5><blockquote><p>参考论文：Lee K, He L, Lewis M, et al. End-to-end Neural Coreference Resolution[J]. 2017:188-197.</p></blockquote><img src="https://i.loli.net/2020/07/27/smUq8pnOEoQVzPk.png" alt="指代消解" width="80%"><p>【具体步骤】</p><ol><li>计算每个span的向量表示，并以此对各潜在mention(同一实体)打分。具体的做法是：将编码信息切分成一组sentence，对每一个sentence独立地构建深度学习模型，将特征矩阵输入到深度学习模型（如LSTM、CNN）中，得到由sentence构成的篇章文本中每一个词的向量表示。对于每个span，将其中的每个词进行组合得到span的向量表示。然后对span的向量表示进行非线性映射，得到每个潜在mention的分数，并以该分数大小对mention进行修剪，得到一定数量的mention。</li><li>对每一对span的向量表示计算先行语得分。通过对两个span的mention score及它们的配对先行语得分求和，得到一对span最终的共指得分。</li></ol><h4 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h4><p><code>意图识别方面</code>，应该是这几块里面最简单的，就是一套深度学习，框架即可完成，fasttext、CNN以及BiRNN-Attention。这里详见<a href>NLP项目专题-01</a></p><h3 id="文档召回和排序"><a href="#文档召回和排序" class="headerlink" title="文档召回和排序"></a>文档召回和排序</h3><p>这个思路和推荐系统类似，我们先把有关的全都拿出来，然后再用更为精细的方法排好序展示给用户，此处就有两个大步，召回和排序。</p><h4 id="文档召回"><a href="#文档召回" class="headerlink" title="文档召回"></a>文档召回</h4><p><code>召回方面</code>，要求更全，此处又有句法召回和语义召回。句法召回说白了就是匹配，但这里面的学问可是非常多的，字符串匹配、倒排索引(搜索系统中非常关键的基础知识)、多路召回(多领域)，语义召回则是通过词向量近邻等方式扩大召回的内容。</p><p><strong>倒排表（Inverted Index）</strong>：有一个完整的词典库，分别记录每个单词出现在哪些文档中，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们：[Doc1，Doc13]</span><br><span class="line">昨天：[Doc2]</span><br><span class="line">在：[Doc1，Doc4，Doc5]</span><br><span class="line">运动：[Doc1，Doc3，Doc5]</span><br><span class="line">什么：[Doc1，Doc6]</span><br></pre></td></tr></table></figure><p>这样可以快速找到哪个单词出现在哪个文档中（否则根据单词一个个去搜索文档时间复杂度非常高）</p><h4 id="文档排序"><a href="#文档排序" class="headerlink" title="文档排序"></a>文档排序</h4><p><code>排序方面</code>，LTR(learningtorank)其实是一个隐含在暗线但实际上已经非常经典的方向，就是为了研究排序的。</p><h5 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a>TextRank</h5><p>TextRank是一种文本排序算法，是由网页排序算法PageRank发展而来。TextRank算法是利用局部词汇之间关系（共现窗口）对后续关键词进行排序，直接从文本本身抽取。TextRank可以进行文档排序、关键词提取、文本摘要提取等等。</p><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>要了解更多关于搜索引擎的排序算法，可参考<a href="https://cloud.tencent.com/developer/news/184638" target="_blank" rel="noopener">《回顾·搜索引擎算法体系简介——排序和意图篇》</a><br>如果是视频搜索排序，可以参考<a href="https://www.infoq.cn/article/RUlwIBXPmUKILgqiyR4I" target="_blank" rel="noopener">《阿里文娱搜索算法实践与思考》</a></p><img src="https://i.loli.net/2020/07/27/WUANVdqfYw28HPs.png" alt="排序算法" width="80%"><h3 id="文本生成辅助搜索"><a href="#文本生成辅助搜索" class="headerlink" title="文本生成辅助搜索"></a>文本生成辅助搜索</h3><p>这块主要用于辅助用户进行搜索，主要体现在下面三块功能上：</p><ol><li>自动补全。很好理解，大家在很多搜索引擎中都会看到，在百度下输入”自然语言”，他能给你预测出你可能要搜”自然语言处理”。这个使用js就可以直接解决。</li><li>搜索重构。举个例子吧，你输入的是吃鸡，实际上文档库的标题是”和平精英”，那要映射过去，其实就是一种同义词重构。这个使用同义词查询就可以解决。</li><li>拼写修正。英文有拼写问题，中文有错别字问题，不能保证用户100%输入正确，平时打字都可能手滑，为了更准确理解语义，我们必须在进行语义分析前修正这些错误。这个可以采用朴素贝叶斯方法解决。</li></ol></div><footer class="article-footer"><div class="post-share"><a href="javascript:;" id="share-sub" class="post-share-fab"><i class="fa fa-share-alt"></i></a><div class="post-share-list" id="share-list"><ul class="share-icons"><li><a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2020/06/20/NLP任务-10-自动补全/&title=《NLP任务-10-自动补全》 — blog&pic=/images/banner.jpg" data-title="微博"><i class="fa fa-weibo"></i></a></li><li><a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信"><i class="fa fa-weixin"></i></a></li><li><a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://localhost:4000/2020/06/20/NLP任务-10-自动补全/&title=《NLP任务-10-自动补全》 — blog&source=一个专注前端智能化开发技术的网站" data-title="QQ"><i class="fa fa-qq"></i></a></li><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2020/06/20/NLP任务-10-自动补全/" data-title="Facebook"><i class="fa fa-facebook"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《NLP任务-10-自动补全》 — blog&url=http://localhost:4000/2020/06/20/NLP任务-10-自动补全/&via=http://localhost:4000" data-title="Twitter"><i class="fa fa-twitter"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://localhost:4000/2020/06/20/NLP任务-10-自动补全/" data-title="Google+"><i class="fa fa-google-plus"></i></a></li></ul></div></div><div class="post-modal wx-share" id="wxShare"><a class="close" href="javascript:;" id="wxShare-close">×</a><p>扫一扫，分享到微信</p><img src="//api.qrserver.com/v1/create-qr-code/?data=http://localhost:4000/2020/06/20/NLP任务-10-自动补全/" alt="微信分享二维码"></div><div class="mask"></div><ul class="article-footer-menu"></ul></footer></div></article><aside class="post-toc-pos post-toc-top" id="post-toc"><nav class="post-toc-wrap"><ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#语言理解"><span class="post-toc-text">语言理解</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#命名实体识别（NER）"><span class="post-toc-text">命名实体识别（NER）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#HMM-隐马尔可夫"><span class="post-toc-text">HMM(隐马尔可夫)</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#CRF-条件随机场"><span class="post-toc-text">CRF(条件随机场)</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#LSTM-CRF"><span class="post-toc-text">LSTM+CRF</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#CNN-CRF"><span class="post-toc-text">CNN+CRF</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#BERT-（LSTM）-CRF"><span class="post-toc-text">BERT+（LSTM）+CRF</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#指代消歧"><span class="post-toc-text">指代消歧</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#基于二元分类的方法"><span class="post-toc-text">基于二元分类的方法</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#端到端的神经共指消解"><span class="post-toc-text">端到端的神经共指消解</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#文本分类"><span class="post-toc-text">文本分类</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#文档召回和排序"><span class="post-toc-text">文档召回和排序</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#文档召回"><span class="post-toc-text">文档召回</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#文档排序"><span class="post-toc-text">文档排序</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#TextRank"><span class="post-toc-text">TextRank</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#其他"><span class="post-toc-text">其他</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#文本生成辅助搜索"><span class="post-toc-text">文本生成辅助搜索</span></a></li></ol></nav></aside><nav id="article-nav"><a href="/2020/06/21/NLP任务-11-语义消歧/" id="article-nav-newer" class="article-nav-link-wrap"><span class="article-nav-title"><i class="fa fa-hand-o-left" aria-hidden="true"></i> NLP任务-11-语义消歧 </span></a><a href="/2020/06/19/NLP任务-09-文本聚类/" id="article-nav-older" class="article-nav-link-wrap"><span class="article-nav-title">NLP任务-09-文本聚类</span> <i class="fa fa-hand-o-right" aria-hidden="true"></i></a></nav><div id="lv-container" data-id="city" data-uid="MTAyMC81MDE5Mi8yNjY4Mg=="><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><noscript>为正常使用来必力评论功能请激活JavaScript</noscript></div></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner"><p><span id="busuanzi_container_site_uv" style="display:none">总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">总访问量：<span id="busuanzi_value_site_pv"></span></span></p><p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a> &copy; 2020 Yang Pei<br></p></div></div></footer><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><script>var mihoConfig={root:"http://localhost:4000",animate:"false",isHome:"false",share:"true"}</script><div class="sidebar"><div id="sidebar-top"><span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span></div></div><div class="sidebar-menu-box" id="sidebar-menu-box"><div class="sidebar-menu-box-container"><div id="sidebar-menu-box-categories"><a class="category-link" href="/categories/AI/">AI</a><a class="category-link" href="/categories/专题/">专题</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/计算机/">计算机</a><a class="category-link" href="/categories/语言/">语言</a></div><div id="sidebar-menu-box-tags"></div></div><a href="javascript:;" class="sidebar-menu-box-close">&times;</a></div>ß<div class="mobile-header-menu-nav" id="mobile-header-menu-nav"><div class="mobile-header-menu-container"><span class="title">Menus</span><ul class="mobile-header-menu-navbar"><li><a href="/"><i class="fa fa-home"></i><span>主页</span></a></li><li><a href="/archives"><i class="fa fa-archive"></i><span>全部</span></a></li><li><a href="/categories/AI/"><i class="fa fa-AI"></i><span>AI</span></a></li><li><a href="/categories/前端/"><i class="fa fa-前端"></i><span>前端</span></a></li><li><a href="/categories/计算机/"><i class="fa fa-计算机"></i><span>计算机</span></a></li><li><a href="/categories/语言/"><i class="fa fa-语言"></i><span>语言</span></a></li><li><a href="/categories/专题/"><i class="fa fa-专题"></i><span>专题</span></a></li></ul></div><div class="mobile-header-tag-container"><span class="title">Tags</span><div id="mobile-header-container-tags"></div></div></div><div class="search-wrap"><span class="search-close">&times;</span> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input class="search-field" placeholder="Search..." id="keywords"> <a id="search-submit" href="javascript:;"><i class="fa fa-search"></i></a><div class="search-container" id="search-container"><ul class="search-result" id="search-result"></ul></div></div><div id="search-tpl"><li class="search-result-item"><a href="{url}" class="search-item-li"><span class="search-item-li-title" title="{title}">{title}</span></a></li></div><script src="/js/search.js"></script><script src="/js/main.js"></script><script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script><div id="particles"></div><script src="/js/particles.js"></script><script src="/js/pop-img.js"></script><script>$(".article-entry p img").popImg()</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({model:{jsonPath:"/live2dw/assets/Epsilon2.1.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body>