<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><link rel="stylesheet" href="/js/fancybox/dist/jquery.fancybox.min.css"><title>blog</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="YP's Blog"><meta name="description" content="一个专注前端智能化开发技术的网站"><meta name="keywords" content="web"><meta property="og:type" content="website"><meta property="og:title" content="blog"><meta property="og:url" content="http://localhost:4000/page/2/index.html"><meta property="og:site_name" content="blog"><meta property="og:description" content="一个专注前端智能化开发技术的网站"><meta property="og:locale" content="default"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="blog"><meta name="twitter:description" content="一个专注前端智能化开发技术的网站"><link rel="icon" href="/favicon.ico"><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/css/style.css"><script src="/js/pace.min.js"></script></head><script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="/js/fancybox/dist/jquery.fancybox.min.js"></script><script src="/js/wrapimg.js"></script></html><body><div id="container"><header id="header"><div id="banner"></div><div id="header-outer"><div id="header-menu" class="header-menu-pos animated"><div class="header-menu-container"><a href="/" class="left"><span class="site-title">YP&#39;s Blog</span></a><nav id="header-menu-nav" class="right"><a href="/"><i class="fa fa-home"></i> <span>主页</span> </a><a href="/categories/前端/"><i class="fa fa-前端"></i> <span>前端</span> </a><a href="/categories/源码/"><i class="fa fa-源码"></i> <span>源码</span> </a><a href="/categories/ML/"><i class="fa fa-ML"></i> <span>ML</span> </a><a href="/categories/CV/"><i class="fa fa-CV"></i> <span>CV</span> </a><a href="/categories/NLP/"><i class="fa fa-NLP"></i> <span>NLP</span> </a><a href="/categories/计算机/"><i class="fa fa-计算机"></i> <span>计算机</span> </a><a href="/categories/专题/"><i class="fa fa-专题"></i> <span>专题</span> </a><a href="/categories/感想/"><i class="fa fa-感想"></i> <span>感想</span> </a><a href="/categories/语言/"><i class="fa fa-语言"></i> <span>语言</span></a></nav><a class="mobile-header-menu-button"><i class="fa fa-bars"></i></a></div></div><div id="header-row"><div id="logo"><a href="/"><img src="/images/logo.png" alt="logo"></a></div><div class="header-info"><div id="header-title"><h2>YP&#39;s Blog</h2></div><div id="header-description"><h3>一个专注前端智能化领域的技术博客</h3></div></div><nav class="header-nav"><div class="social"><a title="Blog" target="_blank" href="https://iloveyou11.github.io/"><i class="fa fa-home fa-2x"></i></a> <a title="Github" target="_blank" href="https://github.com/iloveyou11"><i class="fa fa-github fa-2x"></i></a></div></nav></div></div></header><div class="outer"><section id="main" class="body-wrap"><article id="post-NLP模型-05-transformer" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/05/02/NLP模型-05-transformer/" target="_blank">NLP模型-05-transformer</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">1.8k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">6分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p>什么是transformer一句话解释：transformer是带有self-attention的seq2seq，输出能同时计算，可以代替RNN结构（必须按输入的顺序计算），因此transformer对于sequence to sequence的应用场景更为高效。transformer是首个完全抛弃RNN的recurrence，CNN的convolution，仅用atte...</p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-05-02</li></ul><a href="/2020/05/02/NLP模型-05-transformer/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-NLP模型-04-CRF" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/04/21/NLP模型-04-CRF/" target="_blank">NLP模型-04-CRF</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">958字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">3分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p>CRF简介CRF是条件随机场（Conditional Random Fields）的简称，是给定一组输入序列条件下另一组输出序列的条件概率分布模型。为什么要使用CRF模型？举个例子，我们今有一项任务是“图像分类”，如果我们只是单纯看到了一个人闭着嘴的照片，其实很难去标记他到底在干什么，如果我们能够获得这张照片前一点点时间的照片的话，则很好标记，如在时间序列上前一张的照片里...</p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-04-21</li></ul><a href="/2020/04/21/NLP模型-04-CRF/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-NLP模型-03-GMM" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/04/12/NLP模型-03-GMM/" target="_blank">NLP模型-03-GMM</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">822字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">2分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p>GMM是高斯混合模型（Gaussian Mixture Model）的简称。GMM模型是一种普遍使用的聚类算法，使用了高斯分布作为模型参数，EM算法进行训练。高斯分布高斯分布也叫正态分布，是一种在自然界大量的存在的、最为常见的分布形式。高斯分布的概率密度函数公式如下：参数μ表示均值，参数σ表示标准差，σ越小则数据越集中，σ越大则数据越分散。1234567891011121...</p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-04-12</li></ul><a href="/2020/04/12/NLP模型-03-GMM/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-vue源码解读" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/04/10/vue源码解读/" target="_blank">vue源码解读</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">6.5k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">27分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/源码/">源码</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p>关于如何读源码曾经，我去阅读源码时，总是喜欢一句不漏的从上到下阅读，怕漏掉什么核心代码导致不理解程序的详细流程，遇到一个函数或类就跳过去看，然后……花费了很多的时间而且效果相当不理想。不仅对整体的框架没有大体了解，也让自己像只无头苍蝇，不知道如何抓住重点去深入研究背后的原理。后来看了不少大佬阅读的方式，对我也是收获颇多：不要采用DFS（深度遍历）的方式阅读源码，而是应该先...</p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-04-10</li></ul><a href="/2020/04/10/vue源码解读/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-NLP模型-02-HMM" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/03/30/NLP模型-02-HMM/" target="_blank">NLP模型-02-HMM</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">1.2k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">4分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p>初识HMMHMM是Hidden Markov Model的缩写，是一个时序类的模型，x是观测值，z是隐变量。z可以视为状态，每个状态z都可以产生一个观测值x。HMM是有方向的生成模型。HMM的基础结构如下：HMM中三个不同的参数：θ=(A,B,Π)，其中：A是状态z到状态z的转移矩阵，指从一个状态z转移到下一个状态z的概率。B是状态z到结果x的生成矩阵，表示从一个状态z下...</p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-03-30</li></ul><a href="/2020/03/30/NLP模型-02-HMM/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-NLP-06" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/03/24/NLP-06/" target="_blank">NLP系列6：词向量与文本生成</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">6.2k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">22分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p></p><p>核心内容：词向量、xgboost、RNN、LSTM、GRU、attention机制、self-attention、good-representation、seq2seq、看图说话、深度文本匹配、BERT、LDA、transform</p><p></p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-03-24</li></ul><a href="/2020/03/24/NLP-06/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-NLP模型-01-预训练语言模型" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/03/20/NLP模型-01-预训练语言模型/" target="_blank">NLP模型-01-预训练语言模型</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">2k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">7分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p>什么是预训练模型预训练模型实际上就是已经训练过的模型或模型组件。它已经在大型训练集上经过了很好的训练，学习到了每个词的表示，并且保存下来所有的模型参数。预训练模型的本质是用来文本特征提取，可以适用于迁移学习，减少前期训练词向量的成本。预训练模型在word embedding就开始出现，相当于AlexNet在卷积神经网络的地位（雏形）。word embedding是将文字进...</p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-03-20</li></ul><a href="/2020/03/20/NLP模型-01-预训练语言模型/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-数据增强" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/03/20/数据增强/" target="_blank">数据增强</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">1.1k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">3分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/ML/">ML</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p>何为数据增强数据增强是通过已经获得的数据来产生更多的数据，等质量训练样本的增强会提高模型的精度。这种处理手段常被用于数据不足的场景中。模型需要大量的数据才可能在许多任务中获得较为满意的结果，但是，现实生活中，很多场景无法获得大量的样本数据（如医学图像等），这时，使用数据增强可以解决这个问题，提高样本数据的数量和质量。有一些机器学习库实现了数据增强，如imgaug对计算机视...</p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-03-20</li></ul><a href="/2020/03/20/数据增强/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-NLP-05" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/03/16/NLP-05/" target="_blank">NLP系列5：重要模型与算法</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">4.2k字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">14分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p></p><p>核心内容：HMM、隐变量模型、EM算法、k-means算法、GMM、CRF</p><p></p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-03-16</li></ul><a href="/2020/03/16/NLP-05/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><article id="post-NLP-04" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/03/11/NLP-04/" target="_blank">NLP系列4：NLP核心任务</a></h1><div style="margin-top:10px"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i> <span class="post-meta-item-text">字数统计: </span><span class="post-count">476字</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i> <span class="post-meta-item-text">阅读时长: </span><span class="post-count">1分</span></span></span></div><span class="header-cate"><i class="fa fa-map-marker"></i> <a href="/categories/NLP/">NLP</a></span></header><div class="article-entry" itemprop="articleBody"><div class="article-feed"><p></p><p>核心内容：文本摘要、命名实体识别、关系抽取、实体消歧、实体统一、指代消解、句法分析、CKY算法</p><p></p></div></div><footer class="article-footer"><ul class="article-footer-menu"><li><i class="fa fa-calendar"></i> 2020-03-11</li></ul><a href="/2020/03/11/NLP-04/#more" class="article-more-link" target="_blank">more&gt;&gt;</a></footer></div></article><nav id="page-nav"><a class="extend prev" rel="prev" href="/"><< Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/3/">Next >></a></nav></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner"><p><span id="busuanzi_container_site_uv" style="display:none">总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">总访问量：<span id="busuanzi_value_site_pv"></span></span></p><p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a> &copy; 2020 Yang Pei<br></p></div></div></footer><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><script>var mihoConfig={root:"http://localhost:4000",animate:"true",isHome:"true",share:"true"}</script><div class="sidebar"><div id="sidebar-top"><span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span></div></div><div class="sidebar-menu-box" id="sidebar-menu-box"><div class="sidebar-menu-box-container"><div id="sidebar-menu-box-categories"><a class="category-link" href="/categories/AI/">AI</a><a class="category-link" href="/categories/CV/">CV</a><a class="category-link" href="/categories/ML/">ML</a><a class="category-link" href="/categories/NLP/">NLP</a><a class="category-link" href="/categories/专题/">专题</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/感想/">感想</a><a class="category-link" href="/categories/源码/">源码</a><a class="category-link" href="/categories/计算机/">计算机</a><a class="category-link" href="/categories/语言/">语言</a></div><div id="sidebar-menu-box-tags"></div></div><a href="javascript:;" class="sidebar-menu-box-close">&times;</a></div>ß<div class="mobile-header-menu-nav" id="mobile-header-menu-nav"><div class="mobile-header-menu-container"><span class="title">Menus</span><ul class="mobile-header-menu-navbar"><li><a href="/"><i class="fa fa-home"></i><span>主页</span></a></li><li><a href="/categories/前端/"><i class="fa fa-前端"></i><span>前端</span></a></li><li><a href="/categories/源码/"><i class="fa fa-源码"></i><span>源码</span></a></li><li><a href="/categories/ML/"><i class="fa fa-ML"></i><span>ML</span></a></li><li><a href="/categories/CV/"><i class="fa fa-CV"></i><span>CV</span></a></li><li><a href="/categories/NLP/"><i class="fa fa-NLP"></i><span>NLP</span></a></li><li><a href="/categories/计算机/"><i class="fa fa-计算机"></i><span>计算机</span></a></li><li><a href="/categories/专题/"><i class="fa fa-专题"></i><span>专题</span></a></li><li><a href="/categories/感想/"><i class="fa fa-感想"></i><span>感想</span></a></li><li><a href="/categories/语言/"><i class="fa fa-语言"></i><span>语言</span></a></li></ul></div><div class="mobile-header-tag-container"><span class="title">Tags</span><div id="mobile-header-container-tags"></div></div></div><div class="search-wrap"><span class="search-close">&times;</span> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input class="search-field" placeholder="Search..." id="keywords"> <a id="search-submit" href="javascript:;"><i class="fa fa-search"></i></a><div class="search-container" id="search-container"><ul class="search-result" id="search-result"></ul></div></div><div id="search-tpl"><li class="search-result-item"><a href="{url}" class="search-item-li"><span class="search-item-li-title" title="{title}">{title}</span></a></li></div><script src="/js/search.js"></script><script src="/js/main.js"></script><script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script><div id="particles"></div><script src="/js/particles.js"></script><link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css"><script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script><script src="/js/animate.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({model:{jsonPath:"/live2dw/assets/Epsilon2.1.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body>